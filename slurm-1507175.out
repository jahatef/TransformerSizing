
Lmod is automatically replacing "PrgEnv-cray/8.3.3" with "PrgEnv-gnu/8.3.3".


Lmod is automatically replacing "cce/15.0.0" with "gcc/12.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.23     2) darshan-runtime/3.4.0

0
2
4
6
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
[2023-11-22 14:43:20,649] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 14:43:20,649] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 14:43:20,649] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 14:43:20,649] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 8, hidden_size: 12016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1502x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1502x2048): 87.197
Elapsed time for attention_prob_times_values (32x2048x2048x1502): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1502): 81.680

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 1074.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1503x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1503x2048): 85.926
Elapsed time for attention_prob_times_values (32x2048x2048x1503): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1503): 72.953

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1005.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1504x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1504x2048): 98.434
Elapsed time for attention_prob_times_values (32x2048x2048x1504): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1504): 85.239

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1164.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1505x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1505x2048): 82.847
Elapsed time for attention_prob_times_values (32x2048x2048x1505): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1505): 76.840

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1017.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1506x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1506x2048): 83.470
Elapsed time for attention_prob_times_values (32x2048x2048x1506): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1506): 81.830

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1054.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1507x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1507x2048): 87.119
Elapsed time for attention_prob_times_values (32x2048x2048x1507): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1507): 76.633

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1041.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1508x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1508x2048): 88.324
Elapsed time for attention_prob_times_values (32x2048x2048x1508): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1508): 77.288

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1053.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1509x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1509x2048): 86.971
Elapsed time for attention_prob_times_values (32x2048x2048x1509): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1509): 74.065

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1023.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1510x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1510x2048): 82.111
Elapsed time for attention_prob_times_values (32x2048x2048x1510): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1510): 81.976

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1049.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1511x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1511x2048): 86.789
2.1.1+rocm5.6 

num_attention_heads: 20, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x840x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x840x2048): 80.870
Elapsed time for attention_prob_times_values (80x2048x2048x840): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x840): 89.131

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1476.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x841x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x841x2048): 75.865
Elapsed time for attention_prob_times_values (80x2048x2048x841): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x841): 77.687

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1337.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x842x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x842x2048): 78.922
Elapsed time for attention_prob_times_values (80x2048x2048x842): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x842): 83.179

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1412.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x843x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x843x2048): 78.301
Elapsed time for attention_prob_times_values (80x2048x2048x843): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x843): 81.111

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1391.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x844x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x844x2048): 79.937
Elapsed time for attention_prob_times_values (80x2048x2048x844): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x844): 78.724

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1386.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x845x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x845x2048): 78.242
Elapsed time for attention_prob_times_values (80x2048x2048x845): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x845): 81.121

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1394.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x846x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x846x2048): 78.764
Elapsed time for attention_prob_times_values (80x2048x2048x846): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x846): 83.565

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1421.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x847x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x847x2048): 77.749
Elapsed time for attention_prob_times_values (80x2048x2048x847): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x847): 81.255

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1394.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x848x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x848x2048): 75.636
Elapsed time for attention_prob_times_values (80x2048x2048x848): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x848): 89.619

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1440.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x849x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x849x2048): 77.385
Elapsed time for attention_prob_times_values (80x2048x2048x849): 0.0070
Elapsed time for attention_prob_times_values (32x2048x2048x1511): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1511): 71.181

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1001.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1512x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1512x2048): 88.439
Elapsed time for attention_prob_times_values (32x2048x2048x1512): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1512): 83.968

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1103.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1513x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1513x2048): 86.082
Elapsed time for attention_prob_times_values (32x2048x2048x1513): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1513): 76.499

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1038.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1514x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1514x2048): 87.242
Elapsed time for attention_prob_times_values (32x2048x2048x1514): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1514): 81.968

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 1084.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1515x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1515x2048): 82.431
Elapsed time for attention_prob_times_values (32x2048x2048x1515): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1515): 76.453

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1018.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1516x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1516x2048): 84.078
Elapsed time for attention_prob_times_values (32x2048x2048x1516): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1516): 82.374

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1068.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1517x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1517x2048): 86.288
Elapsed time for attention_prob_times_values (32x2048x2048x1517): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1517): 74.058

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1024.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1518x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1518x2048): 83.551
Elapsed time for attention_prob_times_values (32x2048x2048x1518): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1518): 82.220

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1065.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1519x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1519x2048): 82.776
Elapsed time for attention_prob_times_values (32x2048x2048x1519): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1519): 76.624

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1023.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1520x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1520x2048): 89.106
Elapsed time for attention_prob_times_values (32x2048x2048x1520): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1520): 85.560

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1123.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
2.1.1+rocm5.6 

num_attention_heads: 40, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x518x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x518x2048): 68.969
Elapsed time for attention_prob_times_values (160x2048x2048x518): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x518): 65.411

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1425.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x519x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x519x2048): 68.511
Elapsed time for attention_prob_times_values (160x2048x2048x519): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x519): 60.932

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1372.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x520x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x520x2048): 72.603
Elapsed time for attention_prob_times_values (160x2048x2048x520): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x520): 75.404

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1576.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x521x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x521x2048): 68.812
Elapsed time for attention_prob_times_values (160x2048x2048x521): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x521): 62.355

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1396.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x522x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x522x2048): 69.320
Elapsed time for attention_prob_times_values (160x2048x2048x522): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x522): 65.165

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1436.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x523x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x523x2048): 68.885
Elapsed time for attention_prob_times_values (160x2048x2048x523): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x523): 60.442

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1379.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x524x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x524x2048): 68.372
Elapsed time for attention_prob_times_values (160x2048x2048x524): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x524): 62.822

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1405.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x525x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x525x2048): 69.802
Elapsed time for attention_prob_times_values (160x2048x2048x525): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x525): 60.486

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1393.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x526x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x526x2048): 68.863
Elapsed time for attention_prob_times_values (160x2048x2048x526): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x526): 62.915

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1416.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x527x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x527x2048): 70.900
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1521x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1521x2048): 86.254
Elapsed time for attention_prob_times_values (32x2048x2048x1521): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1521): 76.492

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1044.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1522x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1522x2048): 86.761
Elapsed time for attention_prob_times_values (32x2048x2048x1522): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1522): 82.180

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1088.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1523x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1523x2048): 86.211
Elapsed time for attention_prob_times_values (32x2048x2048x1523): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1523): 76.174

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1043.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1524x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1524x2048): 84.839
Elapsed time for attention_prob_times_values (32x2048x2048x1524): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1524): 82.387

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1078.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1525x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1525x2048): 86.147
Elapsed time for attention_prob_times_values (32x2048x2048x1525): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1525): 74.480

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1031.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1526x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1526x2048): 85.339
Elapsed time for attention_prob_times_values (32x2048x2048x1526): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1526): 82.495

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1084.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1527x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1527x2048): 84.680
Elapsed time for attention_prob_times_values (32x2048x2048x1527): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1527): 75.831

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1034.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1528x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1528x2048): 87.721
Elapsed time for attention_prob_times_values (32x2048x2048x1528): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1528): 88.611

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1140.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1529x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1529x2048): 85.694
Elapsed time for attention_prob_times_values (32x2048x2048x1529): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1529): 75.204

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1037.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x849): 81.588

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1396.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x850x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x850x2048): 75.821
Elapsed time for attention_prob_times_values (80x2048x2048x850): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x850): 83.623

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1399.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x851x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x851x2048): 77.898
Elapsed time for attention_prob_times_values (80x2048x2048x851): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x851): 81.722

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1405.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x852x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x852x2048): 78.177
Elapsed time for attention_prob_times_values (80x2048x2048x852): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x852): 83.656

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1425.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x853x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x853x2048): 77.441
Elapsed time for attention_prob_times_values (80x2048x2048x853): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x853): 81.830

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1405.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x854x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x854x2048): 78.767
Elapsed time for attention_prob_times_values (80x2048x2048x854): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x854): 83.940

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1436.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x855x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x855x2048): 78.167
Elapsed time for attention_prob_times_values (80x2048x2048x855): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x855): 81.145

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1409.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x856x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x856x2048): 80.141
Elapsed time for attention_prob_times_values (80x2048x2048x856): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x856): 90.549

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1506.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x857x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x857x2048): 77.504
Elapsed time for attention_prob_times_values (80x2048x2048x857): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x857): 80.682

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1402.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x858x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x858x2048): 76.944
Elapsed time for attention_prob_times_values (80x2048x2048x858): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x858): 82.656

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1415.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_key_query_prob (32x2048x1530x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1530x2048): 86.651
Elapsed time for attention_prob_times_values (32x2048x2048x1530): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1530): 82.482

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1094.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1531x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1531x2048): 83.759
Elapsed time for attention_prob_times_values (32x2048x2048x1531): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1531): 75.306

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1027.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1532x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1532x2048): 87.027
Elapsed time for attention_prob_times_values (32x2048x2048x1532): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1532): 80.433

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1084.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1533x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1533x2048): 85.692
Elapsed time for attention_prob_times_values (32x2048x2048x1533): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1533): 73.201

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1024.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1534x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1534x2048): 86.473
Elapsed time for attention_prob_times_values (32x2048x2048x1534): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1534): 82.563

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1096.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1535x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1535x2048): 85.195
Elapsed time for attention_prob_times_values (32x2048x2048x1535): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1535): 75.142

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1037.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1536x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1536x2048): 90.373
Elapsed time for attention_prob_times_values (32x2048x2048x1536): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1536): 92.933

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1191.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1537x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1537x2048): 84.386
Elapsed time for attention_prob_times_values (32x2048x2048x1537): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1537): 72.083

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1011.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1538x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1538x2048): 87.585
Elapsed time for attention_prob_times_values (32x2048x2048x1538): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1538): 75.578

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1056.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1539x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1539x2048): 83.988
Elapsed time for attention_prob_times_values (32x2048x2048x1539): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1539): 72.361

Attention duration (in seconds): 0.0106
2.1.1+rocm5.6 

num_attention_heads: 96, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x264x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x264x2048): 58.869
Elapsed time for attention_prob_times_values (384x2048x2048x264): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x264): 54.955

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1463.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x265x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x265x2048): 56.343
Elapsed time for attention_prob_times_values (384x2048x2048x265): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x265): 56.994

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1464.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x266x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x266x2048): 56.626
Elapsed time for attention_prob_times_values (384x2048x2048x266): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x266): 59.935

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 1510.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x267x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x267x2048): 55.380
Elapsed time for attention_prob_times_values (384x2048x2048x267): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x267): 57.399

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1467.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x268x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x268x2048): 57.614
Elapsed time for attention_prob_times_values (384x2048x2048x268): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x268): 60.594

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1543.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x269x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x269x2048): 57.363
Elapsed time for attention_prob_times_values (384x2048x2048x269): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x269): 58.204

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1514.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x270x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x270x2048): 57.600
Elapsed time for attention_prob_times_values (384x2048x2048x270): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x270): 60.144

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1548.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x271x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x271x2048): 55.125
Elapsed time for attention_prob_times_values (384x2048x2048x271): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x271): 58.087

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1493.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x272x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x272x2048): 59.057
Elapsed time for attention_prob_times_values (384x2048x2048x272): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x272): 75.968

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1761.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x273x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x273x2048): 54.737
========================================================================================================================
num_attention_heads: 20, hidden_size: 17180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x859x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x859x2048): 75.570
Elapsed time for attention_prob_times_values (80x2048x2048x859): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x859): 81.701

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1395.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x860x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x860x2048): 79.408
Elapsed time for attention_prob_times_values (80x2048x2048x860): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x860): 84.731

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1459.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x861x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x861x2048): 75.313
Elapsed time for attention_prob_times_values (80x2048x2048x861): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x861): 81.911

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1398.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x862x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x862x2048): 78.534
Elapsed time for attention_prob_times_values (80x2048x2048x862): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x862): 84.899

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1455.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x863x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x863x2048): 77.755
Elapsed time for attention_prob_times_values (80x2048x2048x863): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x863): 81.833

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1423.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x864x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x864x2048): 90.042
Elapsed time for attention_prob_times_values (80x2048x2048x864): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x864): 90.343

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1612.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x865x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x865x2048): 79.458
Elapsed time for attention_prob_times_values (80x2048x2048x865): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x865): 82.823

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1451.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x866x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x866x2048): 80.313
Elapsed time for attention_prob_times_values (80x2048x2048x866): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x866): 85.167

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1480.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x867x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x867x2048): 77.485
Elapsed time for attention_prob_times_values (80x2048x2048x867): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x867): 81.411

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1423.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x868x2048): 0.0072
Attention throughput (in TFLOP/s): 1012.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1540x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1540x2048): 85.367
Elapsed time for attention_prob_times_values (32x2048x2048x1540): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1540): 78.138

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1063.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1541x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1541x2048): 86.526
Elapsed time for attention_prob_times_values (32x2048x2048x1541): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1541): 68.885

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1000.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1542x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1542x2048): 87.897
Elapsed time for attention_prob_times_values (32x2048x2048x1542): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1542): 78.011

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1078.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1543x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1543x2048): 86.759
Elapsed time for attention_prob_times_values (32x2048x2048x1543): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1543): 72.607

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1032.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1544x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1544x2048): 88.136
Elapsed time for attention_prob_times_values (32x2048x2048x1544): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1544): 83.061

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1117.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1545x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1545x2048): 86.264
Elapsed time for attention_prob_times_values (32x2048x2048x1545): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1545): 72.528

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1029.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1546x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1546x2048): 85.213
Elapsed time for attention_prob_times_values (32x2048x2048x1546): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1546): 75.509

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1047.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1547x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1547x2048): 83.723
Elapsed time for attention_prob_times_values (32x2048x2048x1547): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1547): 72.786

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1019.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1548x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1548x2048): 87.715
Elapsed time for attention_prob_times_values (32x2048x2048x1548): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1548): 75.788

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1064.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (160x2048x2048x527): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x527): 62.839

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1438.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x528x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x528x2048): 70.899
Elapsed time for attention_prob_times_values (160x2048x2048x528): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x528): 74.880

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1575.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x529x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x529x2048): 70.140
Elapsed time for attention_prob_times_values (160x2048x2048x529): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x529): 60.286

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1404.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x530x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x530x2048): 70.081
Elapsed time for attention_prob_times_values (160x2048x2048x530): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x530): 65.729

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1472.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x531x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x531x2048): 68.679
Elapsed time for attention_prob_times_values (160x2048x2048x531): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x531): 62.812

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1426.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x532x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x532x2048): 70.553
Elapsed time for attention_prob_times_values (160x2048x2048x532): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x532): 66.618

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1492.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x533x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x533x2048): 68.744
Elapsed time for attention_prob_times_values (160x2048x2048x533): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x533): 61.969

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1422.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x534x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x534x2048): 69.799
Elapsed time for attention_prob_times_values (160x2048x2048x534): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x534): 64.891

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1470.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x535x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x535x2048): 68.775
Elapsed time for attention_prob_times_values (160x2048x2048x535): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x535): 62.627

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1435.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x536x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x536x2048): 70.743
Elapsed time for attention_prob_times_values (160x2048x2048x536): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x536): 73.549

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1582.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
num_attention_heads: 8, hidden_size: 12392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1549x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1549x2048): 86.621
Elapsed time for attention_prob_times_values (32x2048x2048x1549): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1549): 70.743

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1020.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1550x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1550x2048): 87.467
Elapsed time for attention_prob_times_values (32x2048x2048x1550): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1550): 78.097

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1081.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1551x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1551x2048): 86.732
Elapsed time for attention_prob_times_values (32x2048x2048x1551): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1551): 73.247

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1041.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1552x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1552x2048): 88.655
Elapsed time for attention_prob_times_values (32x2048x2048x1552): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1552): 83.985

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1132.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1553x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1553x2048): 81.925
Elapsed time for attention_prob_times_values (32x2048x2048x1553): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1553): 71.246

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1000.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1554x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1554x2048): 87.160
Elapsed time for attention_prob_times_values (32x2048x2048x1554): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1554): 75.949

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1066.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1555x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1555x2048): 83.685
Elapsed time for attention_prob_times_values (32x2048x2048x1555): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1555): 73.337

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1027.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1556x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1556x2048): 87.665
Elapsed time for attention_prob_times_values (32x2048x2048x1556): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1556): 76.375

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1073.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1557x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1557x2048): 86.284
Elapsed time for attention_prob_times_values (32x2048x2048x1557): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1557): 73.327

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1043.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1558x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1558x2048): 83.261
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x868x2048): 80.557
Elapsed time for attention_prob_times_values (80x2048x2048x868): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x868): 84.286

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1478.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x869x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x869x2048): 78.772
Elapsed time for attention_prob_times_values (80x2048x2048x869): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x869): 82.936

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1452.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x870x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x870x2048): 77.270
Elapsed time for attention_prob_times_values (80x2048x2048x870): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x870): 83.644

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1445.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x871x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x871x2048): 78.125
Elapsed time for attention_prob_times_values (80x2048x2048x871): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x871): 83.048

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1450.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x872x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x872x2048): 77.131
Elapsed time for attention_prob_times_values (80x2048x2048x872): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x872): 91.989

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1512.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x873x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x873x2048): 77.927
Elapsed time for attention_prob_times_values (80x2048x2048x873): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x873): 83.202

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1452.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x874x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x874x2048): 78.960
Elapsed time for attention_prob_times_values (80x2048x2048x874): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x874): 82.211

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1455.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x875x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x875x2048): 78.257
Elapsed time for attention_prob_times_values (80x2048x2048x875): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x875): 83.173

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1458.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x876x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x876x2048): 77.349
Elapsed time for attention_prob_times_values (80x2048x2048x876): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x876): 81.888

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1440.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x877x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x877x2048): 78.277
Elapsed time for attention_prob_times_values (80x2048x2048x877): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x877): 83.292

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1463.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Elapsed time for attention_prob_times_values (32x2048x2048x1558): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1558): 78.356

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1063.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1559x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1559x2048): 83.557
Elapsed time for attention_prob_times_values (32x2048x2048x1559): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1559): 73.498

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1030.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1560x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1560x2048): 88.058
Elapsed time for attention_prob_times_values (32x2048x2048x1560): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1560): 83.798

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1132.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1561x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1561x2048): 85.972
Elapsed time for attention_prob_times_values (32x2048x2048x1561): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1561): 73.571

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1046.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1562x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1562x2048): 86.967
Elapsed time for attention_prob_times_values (32x2048x2048x1562): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1562): 78.319

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1088.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1563x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1563x2048): 80.180
Elapsed time for attention_prob_times_values (32x2048x2048x1563): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1563): 73.770

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1015.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1564x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1564x2048): 87.360
Elapsed time for attention_prob_times_values (32x2048x2048x1564): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1564): 78.728

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1094.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1565x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1565x2048): 86.146
Elapsed time for attention_prob_times_values (32x2048x2048x1565): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1565): 73.815

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1051.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1566x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1566x2048): 87.087
Elapsed time for attention_prob_times_values (32x2048x2048x1566): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1566): 78.649

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1093.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1567x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1567x2048): 84.226
Elapsed time for attention_prob_times_values (32x2048x2048x1567): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1567): 73.475

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1039.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x537x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x537x2048): 70.061
Elapsed time for attention_prob_times_values (160x2048x2048x537): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x537): 63.058

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1458.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x538x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x538x2048): 70.873
Elapsed time for attention_prob_times_values (160x2048x2048x538): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x538): 67.086

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1517.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x539x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x539x2048): 70.480
Elapsed time for attention_prob_times_values (160x2048x2048x539): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x539): 63.134

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1468.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x540x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x540x2048): 71.402
Elapsed time for attention_prob_times_values (160x2048x2048x540): 0.0250
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x540): 29.016

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 911.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x541x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x541x2048): 68.398
Elapsed time for attention_prob_times_values (160x2048x2048x541): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x541): 62.881

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1450.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x542x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x542x2048): 70.276
Elapsed time for attention_prob_times_values (160x2048x2048x542): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x542): 66.135

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1510.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x543x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x543x2048): 70.257
Elapsed time for attention_prob_times_values (160x2048x2048x543): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x543): 63.447

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1480.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x544x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x544x2048): 87.224
Elapsed time for attention_prob_times_values (160x2048x2048x544): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x544): 79.608

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1852.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x545x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x545x2048): 73.552
Elapsed time for attention_prob_times_values (160x2048x2048x545): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x545): 64.382

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1530.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1568x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1568x2048): 96.292
Elapsed time for attention_prob_times_values (32x2048x2048x1568): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1568): 83.546

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1185.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1569x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1569x2048): 87.648
Elapsed time for attention_prob_times_values (32x2048x2048x1569): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1569): 71.456

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1043.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1570x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1570x2048): 88.123
Elapsed time for attention_prob_times_values (32x2048x2048x1570): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1570): 76.728

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1088.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1571x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1571x2048): 84.895
Elapsed time for attention_prob_times_values (32x2048x2048x1571): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1571): 73.890

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1048.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1572x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1572x2048): 88.740
Elapsed time for attention_prob_times_values (32x2048x2048x1572): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1572): 76.145

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1088.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1573x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1573x2048): 87.226
Elapsed time for attention_prob_times_values (32x2048x2048x1573): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1573): 71.923

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1047.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1574x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1574x2048): 88.189
Elapsed time for attention_prob_times_values (32x2048x2048x1574): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1574): 78.861

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1107.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1575x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1575x2048): 83.320
Elapsed time for attention_prob_times_values (32x2048x2048x1575): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1575): 73.715

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1040.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1576x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1576x2048): 88.730
Elapsed time for attention_prob_times_values (32x2048x2048x1576): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1576): 84.464

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1152.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x878x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x878x2048): 79.076
Elapsed time for attention_prob_times_values (80x2048x2048x878): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x878): 85.674

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1492.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x879x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x879x2048): 78.291
Elapsed time for attention_prob_times_values (80x2048x2048x879): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x879): 83.449

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1467.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x880x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x880x2048): 80.437
Elapsed time for attention_prob_times_values (80x2048x2048x880): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x880): 90.662

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1550.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x881x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x881x2048): 77.819
Elapsed time for attention_prob_times_values (80x2048x2048x881): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x881): 83.647

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1467.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x882x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x882x2048): 76.121
Elapsed time for attention_prob_times_values (80x2048x2048x882): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x882): 84.379

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1458.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x883x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x883x2048): 76.301
Elapsed time for attention_prob_times_values (80x2048x2048x883): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x883): 81.645

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1439.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x884x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x884x2048): 79.145
Elapsed time for attention_prob_times_values (80x2048x2048x884): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x884): 85.252

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1499.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x885x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x885x2048): 77.809
Elapsed time for attention_prob_times_values (80x2048x2048x885): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x885): 83.823

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1475.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x886x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x886x2048): 77.150
Elapsed time for attention_prob_times_values (80x2048x2048x886): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x886): 86.174

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1490.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


EstimateElapsed time for attention_prob_times_values (384x2048x2048x273): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x273): 58.538

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1504.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x274x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x274x2048): 56.557
Elapsed time for attention_prob_times_values (384x2048x2048x274): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x274): 60.642

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 1561.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x275x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x275x2048): 54.813
Elapsed time for attention_prob_times_values (384x2048x2048x275): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x275): 58.437

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1514.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x276x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x276x2048): 57.746
Elapsed time for attention_prob_times_values (384x2048x2048x276): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x276): 61.571

Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 1601.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0298
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x277x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x277x2048): 57.300
Elapsed time for attention_prob_times_values (384x2048x2048x277): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x277): 59.416

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1573.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x278x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x278x2048): 57.336
Elapsed time for attention_prob_times_values (384x2048x2048x278): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x278): 60.575

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1594.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x279x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x279x2048): 56.829
Elapsed time for attention_prob_times_values (384x2048x2048x279): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x279): 59.612

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1580.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x280x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x280x2048): 59.328
Elapsed time for attention_prob_times_values (384x2048x2048x280): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x280): 77.911

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1835.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x281x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x281x2048): 57.142
Elapsed time for attention_prob_times_values (384x2048x2048x281): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x281): 60.204

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1603.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x282x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x282x2048): 57.543
Elapsed time for attention_prob_times_values (384x2048x2048x282): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x282): 61.886

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1636.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Elapsed time for attention_key_query_prob (32x2048x1577x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1577x2048): 86.405
Elapsed time for attention_prob_times_values (32x2048x2048x1577): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1577): 73.662

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1059.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1578x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1578x2048): 87.220
Elapsed time for attention_prob_times_values (32x2048x2048x1578): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1578): 78.921

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1104.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1579x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1579x2048): 86.376
Elapsed time for attention_prob_times_values (32x2048x2048x1579): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1579): 73.618

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1060.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1580x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1580x2048): 88.022
Elapsed time for attention_prob_times_values (32x2048x2048x1580): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1580): 79.126

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1112.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1581x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1581x2048): 86.490
Elapsed time for attention_prob_times_values (32x2048x2048x1581): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1581): 68.311

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1019.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1582x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1582x2048): 87.475
Elapsed time for attention_prob_times_values (32x2048x2048x1582): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1582): 74.861

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1077.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1583x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1583x2048): 86.675
Elapsed time for attention_prob_times_values (32x2048x2048x1583): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1583): 74.000

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1067.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1584x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1584x2048): 89.020
Elapsed time for attention_prob_times_values (32x2048x2048x1584): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1584): 85.364

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1165.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1585x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1585x2048): 86.398
Elapsed time for attention_prob_times_values (32x2048x2048x1585): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1585): 73.842

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1065.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1586x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1586x2048): 87.345
Elapsed time for attention_prob_times_values (32x2048x2048x1586): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1586): 75.696

Attention duration (in seconds): 0.0105

--------
Elapsed time for attention_key_query_prob (80x2048x887x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x887x2048): 75.204
Elapsed time for attention_prob_times_values (80x2048x2048x887): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x887): 83.795

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1452.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x888x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x888x2048): 79.744
Elapsed time for attention_prob_times_values (80x2048x2048x888): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x888): 93.338

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1577.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x889x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x889x2048): 77.642
Elapsed time for attention_prob_times_values (80x2048x2048x889): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x889): 84.097

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1482.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x890x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x890x2048): 78.214
Elapsed time for attention_prob_times_values (80x2048x2048x890): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x890): 86.737

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1512.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x891x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x891x2048): 76.548
Elapsed time for attention_prob_times_values (80x2048x2048x891): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x891): 84.315

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1476.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x892x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x892x2048): 75.847
Elapsed time for attention_prob_times_values (80x2048x2048x892): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x892): 87.052

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1493.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x893x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x893x2048): 74.051
Elapsed time for attention_prob_times_values (80x2048x2048x893): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x893): 82.553

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1439.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x894x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x894x2048): 78.368
Elapsed time for attention_prob_times_values (80x2048x2048x894): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x894): 87.081

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1522.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x895x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x895x2048): 74.965
Elapsed time for attention_prob_times_values (80x2048x2048x895): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x895): 84.489

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1468.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x896x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x896x2048): 85.117
Elapsed time for attention_prob_times_values (80x2048x2048x896): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x896): 95.745

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1667.195--------
Elapsed time for attention_key_query_prob (160x2048x546x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x546x2048): 74.334
Elapsed time for attention_prob_times_values (160x2048x2048x546): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x546): 66.467

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1567.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x547x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x547x2048): 73.426
Elapsed time for attention_prob_times_values (160x2048x2048x547): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x547): 64.680

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1538.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x548x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x548x2048): 74.934
Elapsed time for attention_prob_times_values (160x2048x2048x548): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x548): 66.525

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1579.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x549x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x549x2048): 73.308
Elapsed time for attention_prob_times_values (160x2048x2048x549): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x549): 65.288

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1550.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x550x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x550x2048): 73.931
Elapsed time for attention_prob_times_values (160x2048x2048x550): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x550): 68.543

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1599.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x551x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x551x2048): 68.681
Elapsed time for attention_prob_times_values (160x2048x2048x551): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x551): 65.861

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1514.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x552x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x552x2048): 74.859
Elapsed time for attention_prob_times_values (160x2048x2048x552): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x552): 75.540

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1696.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x553x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x553x2048): 70.378
Elapsed time for attention_prob_times_values (160x2048x2048x553): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x553): 65.470

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1533.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x554x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x554x2048): 72.559
Elapsed time for attention_prob_times_values (160x2048x2048x554): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x554): 68.472

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1595.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x555x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x555x2048): 71.044
Elapsed time for attention_prob_times_values (160x2048x2048x555): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x555): 62.701

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1086.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1587x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1587x2048): 86.045
Elapsed time for attention_prob_times_values (32x2048x2048x1587): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1587): 73.923

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1065.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1588x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1588x2048): 83.330
Elapsed time for attention_prob_times_values (32x2048x2048x1588): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1588): 79.570

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1091.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1589x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1589x2048): 86.394
Elapsed time for attention_prob_times_values (32x2048x2048x1589): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1589): 70.941

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1045.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1590x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1590x2048): 87.364
Elapsed time for attention_prob_times_values (32x2048x2048x1590): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1590): 76.987

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1098.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1591x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1591x2048): 82.705
Elapsed time for attention_prob_times_values (32x2048x2048x1591): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1591): 73.841

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1047.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1592x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1592x2048): 83.735
Elapsed time for attention_prob_times_values (32x2048x2048x1592): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1592): 82.837

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1119.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1593x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1593x2048): 86.138
Elapsed time for attention_prob_times_values (32x2048x2048x1593): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1593): 71.679

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1052.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1594x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1594x2048): 87.260
Elapsed time for attention_prob_times_values (32x2048x2048x1594): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1594): 76.752

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1098.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1595x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1595x2048): 82.162
Elapsed time for attention_prob_times_values (32x2048x2048x1595): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1595): 74.060

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1048.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1596x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1596x2048): 87.788
Elapsed time for attention_prob_times_values (32x2048x2048x1596): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1596): 77.766

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1110.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1597x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1597x2048): 84.483
Elapsed time for attention_prob_times_values (32x2048x2048x1597): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1597): 71.070

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1040.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1598x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1598x2048): 87.360
Elapsed time for attention_prob_times_values (32x2048x2048x1598): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1598): 79.776

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1124.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1599x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1599x2048): 86.579
Elapsed time for attention_prob_times_values (32x2048x2048x1599): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1599): 74.206

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1078.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1600x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1600x2048): 93.991
Elapsed time for attention_prob_times_values (32x2048x2048x1600): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1600): 86.119

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 1213.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1601x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1601x2048): 84.755
Elapsed time for attention_prob_times_values (32x2048x2048x1601): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1601): 71.638

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1048.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1602x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1602x2048): 89.133
Elapsed time for attention_prob_times_values (32x2048x2048x1602): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1602): 80.054

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1140.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1603x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1603x2048): 85.797
Elapsed time for attention_prob_times_values (32x2048x2048x1603): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1603): 74.383

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1077.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1604x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1604x2048): 85.838
Elapsed time for attention_prob_times_values (32x2048x2048x1604): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1604): 80.350

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1123.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1605x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1605x2048): 83.068

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x897x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x897x2048): 77.089
Elapsed time for attention_prob_times_values (80x2048x2048x897): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x897): 74.003

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1398.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x898x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x898x2048): 77.879
Elapsed time for attention_prob_times_values (80x2048x2048x898): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x898): 77.004

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1435.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x899x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x899x2048): 78.875
Elapsed time for attention_prob_times_values (80x2048x2048x899): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x899): 76.602

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1442.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x900x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x900x2048): 78.524
Elapsed time for attention_prob_times_values (80x2048x2048x900): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x900): 76.964

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1444.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x901x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x901x2048): 76.589
Elapsed time for attention_prob_times_values (80x2048x2048x901): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x901): 74.118

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1401.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x902x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x902x2048): 76.159
Elapsed time for attention_prob_times_values (80x2048x2048x902): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x902): 77.159

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1427.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x903x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x903x2048): 76.938
Elapsed time for attention_prob_times_values (80x2048x2048x903): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x903): 76.625

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1430.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x904x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x904x2048): 80.822
Elapsed time for attention_prob_times_values (80x2048x2048x904): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x904): 84.623

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1542.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x905x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x905x2048): 78.799
Elapsed time for attention_prob_times_values (80x2048x2048x905): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x905): 76.966

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1454.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (32x2048x2048x1605): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1605): 74.475

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1063.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1606x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1606x2048): 88.576
Elapsed time for attention_prob_times_values (32x2048x2048x1606): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1606): 77.392

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1119.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1607x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1607x2048): 87.589
Elapsed time for attention_prob_times_values (32x2048x2048x1607): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1607): 72.757

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1077.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1608x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1608x2048): 85.386
Elapsed time for attention_prob_times_values (32x2048x2048x1608): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1608): 86.044

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1162.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1609x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1609x2048): 86.836
Elapsed time for attention_prob_times_values (32x2048x2048x1609): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1609): 74.815

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1090.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1610x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1610x2048): 87.871
Elapsed time for attention_prob_times_values (32x2048x2048x1610): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1610): 77.502

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1118.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1611x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1611x2048): 87.132
Elapsed time for attention_prob_times_values (32x2048x2048x1611): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1611): 72.301

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1073.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1612x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1612x2048): 87.249
Elapsed time for attention_prob_times_values (32x2048x2048x1612): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1612): 80.862

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1140.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1613x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1613x2048): 87.207
Elapsed time for attention_prob_times_values (32x2048x2048x1613): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1613): 75.091

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1097.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1614x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1614x2048): 88.257
Elapsed time for attention_prob_times_values (32x2048x2048x1614): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1614): 80.786

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1148.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1510.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x556x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x556x2048): 71.814
Elapsed time for attention_prob_times_values (160x2048x2048x556): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x556): 68.303

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1590.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x557x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x557x2048): 71.448
Elapsed time for attention_prob_times_values (160x2048x2048x557): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x557): 65.987

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1561.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x558x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x558x2048): 72.505
Elapsed time for attention_prob_times_values (160x2048x2048x558): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x558): 68.924

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1611.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x559x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x559x2048): 72.532
Elapsed time for attention_prob_times_values (160x2048x2048x559): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x559): 66.735

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1587.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x560x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x560x2048): 72.751
Elapsed time for attention_prob_times_values (160x2048x2048x560): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x560): 78.501

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1727.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x561x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x561x2048): 71.115
Elapsed time for attention_prob_times_values (160x2048x2048x561): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x561): 65.921

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1567.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x562x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x562x2048): 71.658
Elapsed time for attention_prob_times_values (160x2048x2048x562): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x562): 68.767

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1610.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x563x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x563x2048): 69.579
Elapsed time for attention_prob_times_values (160x2048x2048x563): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x563): 65.766

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1554.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x564x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x564x2048): 71.790
Elapsed time for attention_prob_times_values (160x2048x2048x564): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x564): 68.758

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1617.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x283x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x283x2048): 57.133
Elapsed time for attention_prob_times_values (384x2048x2048x283): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x283): 58.509

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1591.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x284x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x284x2048): 58.283
Elapsed time for attention_prob_times_values (384x2048x2048x284): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x284): 63.543

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1679.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x285x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x285x2048): 57.238
Elapsed time for attention_prob_times_values (384x2048x2048x285): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x285): 61.595

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1644.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x286x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x286x2048): 56.083
Elapsed time for attention_prob_times_values (384x2048x2048x286): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x286): 63.622

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1658.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x287x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x287x2048): 57.997
Elapsed time for attention_prob_times_values (384x2048x2048x287): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x287): 59.773

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1642.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x288x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x288x2048): 76.983
Elapsed time for attention_prob_times_values (384x2048x2048x288): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x288): 81.770

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2220.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x289x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x289x2048): 60.096
Elapsed time for attention_prob_times_values (384x2048x2048x289): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x289): 61.730

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1710.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x290x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x290x2048): 61.578
Elapsed time for attention_prob_times_values (384x2048x2048x290): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x290): 64.133

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1771.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x291x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x291x2048): 60.518
Elapsed time for attention_prob_times_values (384x2048x2048x291): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x291): 62.214

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1735.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1615x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1615x2048): 87.449
Elapsed time for attention_prob_times_values (32x2048x2048x1615): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1615): 75.606

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1104.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1616x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1616x2048): 89.395
Elapsed time for attention_prob_times_values (32x2048x2048x1616): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1616): 87.474

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1204.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1617x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1617x2048): 86.798
Elapsed time for attention_prob_times_values (32x2048x2048x1617): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1617): 73.089

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1081.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1618x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1618x2048): 86.951
Elapsed time for attention_prob_times_values (32x2048x2048x1618): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1618): 81.008

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1144.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1619x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1619x2048): 86.284
Elapsed time for attention_prob_times_values (32x2048x2048x1619): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1619): 75.652

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1100.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1620x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1620x2048): 85.511
Elapsed time for attention_prob_times_values (32x2048x2048x1620): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1620): 81.162

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1137.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1621x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1621x2048): 86.789
Elapsed time for attention_prob_times_values (32x2048x2048x1621): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1621): 73.062

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1084.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1622x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1622x2048): 87.735
Elapsed time for attention_prob_times_values (32x2048x2048x1622): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1622): 77.940

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1128.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1623x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1623x2048): 87.066
Elapsed time for attention_prob_times_values (32x2048x2048x1623): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1623): 75.829

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1108.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 20, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x906x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x906x2048): 79.770
Elapsed time for attention_prob_times_values (80x2048x2048x906): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x906): 79.210

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1486.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x907x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x907x2048): 79.436
Elapsed time for attention_prob_times_values (80x2048x2048x907): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x907): 77.134

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1464.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x908x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x908x2048): 79.017
Elapsed time for attention_prob_times_values (80x2048x2048x908): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x908): 76.707

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1458.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x909x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x909x2048): 77.672
Elapsed time for attention_prob_times_values (80x2048x2048x909): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x909): 74.894

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1430.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x910x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x910x2048): 78.216
Elapsed time for attention_prob_times_values (80x2048x2048x910): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x910): 77.172

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1458.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x911x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x911x2048): 76.673
Elapsed time for attention_prob_times_values (80x2048x2048x911): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x911): 75.187

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1426.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x912x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x912x2048): 81.110
Elapsed time for attention_prob_times_values (80x2048x2048x912): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x912): 80.396

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1519.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x913x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x913x2048): 79.263
Elapsed time for attention_prob_times_values (80x2048x2048x913): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x913): 77.898

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1479.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x914x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x914x2048): 76.158
Elapsed time for attention_prob_times_values (80x2048x2048x914): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x914): 76.361

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1437.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x915x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x915x2048): 78.901
Elapsed time for attention_prob_times_values (80x2048x2048x915): 0.0082
Elapsed time for attention_key_query_prob (32x2048x1624x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1624x2048): 84.596
Elapsed time for attention_prob_times_values (32x2048x2048x1624): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1624): 84.424

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1156.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1625x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1625x2048): 86.534
Elapsed time for attention_prob_times_values (32x2048x2048x1625): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1625): 73.643

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1089.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1626x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1626x2048): 83.442
Elapsed time for attention_prob_times_values (32x2048x2048x1626): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1626): 81.336

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1128.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1627x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1627x2048): 86.653
Elapsed time for attention_prob_times_values (32x2048x2048x1627): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1627): 75.730

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1108.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1628x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1628x2048): 88.013
Elapsed time for attention_prob_times_values (32x2048x2048x1628): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1628): 81.339

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1159.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1629x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1629x2048): 86.326
Elapsed time for attention_prob_times_values (32x2048x2048x1629): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1629): 76.271

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1111.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1630x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1630x2048): 87.262
Elapsed time for attention_prob_times_values (32x2048x2048x1630): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1630): 81.486

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1157.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1631x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1631x2048): 86.866
Elapsed time for attention_prob_times_values (32x2048x2048x1631): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1631): 75.607

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1111.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1632x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1632x2048): 97.318
Elapsed time for attention_prob_times_values (32x2048x2048x1632): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1632): 89.510

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1282.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1633x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1633x2048): 88.522
Elapsed time for attention_prob_times_values (32x2048x2048x1633): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1633): 76.013

Attention duration (in seconds): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x915): 74.889

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1450.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x916x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x916x2048): 77.711
Elapsed time for attention_prob_times_values (80x2048x2048x916): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x916): 79.097

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1480.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x917x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x917x2048): 74.612
Elapsed time for attention_prob_times_values (80x2048x2048x917): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x917): 78.214

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1444.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x918x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x918x2048): 79.712
Elapsed time for attention_prob_times_values (80x2048x2048x918): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x918): 76.351

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1476.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x919x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x919x2048): 76.522
Elapsed time for attention_prob_times_values (80x2048x2048x919): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x919): 78.439

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1467.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x920x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x920x2048): 80.752
Elapsed time for attention_prob_times_values (80x2048x2048x920): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x920): 86.147

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1581.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x921x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x921x2048): 78.450
Elapsed time for attention_prob_times_values (80x2048x2048x921): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x921): 75.810

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1464.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x922x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x922x2048): 75.651
Elapsed time for attention_prob_times_values (80x2048x2048x922): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x922): 78.540

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1464.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x923x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x923x2048): 77.104
Elapsed time for attention_prob_times_values (80x2048x2048x923): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x923): 76.219

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1458.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x924x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x924x2048): 80.186
Elapsed time for attention_prob_times_values (80x2048x2048x924): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x924): 78.317

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1509.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x565x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x565x2048): 68.993
Elapsed time for attention_prob_times_values (160x2048x2048x565): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x565): 66.312

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1560.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x566x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x566x2048): 71.047
Elapsed time for attention_prob_times_values (160x2048x2048x566): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x566): 70.545

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1636.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x567x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x567x2048): 72.455
Elapsed time for attention_prob_times_values (160x2048x2048x567): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x567): 67.074

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1612.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x568x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x568x2048): 72.277
Elapsed time for attention_prob_times_values (160x2048x2048x568): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x568): 80.986

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1771.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x569x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x569x2048): 70.058
Elapsed time for attention_prob_times_values (160x2048x2048x569): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x569): 66.805

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1588.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x570x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x570x2048): 70.858
Elapsed time for attention_prob_times_values (160x2048x2048x570): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x570): 69.711

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1635.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x571x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x571x2048): 70.536
Elapsed time for attention_prob_times_values (160x2048x2048x571): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x571): 65.210

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1579.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x572x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x572x2048): 71.174
Elapsed time for attention_prob_times_values (160x2048x2048x572): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x572): 69.136

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1637.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x573x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x573x2048): 70.356
Elapsed time for attention_prob_times_values (160x2048x2048x573): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x573): 66.667

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1600.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x574x2048): 0.0109
Attention throughput (in TFLOP/s): 1125.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1634x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1634x2048): 89.931
Elapsed time for attention_prob_times_values (32x2048x2048x1634): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1634): 79.391

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1160.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1635x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1635x2048): 88.715
Elapsed time for attention_prob_times_values (32x2048x2048x1635): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1635): 75.590

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1124.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1636x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1636x2048): 89.634
Elapsed time for attention_prob_times_values (32x2048x2048x1636): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1636): 80.117

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1166.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1637x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1637x2048): 88.581
Elapsed time for attention_prob_times_values (32x2048x2048x1637): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1637): 76.098

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1128.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1638x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1638x2048): 88.648
Elapsed time for attention_prob_times_values (32x2048x2048x1638): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1638): 81.740

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1173.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1639x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1639x2048): 85.735
Elapsed time for attention_prob_times_values (32x2048x2048x1639): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1639): 76.053

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1112.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1640x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1640x2048): 89.228
Elapsed time for attention_prob_times_values (32x2048x2048x1640): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1640): 81.797

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1178.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1641x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1641x2048): 87.279
Elapsed time for attention_prob_times_values (32x2048x2048x1641): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1641): 74.017

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1107.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1642x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1642x2048): 88.907
Elapsed time for attention_prob_times_values (32x2048x2048x1642): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1642): 78.098

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1149.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1643x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1643x2048): 87.319
Elapsed time for attention_prob_times_values (32x2048x2048x1643): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1643): 72.697

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1097.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1644x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1644x2048): 88.772
Elapsed time for attention_prob_times_values (32x2048x2048x1644): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1644): 82.051

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1180.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1645x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1645x2048): 86.600
Elapsed time for attention_prob_times_values (32x2048x2048x1645): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1645): 76.410

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1124.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1646x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1646x2048): 87.831
Elapsed time for attention_prob_times_values (32x2048x2048x1646): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1646): 82.056

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1175.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1647x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1647x2048): 87.030
Elapsed time for attention_prob_times_values (32x2048x2048x1647): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1647): 74.323

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1111.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1648x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1648x2048): 89.637
Elapsed time for attention_prob_times_values (32x2048x2048x1648): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1648): 86.101

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1218.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1649x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1649x2048): 84.261
Elapsed time for attention_prob_times_values (32x2048x2048x1649): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1649): 76.281

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1111.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1650x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1650x2048): 84.236
Elapsed time for attention_prob_times_values (32x2048x2048x1650): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1650): 82.186

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1155.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1651x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1651x2048): 86.936
Elapsed time for attention_prob_times_values (32x2048x2048x1651): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1651): 73.583

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1107.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1652x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1652x2048): 86.206
========================================================================================================================
num_attention_heads: 20, hidden_size: 18500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x925x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x925x2048): 78.643
Elapsed time for attention_prob_times_values (80x2048x2048x925): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x925): 78.612

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1499.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x926x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x926x2048): 77.313
Elapsed time for attention_prob_times_values (80x2048x2048x926): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x926): 79.441

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1495.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x927x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x927x2048): 78.961
Elapsed time for attention_prob_times_values (80x2048x2048x927): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x927): 77.861

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1498.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x928x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x928x2048): 92.310
Elapsed time for attention_prob_times_values (80x2048x2048x928): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x928): 87.542

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1718.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x929x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x929x2048): 79.599
Elapsed time for attention_prob_times_values (80x2048x2048x929): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x929): 76.470

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1493.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x930x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x930x2048): 81.132
Elapsed time for attention_prob_times_values (80x2048x2048x930): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x930): 79.423

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1538.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x931x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x931x2048): 77.751
Elapsed time for attention_prob_times_values (80x2048x2048x931): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x931): 75.618

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1470.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x932x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x932x2048): 79.681
Elapsed time for attention_prob_times_values (80x2048x2048x932): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x932): 79.243

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1525.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x933x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x933x2048): 80.360
Elapsed time for attention_prob_times_values (80x2048x2048x933): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x933): 78.480

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1526.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x934x2048): 0.0080
Elapsed time for attention_prob_times_values (32x2048x2048x1652): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1652): 82.464

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1172.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1653x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1653x2048): 87.033
Elapsed time for attention_prob_times_values (32x2048x2048x1653): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1653): 73.626

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1109.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1654x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1654x2048): 86.395
Elapsed time for attention_prob_times_values (32x2048x2048x1654): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1654): 79.559

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1153.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1655x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1655x2048): 85.264
Elapsed time for attention_prob_times_values (32x2048x2048x1655): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1655): 85.171

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1187.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1656x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1656x2048): 88.739
Elapsed time for attention_prob_times_values (32x2048x2048x1656): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1656): 77.205

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1150.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1657x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1657x2048): 83.402
Elapsed time for attention_prob_times_values (32x2048x2048x1657): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1657): 86.271

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1182.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1658x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1658x2048): 87.268
Elapsed time for attention_prob_times_values (32x2048x2048x1658): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1658): 88.948

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1229.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1659x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1659x2048): 86.684
Elapsed time for attention_prob_times_values (32x2048x2048x1659): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1659): 84.214

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1192.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1660x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1660x2048): 87.858
Elapsed time for attention_prob_times_values (32x2048x2048x1660): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1660): 88.987

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1235.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1661x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1661x2048): 86.628
Elapsed time for attention_prob_times_values (32x2048x2048x1661): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1661): 81.768

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1175.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
--------
Elapsed time for attention_key_query_prob (384x2048x292x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x292x2048): 61.836
Elapsed time for attention_prob_times_values (384x2048x2048x292): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x292): 63.037

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1771.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x293x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x293x2048): 58.321
Elapsed time for attention_prob_times_values (384x2048x2048x293): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x293): 63.284

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1728.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x294x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x294x2048): 61.452
Elapsed time for attention_prob_times_values (384x2048x2048x294): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x294): 63.289

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1781.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x295x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x295x2048): 60.351
Elapsed time for attention_prob_times_values (384x2048x2048x295): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x295): 64.072

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1781.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x296x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x296x2048): 61.643
Elapsed time for attention_prob_times_values (384x2048x2048x296): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x296): 58.344

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 1723.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x297x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x297x2048): 60.031
Elapsed time for attention_prob_times_values (384x2048x2048x297): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x297): 63.567

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1781.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x298x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x298x2048): 60.275
Elapsed time for attention_prob_times_values (384x2048x2048x298): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x298): 65.736

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1819.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x299x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x299x2048): 59.660
Elapsed time for attention_prob_times_values (384x2048x2048x299): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x299): 63.435

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1785.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x300x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x300x2048): 61.681
Elapsed time for attention_prob_times_values (384x2048x2048x300): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x300): 67.534

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1877.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x301x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x301x2048): 60.631
Elapsed time for attention_prob_times_values (384x2048x2048x301): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x301): 64.553

Attention duration (in seconds): 0.0310
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x574x2048): 70.771
Elapsed time for attention_prob_times_values (160x2048x2048x574): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x574): 68.704

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1633.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x575x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x575x2048): 71.897
Elapsed time for attention_prob_times_values (160x2048x2048x575): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x575): 67.708

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1636.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x576x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x576x2048): 87.536
Elapsed time for attention_prob_times_values (160x2048x2048x576): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x576): 84.985

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2026.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x577x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x577x2048): 71.911
Elapsed time for attention_prob_times_values (160x2048x2048x577): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x577): 61.940

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1566.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x578x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x578x2048): 74.267
Elapsed time for attention_prob_times_values (160x2048x2048x578): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x578): 64.016

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1621.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x579x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x579x2048): 72.924
Elapsed time for attention_prob_times_values (160x2048x2048x579): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x579): 62.431

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1588.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x580x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x580x2048): 75.727
Elapsed time for attention_prob_times_values (160x2048x2048x580): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x580): 36.980

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1175.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x581x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x581x2048): 73.076
Elapsed time for attention_prob_times_values (160x2048x2048x581): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x581): 61.128

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1577.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x582x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x582x2048): 73.182
Elapsed time for attention_prob_times_values (160x2048x2048x582): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x582): 63.461

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1613.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x583x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x583x2048): 75.835
Elapsed time for attention_prob_times_values (160x2048x2048x583): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x583): 61.516

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1614.910
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x934x2048): 77.866
Elapsed time for attention_prob_times_values (80x2048x2048x934): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x934): 78.599

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1505.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x935x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x935x2048): 79.233
Elapsed time for attention_prob_times_values (80x2048x2048x935): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x935): 77.905

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1513.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x936x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x936x2048): 82.072
Elapsed time for attention_prob_times_values (80x2048x2048x936): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x936): 84.583

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1606.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x937x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x937x2048): 79.233
Elapsed time for attention_prob_times_values (80x2048x2048x937): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x937): 79.268

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1529.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x938x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x938x2048): 76.986
Elapsed time for attention_prob_times_values (80x2048x2048x938): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x938): 79.329

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1509.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x939x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x939x2048): 77.300
Elapsed time for attention_prob_times_values (80x2048x2048x939): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x939): 76.813

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1490.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x940x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x940x2048): 79.014
Elapsed time for attention_prob_times_values (80x2048x2048x940): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x940): 81.622

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1554.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x941x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x941x2048): 78.713
Elapsed time for attention_prob_times_values (80x2048x2048x941): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x941): 79.578

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1533.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x942x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x942x2048): 80.649
Elapsed time for attention_prob_times_values (80x2048x2048x942): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x942): 80.954

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1567.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x943x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x943x2048): 79.642
Elapsed time for attention_prob_times_values (80x2048x2048x943): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x943): 79.394

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1544.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1662x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1662x2048): 83.646
Elapsed time for attention_prob_times_values (32x2048x2048x1662): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1662): 89.031

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1206.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1663x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1663x2048): 82.969
Elapsed time for attention_prob_times_values (32x2048x2048x1663): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1663): 86.676

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1186.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1664x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1664x2048): 95.271
Elapsed time for attention_prob_times_values (32x2048x2048x1664): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1664): 92.830

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 1316.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1665x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1665x2048): 87.428
Elapsed time for attention_prob_times_values (32x2048x2048x1665): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1665): 75.186

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1132.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1666x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1666x2048): 88.692
Elapsed time for attention_prob_times_values (32x2048x2048x1666): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1666): 79.935

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1178.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1667x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1667x2048): 84.229
Elapsed time for attention_prob_times_values (32x2048x2048x1667): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1667): 81.917

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1164.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1668x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1668x2048): 88.610
Elapsed time for attention_prob_times_values (32x2048x2048x1668): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1668): 84.317

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1212.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1669x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1669x2048): 87.719
Elapsed time for attention_prob_times_values (32x2048x2048x1669): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1669): 80.236

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1176.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1670x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1670x2048): 88.796
Elapsed time for attention_prob_times_values (32x2048x2048x1670): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1670): 84.244

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1214.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1671x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1671x2048): 85.763
Elapsed time for attention_prob_times_values (32x2048x2048x1671): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1671): 78.202

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1149.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1672x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1672x2048): 89.626
Elapsed time for attention_prob_times_values (32x2048x2048x1672): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1672): 82.526

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1208.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1673x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1673x2048): 87.274
Elapsed time for attention_prob_times_values (32x2048x2048x1673): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1673): 80.367

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1177.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1674x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1674x2048): 88.151
Elapsed time for attention_prob_times_values (32x2048x2048x1674): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1674): 84.480

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1214.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1675x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1675x2048): 87.156
Elapsed time for attention_prob_times_values (32x2048x2048x1675): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1675): 82.224

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1191.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1676x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1676x2048): 88.555
Elapsed time for attention_prob_times_values (32x2048x2048x1676): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1676): 80.264

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1186.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1677x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1677x2048): 87.337
Elapsed time for attention_prob_times_values (32x2048x2048x1677): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1677): 82.290

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1194.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1678x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1678x2048): 84.380
Elapsed time for attention_prob_times_values (32x2048x2048x1678): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1678): 81.458

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1169.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1679x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1679x2048): 87.473
Elapsed time for attention_prob_times_values (32x2048x2048x1679): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1679): 82.298

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1197.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1680x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1680x2048): 89.811
Elapsed time for attention_prob_times_values (32x2048x2048x1680): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1680): 84.052

Attention duration (in seconds): 0.0104
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x944x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x944x2048): 75.996
Elapsed time for attention_prob_times_values (80x2048x2048x944): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x944): 83.209

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1544.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x945x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x945x2048): 77.084
Elapsed time for attention_prob_times_values (80x2048x2048x945): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x945): 77.640

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1505.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x946x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x946x2048): 80.281
Elapsed time for attention_prob_times_values (80x2048x2048x946): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x946): 80.121

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1562.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x947x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x947x2048): 79.344
Elapsed time for attention_prob_times_values (80x2048x2048x947): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x947): 77.245

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1526.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x948x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x948x2048): 81.062
Elapsed time for attention_prob_times_values (80x2048x2048x948): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x948): 82.213

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1593.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x949x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x949x2048): 79.637
Elapsed time for attention_prob_times_values (80x2048x2048x949): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x949): 76.940

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1528.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x950x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x950x2048): 80.334
Elapsed time for attention_prob_times_values (80x2048x2048x950): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x950): 82.201

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1588.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x951x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x951x2048): 79.544
Elapsed time for attention_prob_times_values (80x2048x2048x951): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x951): 76.248

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1524.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x952x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x952x2048): 77.874
Elapsed time for attention_prob_times_values (80x2048x2048x952): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x952): 88.806

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1625.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


EstimateMLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x584x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x584x2048): 74.761
Elapsed time for attention_prob_times_values (160x2048x2048x584): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x584): 83.153

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1874.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x585x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x585x2048): 70.972
Elapsed time for attention_prob_times_values (160x2048x2048x585): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x585): 62.806

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1589.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x586x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x586x2048): 72.398
Elapsed time for attention_prob_times_values (160x2048x2048x586): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x586): 64.015

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1623.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x587x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x587x2048): 70.200
Elapsed time for attention_prob_times_values (160x2048x2048x587): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x587): 60.538

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1555.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x588x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x588x2048): 72.677
Elapsed time for attention_prob_times_values (160x2048x2048x588): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x588): 63.352

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1622.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x589x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x589x2048): 72.438
Elapsed time for attention_prob_times_values (160x2048x2048x589): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x589): 63.206

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1620.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x590x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x590x2048): 74.142
Elapsed time for attention_prob_times_values (160x2048x2048x590): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x590): 65.985

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1679.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x591x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x591x2048): 73.351
Elapsed time for attention_prob_times_values (160x2048x2048x591): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x591): 61.505

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1611.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x592x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x592x2048): 74.054
Elapsed time for attention_prob_times_values (160x2048x2048x592): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x592): 86.044

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1920.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 1226.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1681x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1681x2048): 86.544
Elapsed time for attention_prob_times_values (32x2048x2048x1681): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1681): 82.482

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1193.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1682x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1682x2048): 85.397
Elapsed time for attention_prob_times_values (32x2048x2048x1682): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1682): 84.861

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1203.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1683x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1683x2048): 84.969
Elapsed time for attention_prob_times_values (32x2048x2048x1683): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1683): 81.186

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1174.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1684x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1684x2048): 88.413
Elapsed time for attention_prob_times_values (32x2048x2048x1684): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1684): 84.951

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1226.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1685x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1685x2048): 86.993
Elapsed time for attention_prob_times_values (32x2048x2048x1685): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1685): 82.722

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1201.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1686x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1686x2048): 87.647
Elapsed time for attention_prob_times_values (32x2048x2048x1686): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1686): 84.691

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1220.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1687x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1687x2048): 87.108
Elapsed time for attention_prob_times_values (32x2048x2048x1687): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1687): 82.787

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1203.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1688x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1688x2048): 89.076
Elapsed time for attention_prob_times_values (32x2048x2048x1688): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1688): 81.205

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1205.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1689x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1689x2048): 86.667
Elapsed time for attention_prob_times_values (32x2048x2048x1689): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1689): 82.813

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1202.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1690x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1690x2048): 87.480
Elapsed time for attention_prob_times_values (32x2048x2048x1690): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1690): 80.564

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1191.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1691x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1691x2048): 86.726
Elapsed time for attention_prob_times_values (32x2048x2048x1691): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1691): 77.961

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1166.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1692x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1692x2048): 88.085
Elapsed time for attention_prob_times_values (32x2048x2048x1692): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1692): 85.217

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1231.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1693x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1693x2048): 82.031
Elapsed time for attention_prob_times_values (32x2048x2048x1693): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1693): 80.303

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1154.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1694x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1694x2048): 83.008
Elapsed time for attention_prob_times_values (32x2048x2048x1694): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1694): 85.108

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1196.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1695x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1695x2048): 86.943
Elapsed time for attention_prob_times_values (32x2048x2048x1695): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1695): 79.503

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1182.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1696x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1696x2048): 97.142
Elapsed time for attention_prob_times_values (32x2048x2048x1696): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1696): 82.635

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1272.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1697x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1697x2048): 88.061
Elapsed time for attention_prob_times_values (32x2048x2048x1697): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1697): 83.025

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1218.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1698x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1698x2048): 85.492
Elapsed time for attention_prob_times_values (32x2048x2048x1698): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1698): 85.324

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1218.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1699x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1699x2048): 83.613

--------
Elapsed time for attention_key_query_prob (80x2048x953x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x953x2048): 79.252
Elapsed time for attention_prob_times_values (80x2048x2048x953): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x953): 78.096

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1542.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x954x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x954x2048): 79.926
Elapsed time for attention_prob_times_values (80x2048x2048x954): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x954): 79.142

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1561.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x955x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x955x2048): 79.325
Elapsed time for attention_prob_times_values (80x2048x2048x955): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x955): 79.496

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1560.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x956x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x956x2048): 78.331
Elapsed time for attention_prob_times_values (80x2048x2048x956): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x956): 82.095

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1577.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x957x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x957x2048): 79.332
Elapsed time for attention_prob_times_values (80x2048x2048x957): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x957): 77.790

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1546.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x958x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x958x2048): 78.924
Elapsed time for attention_prob_times_values (80x2048x2048x958): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x958): 80.763

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1573.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x959x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x959x2048): 77.869
Elapsed time for attention_prob_times_values (80x2048x2048x959): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x959): 79.491

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1552.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x960x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x960x2048): 92.527
Elapsed time for attention_prob_times_values (80x2048x2048x960): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x960): 90.192

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1804.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x961x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x961x2048): 81.033
Elapsed time for attention_prob_times_values (80x2048x2048x961): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x961): 80.709

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1598.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x962x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x962x2048): 80.243
Elapsed time for attention_prob_times_values (80x2048x2048x962): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x962): 81.745

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1602.650Attention throughput (in TFLOP/s): 1827.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x302x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x302x2048): 59.723
Elapsed time for attention_prob_times_values (384x2048x2048x302): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x302): 68.442

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1869.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x303x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x303x2048): 60.494
Elapsed time for attention_prob_times_values (384x2048x2048x303): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x303): 65.738

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1852.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x304x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x304x2048): 59.686
Elapsed time for attention_prob_times_values (384x2048x2048x304): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x304): 80.119

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 2018.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x305x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x305x2048): 60.352
Elapsed time for attention_prob_times_values (384x2048x2048x305): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x305): 65.542

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1859.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x306x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x306x2048): 59.675
Elapsed time for attention_prob_times_values (384x2048x2048x306): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x306): 68.484

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1893.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x307x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x307x2048): 58.174
Elapsed time for attention_prob_times_values (384x2048x2048x307): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x307): 67.019

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 1854.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x308x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x308x2048): 62.294
Elapsed time for attention_prob_times_values (384x2048x2048x308): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x308): 68.640

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1951.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x309x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x309x2048): 59.545
Elapsed time for attention_prob_times_values (384x2048x2048x309): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x309): 65.589

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 1870.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x310x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x310x2048): 61.679
Elapsed time for attention_prob_times_values (384x2048x2048x310): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x310): 68.603

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1952.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (32x2048x2048x1699): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1699): 83.119

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1189.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1700x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1700x2048): 89.932
Elapsed time for attention_prob_times_values (32x2048x2048x1700): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1700): 81.824

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1223.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1701x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1701x2048): 87.867
Elapsed time for attention_prob_times_values (32x2048x2048x1701): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1701): 83.015

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1219.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1702x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1702x2048): 85.732
Elapsed time for attention_prob_times_values (32x2048x2048x1702): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1702): 85.491

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1223.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1703x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1703x2048): 87.773
Elapsed time for attention_prob_times_values (32x2048x2048x1703): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1703): 83.066

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1220.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1704x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1704x2048): 89.697
Elapsed time for attention_prob_times_values (32x2048x2048x1704): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1704): 78.148

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1195.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1705x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1705x2048): 84.440
Elapsed time for attention_prob_times_values (32x2048x2048x1705): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1705): 83.246

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1200.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1706x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1706x2048): 83.266
Elapsed time for attention_prob_times_values (32x2048x2048x1706): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1706): 81.562

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1180.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1707x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1707x2048): 87.177
Elapsed time for attention_prob_times_values (32x2048x2048x1707): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1707): 83.081

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1219.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1708x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1708x2048): 86.000
Elapsed time for attention_prob_times_values (32x2048x2048x1708): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1708): 85.823

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1232.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
num_attention_heads: 40, hidden_size: 23720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x593x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x593x2048): 72.698
Elapsed time for attention_prob_times_values (160x2048x2048x593): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x593): 63.454

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1637.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x594x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x594x2048): 71.155
Elapsed time for attention_prob_times_values (160x2048x2048x594): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x594): 66.550

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1664.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x595x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x595x2048): 72.992
Elapsed time for attention_prob_times_values (160x2048x2048x595): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x595): 61.651

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1620.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x596x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x596x2048): 74.374
Elapsed time for attention_prob_times_values (160x2048x2048x596): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x596): 67.324

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1716.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x597x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x597x2048): 73.015
Elapsed time for attention_prob_times_values (160x2048x2048x597): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x597): 64.311

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1663.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x598x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x598x2048): 73.947
Elapsed time for attention_prob_times_values (160x2048x2048x598): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x598): 66.902

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1711.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x599x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x599x2048): 72.916
Elapsed time for attention_prob_times_values (160x2048x2048x599): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x599): 63.237

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1652.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x600x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x600x2048): 72.064
Elapsed time for attention_prob_times_values (160x2048x2048x600): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x600): 86.949

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1925.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x601x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x601x2048): 72.638
Elapsed time for attention_prob_times_values (160x2048x2048x601): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x601): 63.683

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1661.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x602x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x602x2048): 71.998

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x963x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x963x2048): 79.364
Elapsed time for attention_prob_times_values (80x2048x2048x963): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x963): 77.723

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1555.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x964x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x964x2048): 82.605
Elapsed time for attention_prob_times_values (80x2048x2048x964): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x964): 83.275

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1644.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x965x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x965x2048): 78.055
Elapsed time for attention_prob_times_values (80x2048x2048x965): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x965): 79.585

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1564.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x966x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x966x2048): 80.619
Elapsed time for attention_prob_times_values (80x2048x2048x966): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x966): 80.184

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1597.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x967x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x967x2048): 80.665
Elapsed time for attention_prob_times_values (80x2048x2048x967): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x967): 79.643

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1593.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x968x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x968x2048): 79.059
Elapsed time for attention_prob_times_values (80x2048x2048x968): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x968): 89.016

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1666.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x969x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x969x2048): 79.833
Elapsed time for attention_prob_times_values (80x2048x2048x969): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x969): 81.214

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1604.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x970x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x970x2048): 78.071
Elapsed time for attention_prob_times_values (80x2048x2048x970): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x970): 81.460

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1590.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x971x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x971x2048): 78.554
Elapsed time for attention_prob_times_values (80x2048x2048x971): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x971): 81.608

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1598.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1709x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1709x2048): 82.788
Elapsed time for attention_prob_times_values (32x2048x2048x1709): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1709): 83.377

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1192.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1710x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1710x2048): 84.673
Elapsed time for attention_prob_times_values (32x2048x2048x1710): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1710): 82.401

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1199.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1711x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1711x2048): 87.576
Elapsed time for attention_prob_times_values (32x2048x2048x1711): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1711): 83.118

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1225.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1712x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1712x2048): 90.228
Elapsed time for attention_prob_times_values (32x2048x2048x1712): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1712): 81.351

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1229.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1713x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1713x2048): 82.081
Elapsed time for attention_prob_times_values (32x2048x2048x1713): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1713): 83.384

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1189.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1714x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1714x2048): 84.219
Elapsed time for attention_prob_times_values (32x2048x2048x1714): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1714): 86.078

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1225.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1715x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1715x2048): 87.348
Elapsed time for attention_prob_times_values (32x2048x2048x1715): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1715): 83.626

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1230.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1716x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1716x2048): 88.573
Elapsed time for attention_prob_times_values (32x2048x2048x1716): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1716): 83.231

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1236.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1717x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1717x2048): 87.268
Elapsed time for attention_prob_times_values (32x2048x2048x1717): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1717): 81.232

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1212.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1718x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1718x2048): 84.756
Elapsed time for attention_prob_times_values (32x2048x2048x1718): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1718): 86.149

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1232.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1719x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1719x2048): 87.181
Elapsed time for attention_prob_times_values (32x2048x2048x1719): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1719): 83.752

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1232.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1720x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1720x2048): 89.655
Elapsed time for attention_prob_times_values (32x2048x2048x1720): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1720): 81.228

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1230.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1721x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1721x2048): 87.102
Elapsed time for attention_prob_times_values (32x2048x2048x1721): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1721): 81.425

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1215.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1722x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1722x2048): 87.967
Elapsed time for attention_prob_times_values (32x2048x2048x1722): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1722): 86.492

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1260.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1723x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1723x2048): 84.411
Elapsed time for attention_prob_times_values (32x2048x2048x1723): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1723): 84.012

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1217.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1724x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1724x2048): 84.907
Elapsed time for attention_prob_times_values (32x2048x2048x1724): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1724): 83.318

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1216.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1725x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1725x2048): 87.438
Elapsed time for attention_prob_times_values (32x2048x2048x1725): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1725): 84.084

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1241.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1726x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1726x2048): 87.234
Elapsed time for attention_prob_times_values (32x2048x2048x1726): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1726): 86.615

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1259.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1727x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1727x2048): 81.981
Elapsed time for attention_prob_times_values (32x2048x2048x1727): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1727): 80.093

Attention duration (in seconds): 0.0114
num_attention_heads: 20, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x972x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x972x2048): 79.577
Elapsed time for attention_prob_times_values (80x2048x2048x972): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x972): 81.736

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1611.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x973x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x973x2048): 78.691
Elapsed time for attention_prob_times_values (80x2048x2048x973): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x973): 81.869

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1605.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x974x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x974x2048): 81.215
Elapsed time for attention_prob_times_values (80x2048x2048x974): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x974): 84.030

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1653.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x975x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x975x2048): 80.243
Elapsed time for attention_prob_times_values (80x2048x2048x975): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x975): 78.600

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1591.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x976x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x976x2048): 80.285
Elapsed time for attention_prob_times_values (80x2048x2048x976): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x976): 88.981

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1693.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x977x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x977x2048): 79.644
Elapsed time for attention_prob_times_values (80x2048x2048x977): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x977): 82.230

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1624.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x978x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x978x2048): 80.548
Elapsed time for attention_prob_times_values (80x2048x2048x978): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x978): 79.617

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1609.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x979x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x979x2048): 79.249
Elapsed time for attention_prob_times_values (80x2048x2048x979): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x979): 82.425

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1625.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x980x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x980x2048): 78.719
Elapsed time for attention_prob_times_values (80x2048x2048x980): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x980): 83.011

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1627.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x981x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x981x2048): 79.870
Elapsed time for attention_prob_times_values (80x2048x2048x981): 0.0083
Elapsed time for attention_prob_times_values (160x2048x2048x602): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x602): 62.488

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1640.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x603x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x603x2048): 70.190
Elapsed time for attention_prob_times_values (160x2048x2048x603): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x603): 63.242

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1633.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x604x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x604x2048): 70.472
Elapsed time for attention_prob_times_values (160x2048x2048x604): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x604): 66.408

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1681.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x605x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x605x2048): 72.868
Elapsed time for attention_prob_times_values (160x2048x2048x605): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x605): 63.478

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1671.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x606x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x606x2048): 73.593
Elapsed time for attention_prob_times_values (160x2048x2048x606): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x606): 67.151

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1732.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x607x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x607x2048): 72.914
Elapsed time for attention_prob_times_values (160x2048x2048x607): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x607): 64.107

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1685.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x608x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x608x2048): 88.782
Elapsed time for attention_prob_times_values (160x2048x2048x608): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x608): 87.830

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2185.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x609x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x609x2048): 74.801
Elapsed time for attention_prob_times_values (160x2048x2048x609): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x609): 65.345

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1729.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x610x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x610x2048): 75.719
Elapsed time for attention_prob_times_values (160x2048x2048x610): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x610): 68.039

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1779.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x611x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x611x2048): 73.896
Elapsed time for attention_prob_times_values (160x2048x2048x611): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x611): 65.507

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1726.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1174.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1728x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1728x2048): 97.253
Elapsed time for attention_prob_times_values (32x2048x2048x1728): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1728): 85.889

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1322.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1729x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1729x2048): 88.647
Elapsed time for attention_prob_times_values (32x2048x2048x1729): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1729): 84.261

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1253.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1730x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1730x2048): 79.593
Elapsed time for attention_prob_times_values (32x2048x2048x1730): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1730): 86.859

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1205.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1731x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1731x2048): 80.167
Elapsed time for attention_prob_times_values (32x2048x2048x1731): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1731): 80.983

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1170.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1732x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1732x2048): 86.998
Elapsed time for attention_prob_times_values (32x2048x2048x1732): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1732): 87.087

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1264.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1733x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1733x2048): 88.553
Elapsed time for attention_prob_times_values (32x2048x2048x1733): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1733): 84.522

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1257.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1734x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1734x2048): 89.302
Elapsed time for attention_prob_times_values (32x2048x2048x1734): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1734): 84.839

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1265.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1735x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1735x2048): 88.387
Elapsed time for attention_prob_times_values (32x2048x2048x1735): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1735): 82.853

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1244.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1736x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1736x2048): 84.347
Elapsed time for attention_prob_times_values (32x2048x2048x1736): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1736): 85.696

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1238.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 96, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x311x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x311x2048): 60.549
Elapsed time for attention_prob_times_values (384x2048x2048x311): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x311): 66.291

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1908.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x312x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x312x2048): 62.789
Elapsed time for attention_prob_times_values (384x2048x2048x312): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x312): 85.919

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2194.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x313x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x313x2048): 60.394
Elapsed time for attention_prob_times_values (384x2048x2048x313): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x313): 65.753

Attention duration (in seconds): 0.0320
Attention throughput (in TFLOP/s): 1910.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x314x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x314x2048): 61.070
Elapsed time for attention_prob_times_values (384x2048x2048x314): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x314): 67.615

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1953.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x315x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x315x2048): 61.044
Elapsed time for attention_prob_times_values (384x2048x2048x315): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x315): 67.286

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1954.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x316x2048): 0.0169
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x316x2048): 60.353
Elapsed time for attention_prob_times_values (384x2048x2048x316): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x316): 69.568

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1979.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x317x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x317x2048): 59.296
Elapsed time for attention_prob_times_values (384x2048x2048x317): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x317): 68.413

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 1951.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x318x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x318x2048): 61.664
Elapsed time for attention_prob_times_values (384x2048x2048x318): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x318): 68.260

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1996.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x319x2048): 0.0169
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x319x2048): 60.692
Elapsed time for attention_prob_times_values (384x2048x2048x319): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x319): 69.166

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 1998.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x320x2048): 0.0129
num_attention_heads: 8, hidden_size: 13896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1737x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1737x2048): 82.728
Elapsed time for attention_prob_times_values (32x2048x2048x1737): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1737): 80.763

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1190.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1738x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1738x2048): 88.728
Elapsed time for attention_prob_times_values (32x2048x2048x1738): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1738): 87.155

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1281.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1739x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1739x2048): 87.851
Elapsed time for attention_prob_times_values (32x2048x2048x1739): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1739): 84.811

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1258.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1740x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1740x2048): 89.052
Elapsed time for attention_prob_times_values (32x2048x2048x1740): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1740): 87.400

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1287.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1741x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1741x2048): 87.626
Elapsed time for attention_prob_times_values (32x2048x2048x1741): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1741): 84.923

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1259.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1742x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1742x2048): 88.812
Elapsed time for attention_prob_times_values (32x2048x2048x1742): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1742): 87.220

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1285.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1743x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1743x2048): 87.896
Elapsed time for attention_prob_times_values (32x2048x2048x1743): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1743): 83.776

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1253.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1744x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1744x2048): 90.159
Elapsed time for attention_prob_times_values (32x2048x2048x1744): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1744): 86.922

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1294.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1745x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1745x2048): 87.425
Elapsed time for attention_prob_times_values (32x2048x2048x1745): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1745): 85.122

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1262.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1746x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1746x2048): 85.237
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x981): 79.329

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1604.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x982x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x982x2048): 78.758
Elapsed time for attention_prob_times_values (80x2048x2048x982): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x982): 84.598

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1646.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x983x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x983x2048): 78.538
Elapsed time for attention_prob_times_values (80x2048x2048x983): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x983): 82.656

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1626.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x984x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x984x2048): 80.463
Elapsed time for attention_prob_times_values (80x2048x2048x984): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x984): 89.013

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1708.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x985x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x985x2048): 78.600
Elapsed time for attention_prob_times_values (80x2048x2048x985): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x985): 81.230

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1616.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x986x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x986x2048): 77.832
Elapsed time for attention_prob_times_values (80x2048x2048x986): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x986): 84.914

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1645.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x987x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x987x2048): 77.231
Elapsed time for attention_prob_times_values (80x2048x2048x987): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x987): 82.326

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1616.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x988x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x988x2048): 78.492
Elapsed time for attention_prob_times_values (80x2048x2048x988): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x988): 84.770

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1654.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x989x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x989x2048): 79.774
Elapsed time for attention_prob_times_values (80x2048x2048x989): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x989): 79.966

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1622.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x990x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x990x2048): 77.554
Elapsed time for attention_prob_times_values (80x2048x2048x990): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x990): 85.196

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1651.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x612x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x612x2048): 75.964
Elapsed time for attention_prob_times_values (160x2048x2048x612): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x612): 66.728

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1769.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x613x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x613x2048): 74.249
Elapsed time for attention_prob_times_values (160x2048x2048x613): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x613): 65.581

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1737.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x614x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x614x2048): 73.769
Elapsed time for attention_prob_times_values (160x2048x2048x614): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x614): 67.784

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1765.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x615x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x615x2048): 72.445
Elapsed time for attention_prob_times_values (160x2048x2048x615): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x615): 66.076

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1729.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x616x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x616x2048): 75.793
Elapsed time for attention_prob_times_values (160x2048x2048x616): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x616): 89.093

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2052.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x617x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x617x2048): 73.303
Elapsed time for attention_prob_times_values (160x2048x2048x617): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x617): 65.233

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1732.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x618x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x618x2048): 72.786
Elapsed time for attention_prob_times_values (160x2048x2048x618): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x618): 66.661

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1749.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x619x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x619x2048): 71.973
Elapsed time for attention_prob_times_values (160x2048x2048x619): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x619): 64.928

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1718.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x620x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x620x2048): 72.465
Elapsed time for attention_prob_times_values (160x2048x2048x620): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x620): 67.932

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1768.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_prob_times_values (32x2048x2048x1746): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1746): 87.486

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1264.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1747x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1747x2048): 85.445
Elapsed time for attention_prob_times_values (32x2048x2048x1747): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1747): 82.216

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1227.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1748x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1748x2048): 86.221
Elapsed time for attention_prob_times_values (32x2048x2048x1748): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1748): 80.834

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1222.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 13992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1749x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1749x2048): 87.703
Elapsed time for attention_prob_times_values (32x2048x2048x1749): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1749): 85.258

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1267.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1750x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1750x2048): 85.012
Elapsed time for attention_prob_times_values (32x2048x2048x1750): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1750): 87.613

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1266.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1751x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1751x2048): 83.735
Elapsed time for attention_prob_times_values (32x2048x2048x1751): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1751): 85.333

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1240.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1752x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1752x2048): 89.801
Elapsed time for attention_prob_times_values (32x2048x2048x1752): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1752): 75.734

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1206.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1753x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1753x2048): 87.429
Elapsed time for attention_prob_times_values (32x2048x2048x1753): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1753): 83.536

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1255.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1754x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1754x2048): 88.190
Elapsed time for attention_prob_times_values (32x2048x2048x1754): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1754): 87.708

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1293.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1755x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1755x2048): 87.374
Elapsed time for attention_prob_times_values (32x2048x2048x1755): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1755): 81.198

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1238.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
========================================================================================================================
num_attention_heads: 20, hidden_size: 19820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x991x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x991x2048): 76.937
Elapsed time for attention_prob_times_values (80x2048x2048x991): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x991): 83.093

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1626.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x992x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x992x2048): 90.633
Elapsed time for attention_prob_times_values (80x2048x2048x992): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x992): 89.323

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1833.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x993x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x993x2048): 77.339
Elapsed time for attention_prob_times_values (80x2048x2048x993): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x993): 80.913

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1612.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x994x2048): 0.0199
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x994x2048): 33.460
Elapsed time for attention_prob_times_values (80x2048x2048x994): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x994): 80.791

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 966.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x995x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x995x2048): 80.980
Elapsed time for attention_prob_times_values (80x2048x2048x995): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x995): 82.875

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1673.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x996x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x996x2048): 82.699
Elapsed time for attention_prob_times_values (80x2048x2048x996): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x996): 85.340

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1718.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x997x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x997x2048): 78.035
Elapsed time for attention_prob_times_values (80x2048x2048x997): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x997): 79.995

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1617.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x998x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x998x2048): 81.450
Elapsed time for attention_prob_times_values (80x2048x2048x998): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x998): 83.238

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1687.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x999x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x999x2048): 77.682
Elapsed time for attention_prob_times_values (80x2048x2048x999): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x999): 82.141

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1637.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1000x2048): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1756x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1756x2048): 88.702
Elapsed time for attention_prob_times_values (32x2048x2048x1756): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1756): 83.158

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1263.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1757x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1757x2048): 87.458
Elapsed time for attention_prob_times_values (32x2048x2048x1757): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1757): 85.581

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1273.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1758x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1758x2048): 88.501
Elapsed time for attention_prob_times_values (32x2048x2048x1758): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1758): 87.962

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1300.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1759x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1759x2048): 87.732
Elapsed time for attention_prob_times_values (32x2048x2048x1759): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1759): 85.682

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1278.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1760x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1760x2048): 95.104
Elapsed time for attention_prob_times_values (32x2048x2048x1760): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1760): 89.297

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1358.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1761x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1761x2048): 86.408
Elapsed time for attention_prob_times_values (32x2048x2048x1761): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1761): 85.606

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1269.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1762x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1762x2048): 91.035
Elapsed time for attention_prob_times_values (32x2048x2048x1762): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1762): 87.784

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1319.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1763x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1763x2048): 89.152
Elapsed time for attention_prob_times_values (32x2048x2048x1763): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1763): 83.651

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1275.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1764x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1764x2048): 89.560
Elapsed time for attention_prob_times_values (32x2048x2048x1764): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1764): 88.396

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1315.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1765x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1765x2048): 88.826
Elapsed time for attention_prob_times_values (32x2048x2048x1765): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1765): 85.711

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1290.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1766x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1766x2048): 89.913
Elapsed time for attention_prob_times_values (32x2048x2048x1766): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1766): 88.140

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1317.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1767x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1767x2048): 88.111
Elapsed time for attention_prob_times_values (32x2048x2048x1767): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1767): 85.916

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1288.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1768x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1768x2048): 90.297
Elapsed time for attention_prob_times_values (32x2048x2048x1768): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1768): 86.341

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1307.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1769x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1769x2048): 87.845
Elapsed time for attention_prob_times_values (32x2048x2048x1769): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1769): 85.662

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1285.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1770x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1770x2048): 88.796
Elapsed time for attention_prob_times_values (32x2048x2048x1770): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1770): 88.386

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1313.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1771x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1771x2048): 88.116
Elapsed time for attention_prob_times_values (32x2048x2048x1771): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1771): 85.973

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1291.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1772x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1772x2048): 84.865
Elapsed time for attention_prob_times_values (32x2048x2048x1772): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1772): 84.952

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1260.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1773x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1773x2048): 87.924
Elapsed time for attention_prob_times_values (32x2048x2048x1773): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1773): 86.040

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1291.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1774x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1774x2048): 88.965
Elapsed time for attention_prob_times_values (32x2048x2048x1774): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1774): 88.641

Attention duration (in seconds): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1000x2048): 79.019
Elapsed time for attention_prob_times_values (80x2048x2048x1000): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1000): 93.114

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1755.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1001x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1001x2048): 79.972
Elapsed time for attention_prob_times_values (80x2048x2048x1001): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1001): 78.787

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1631.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1002x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1002x2048): 78.867
Elapsed time for attention_prob_times_values (80x2048x2048x1002): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1002): 84.236

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1675.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1003x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1003x2048): 74.326
Elapsed time for attention_prob_times_values (80x2048x2048x1003): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1003): 81.652

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1602.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1004x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1004x2048): 81.906
Elapsed time for attention_prob_times_values (80x2048x2048x1004): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1004): 85.923

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1728.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1005x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1005x2048): 79.526
Elapsed time for attention_prob_times_values (80x2048x2048x1005): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1005): 83.909

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1684.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1006x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1006x2048): 80.992
Elapsed time for attention_prob_times_values (80x2048x2048x1006): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1006): 85.265

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1715.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1007x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1007x2048): 80.239
Elapsed time for attention_prob_times_values (80x2048x2048x1007): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1007): 83.479

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1691.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1008x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1008x2048): 82.428
Elapsed time for attention_prob_times_values (80x2048x2048x1008): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1008): 94.124

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1818.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1009x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1009x2048): 78.280
Elapsed time for attention_prob_times_values (80x2048x2048x1009): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1009): 83.417

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1672.440
MLP duration (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (160x2048x621x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x621x2048): 70.605
Elapsed time for attention_prob_times_values (160x2048x2048x621): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x621): 64.494

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1702.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x622x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x622x2048): 72.612
Elapsed time for attention_prob_times_values (160x2048x2048x622): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x622): 68.400

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1781.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x623x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x623x2048): 71.975
Elapsed time for attention_prob_times_values (160x2048x2048x623): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x623): 65.376

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1735.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x624x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x624x2048): 74.369
Elapsed time for attention_prob_times_values (160x2048x2048x624): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x624): 89.151

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2057.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x625x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x625x2048): 72.112
Elapsed time for attention_prob_times_values (160x2048x2048x625): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x625): 65.613

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1746.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x626x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x626x2048): 72.906
Elapsed time for attention_prob_times_values (160x2048x2048x626): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x626): 69.067

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1805.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x627x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x627x2048): 68.931
Elapsed time for attention_prob_times_values (160x2048x2048x627): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x627): 63.158

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1680.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x628x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x628x2048): 71.883
Elapsed time for attention_prob_times_values (160x2048x2048x628): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x628): 66.339

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1761.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x629x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x629x2048): 69.555
Elapsed time for attention_prob_times_values (160x2048x2048x629): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x629): 63.414

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1696.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x630x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x630x2048): 73.005
Elapsed time for attention_prob_times_values (160x2048x2048x630): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x630): 68.583

Attention duration (in seconds): 0.0239
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x320x2048): 79.968
Elapsed time for attention_prob_times_values (384x2048x2048x320): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x320): 88.832

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2609.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x321x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x321x2048): 64.161
Elapsed time for attention_prob_times_values (384x2048x2048x321): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x321): 59.084

Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 1912.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0336
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x322x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x322x2048): 65.225
Elapsed time for attention_prob_times_values (384x2048x2048x322): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x322): 62.551

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 1991.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x323x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x323x2048): 61.793
Elapsed time for attention_prob_times_values (384x2048x2048x323): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x323): 60.508

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 1912.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x324x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x324x2048): 66.015
Elapsed time for attention_prob_times_values (384x2048x2048x324): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x324): 63.528

Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 2031.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x325x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x325x2048): 64.112
Elapsed time for attention_prob_times_values (384x2048x2048x325): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x325): 60.738

Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 1963.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0336
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x326x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x326x2048): 63.643
Elapsed time for attention_prob_times_values (384x2048x2048x326): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x326): 63.669

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 2009.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x327x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x327x2048): 63.519
Elapsed time for attention_prob_times_values (384x2048x2048x327): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x327): 61.120

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 1972.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x328x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x328x2048): 65.526
Elapsed time for attention_prob_times_values (384x2048x2048x328): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x328): 76.749

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 2244.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x329x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x329x2048): 61.100
Elapsed time for attention_prob_times_values (384x2048x2048x329): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x329): 60.376

Attention duration (in seconds): 0.0349
Attention throughput (in TFLOP/s): 1934.061
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 1319.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1775x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1775x2048): 88.066
Elapsed time for attention_prob_times_values (32x2048x2048x1775): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1775): 86.148

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1294.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1776x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1776x2048): 90.508
Elapsed time for attention_prob_times_values (32x2048x2048x1776): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1776): 88.388

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1330.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1777x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1777x2048): 84.719
Elapsed time for attention_prob_times_values (32x2048x2048x1777): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1777): 81.304

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1234.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1778x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1778x2048): 88.916
Elapsed time for attention_prob_times_values (32x2048x2048x1778): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1778): 88.558

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1321.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1779x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1779x2048): 87.900
Elapsed time for attention_prob_times_values (32x2048x2048x1779): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1779): 86.257

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1297.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1780x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1780x2048): 89.437
Elapsed time for attention_prob_times_values (32x2048x2048x1780): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1780): 89.079

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1330.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1781x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1781x2048): 87.910
Elapsed time for attention_prob_times_values (32x2048x2048x1781): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1781): 86.496

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1300.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1782x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1782x2048): 86.321
Elapsed time for attention_prob_times_values (32x2048x2048x1782): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1782): 86.455

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1289.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1783x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1783x2048): 85.323
Elapsed time for attention_prob_times_values (32x2048x2048x1783): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1783): 83.246

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1258.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1010x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1010x2048): 80.563
Elapsed time for attention_prob_times_values (80x2048x2048x1010): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1010): 86.573

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1729.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1011x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1011x2048): 78.080
Elapsed time for attention_prob_times_values (80x2048x2048x1011): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1011): 82.833

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1667.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1012x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1012x2048): 80.572
Elapsed time for attention_prob_times_values (80x2048x2048x1012): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1012): 86.912

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1736.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1013x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1013x2048): 77.986
Elapsed time for attention_prob_times_values (80x2048x2048x1013): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1013): 82.951

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1670.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1014x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1014x2048): 79.443
Elapsed time for attention_prob_times_values (80x2048x2048x1014): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1014): 84.008

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1698.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1015x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1015x2048): 76.987
Elapsed time for attention_prob_times_values (80x2048x2048x1015): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1015): 82.986

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1663.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1016x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1016x2048): 82.025
Elapsed time for attention_prob_times_values (80x2048x2048x1016): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1016): 93.019

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1817.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1017x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1017x2048): 74.357
Elapsed time for attention_prob_times_values (80x2048x2048x1017): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1017): 84.497

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1650.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1018x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1018x2048): 76.825
Elapsed time for attention_prob_times_values (80x2048x2048x1018): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1018): 83.116

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1667.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1784x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1784x2048): 89.726
Elapsed time for attention_prob_times_values (32x2048x2048x1784): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1784): 84.666

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1301.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1785x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1785x2048): 86.022
Elapsed time for attention_prob_times_values (32x2048x2048x1785): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1785): 86.660

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1290.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1786x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1786x2048): 86.089
Elapsed time for attention_prob_times_values (32x2048x2048x1786): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1786): 85.883

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1285.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1787x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1787x2048): 87.660
Elapsed time for attention_prob_times_values (32x2048x2048x1787): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1787): 86.723

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1304.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1788x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1788x2048): 89.166
Elapsed time for attention_prob_times_values (32x2048x2048x1788): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1788): 89.487

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1337.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1789x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1789x2048): 82.507
Elapsed time for attention_prob_times_values (32x2048x2048x1789): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1789): 86.828

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1267.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1790x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1790x2048): 88.525
Elapsed time for attention_prob_times_values (32x2048x2048x1790): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1790): 89.337

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1332.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1791x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1791x2048): 87.510
Elapsed time for attention_prob_times_values (32x2048x2048x1791): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1791): 86.850

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1306.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1792x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1792x2048): 96.127
Elapsed time for attention_prob_times_values (32x2048x2048x1792): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1792): 89.086

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1387.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1793x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1793x2048): 86.308
Attention throughput (in TFLOP/s): 1811.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x631x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x631x2048): 72.308
Elapsed time for attention_prob_times_values (160x2048x2048x631): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x631): 65.318

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1760.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x632x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x632x2048): 74.446
Elapsed time for attention_prob_times_values (160x2048x2048x632): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x632): 91.412

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2107.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x633x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x633x2048): 72.249
Elapsed time for attention_prob_times_values (160x2048x2048x633): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x633): 67.472

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1795.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x634x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x634x2048): 73.519
Elapsed time for attention_prob_times_values (160x2048x2048x634): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x634): 69.583

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1842.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x635x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x635x2048): 72.854
Elapsed time for attention_prob_times_values (160x2048x2048x635): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x635): 64.216

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1761.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x636x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x636x2048): 72.583
Elapsed time for attention_prob_times_values (160x2048x2048x636): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x636): 68.943

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1827.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x637x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x637x2048): 71.832
Elapsed time for attention_prob_times_values (160x2048x2048x637): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x637): 66.491

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1787.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x638x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x638x2048): 73.692
Elapsed time for attention_prob_times_values (160x2048x2048x638): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x638): 69.717

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1857.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x639x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x639x2048): 72.317
Elapsed time for attention_prob_times_values (160x2048x2048x639): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x639): 62.913

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1746.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (32x2048x2048x1793): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1793): 82.766

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1268.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1794x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1794x2048): 86.441
Elapsed time for attention_prob_times_values (32x2048x2048x1794): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1794): 83.638

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1276.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1795x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1795x2048): 88.122
Elapsed time for attention_prob_times_values (32x2048x2048x1795): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1795): 83.103

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1285.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1796x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1796x2048): 90.229
Elapsed time for attention_prob_times_values (32x2048x2048x1796): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1796): 82.576

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1296.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1797x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1797x2048): 88.657
Elapsed time for attention_prob_times_values (32x2048x2048x1797): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1797): 82.349

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1284.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1798x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1798x2048): 89.487
Elapsed time for attention_prob_times_values (32x2048x2048x1798): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1798): 83.119

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1296.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1799x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1799x2048): 87.794
Elapsed time for attention_prob_times_values (32x2048x2048x1799): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1799): 80.997

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1268.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1800x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1800x2048): 90.200
Elapsed time for attention_prob_times_values (32x2048x2048x1800): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1800): 83.383

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1305.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1801x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1801x2048): 88.255
Elapsed time for attention_prob_times_values (32x2048x2048x1801): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1801): 83.210

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1290.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1802x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1802x2048): 88.986
Elapsed time for attention_prob_times_values (32x2048x2048x1802): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1802): 85.614

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1315.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
num_attention_heads: 20, hidden_size: 20380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1019x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1019x2048): 80.424
Elapsed time for attention_prob_times_values (80x2048x2048x1019): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1019): 82.349

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1700.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1020x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1020x2048): 85.275
Elapsed time for attention_prob_times_values (80x2048x2048x1020): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1020): 87.125

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1803.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1021x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1021x2048): 78.243
Elapsed time for attention_prob_times_values (80x2048x2048x1021): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1021): 80.210

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1658.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1022x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1022x2048): 78.811
Elapsed time for attention_prob_times_values (80x2048x2048x1022): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1022): 85.071

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1715.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1023x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1023x2048): 79.678
Elapsed time for attention_prob_times_values (80x2048x2048x1023): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1023): 83.410

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1709.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1024x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1024x2048): 91.183
Elapsed time for attention_prob_times_values (80x2048x2048x1024): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1024): 96.456

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1968.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1025x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1025x2048): 79.401
Elapsed time for attention_prob_times_values (80x2048x2048x1025): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1025): 76.930

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1642.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1026x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1026x2048): 80.671
Elapsed time for attention_prob_times_values (80x2048x2048x1026): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1026): 78.215

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1671.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1027x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1027x2048): 80.493
Elapsed time for attention_prob_times_values (80x2048x2048x1027): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1027): 75.940

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1645.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1028x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1028x2048): 87.437
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1803x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1803x2048): 86.050
Elapsed time for attention_prob_times_values (32x2048x2048x1803): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1803): 83.450

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1278.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1804x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1804x2048): 89.595
Elapsed time for attention_prob_times_values (32x2048x2048x1804): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1804): 85.839

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1323.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1805x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1805x2048): 87.946
Elapsed time for attention_prob_times_values (32x2048x2048x1805): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1805): 83.224

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1291.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1806x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1806x2048): 89.019
Elapsed time for attention_prob_times_values (32x2048x2048x1806): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1806): 85.634

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1318.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1807x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1807x2048): 88.242
Elapsed time for attention_prob_times_values (32x2048x2048x1807): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1807): 83.576

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1297.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1808x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1808x2048): 90.722
Elapsed time for attention_prob_times_values (32x2048x2048x1808): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1808): 85.373

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1330.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1809x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1809x2048): 87.753
Elapsed time for attention_prob_times_values (32x2048x2048x1809): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1809): 83.772

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1297.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1810x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1810x2048): 88.839
Elapsed time for attention_prob_times_values (32x2048x2048x1810): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1810): 86.095

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1323.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1811x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1811x2048): 88.051
Elapsed time for attention_prob_times_values (32x2048x2048x1811): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1811): 83.799

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1300.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x330x2048): 0.0177
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x330x2048): 59.954
Elapsed time for attention_prob_times_values (384x2048x2048x330): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x330): 63.422

Attention duration (in seconds): 0.0345
Attention throughput (in TFLOP/s): 1968.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0345
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x331x2048): 0.0179
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x331x2048): 59.682
Elapsed time for attention_prob_times_values (384x2048x2048x331): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x331): 59.545

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 1909.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x332x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x332x2048): 64.698
Elapsed time for attention_prob_times_values (384x2048x2048x332): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x332): 64.948

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 2082.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x333x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x333x2048): 61.713
Elapsed time for attention_prob_times_values (384x2048x2048x333): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x333): 59.632

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 1954.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x334x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x334x2048): 63.945
Elapsed time for attention_prob_times_values (384x2048x2048x334): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x334): 63.361

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 2056.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x335x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x335x2048): 61.907
Elapsed time for attention_prob_times_values (384x2048x2048x335): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x335): 60.608

Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 1984.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x336x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x336x2048): 64.526
Elapsed time for attention_prob_times_values (384x2048x2048x336): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x336): 78.806

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2306.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x337x2048): 0.0178
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x337x2048): 61.155
Elapsed time for attention_prob_times_values (384x2048x2048x337): 0.0181
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x337): 60.022

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 1974.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x338x2048): 0.0178
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x338x2048): 61.337
Elapsed time for attention_prob_times_values (384x2048x2048x338): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x338): 64.066

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 2048.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (32x2048x1812x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1812x2048): 89.961
Elapsed time for attention_prob_times_values (32x2048x2048x1812): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1812): 85.864

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1331.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1813x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1813x2048): 88.739
Elapsed time for attention_prob_times_values (32x2048x2048x1813): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1813): 83.792

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1307.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1814x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1814x2048): 89.214
Elapsed time for attention_prob_times_values (32x2048x2048x1814): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1814): 82.838

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1303.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1815x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1815x2048): 88.533
Elapsed time for attention_prob_times_values (32x2048x2048x1815): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1815): 84.019

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1308.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1816x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1816x2048): 90.147
Elapsed time for attention_prob_times_values (32x2048x2048x1816): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1816): 83.594

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1317.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1817x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1817x2048): 84.781
Elapsed time for attention_prob_times_values (32x2048x2048x1817): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1817): 80.456

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1254.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1818x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1818x2048): 88.767
Elapsed time for attention_prob_times_values (32x2048x2048x1818): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1818): 82.961

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1303.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1819x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1819x2048): 87.653
Elapsed time for attention_prob_times_values (32x2048x2048x1819): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1819): 83.678

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1302.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1820x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1820x2048): 89.314
Elapsed time for attention_prob_times_values (32x2048x2048x1820): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1820): 86.421

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1336.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1821x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1821x2048): 87.693
Elapsed time for attention_prob_times_values (32x2048x2048x1821): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1821): 84.148

Attention duration (in seconds): 0.0114
Elapsed time for attention_prob_times_values (80x2048x2048x1028): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1028): 79.524

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1755.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1029x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1029x2048): 80.574
Elapsed time for attention_prob_times_values (80x2048x2048x1029): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1029): 75.375

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1643.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1030x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1030x2048): 80.064
Elapsed time for attention_prob_times_values (80x2048x2048x1030): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1030): 78.697

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1676.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1031x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1031x2048): 81.148
Elapsed time for attention_prob_times_values (80x2048x2048x1031): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1031): 77.631

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1677.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1032x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1032x2048): 80.683
Elapsed time for attention_prob_times_values (80x2048x2048x1032): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1032): 84.969

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1751.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1033x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1033x2048): 78.879
Elapsed time for attention_prob_times_values (80x2048x2048x1033): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1033): 75.945

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1638.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1034x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1034x2048): 81.143
Elapsed time for attention_prob_times_values (80x2048x2048x1034): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1034): 78.701

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1693.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1035x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1035x2048): 79.532
Elapsed time for attention_prob_times_values (80x2048x2048x1035): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1035): 78.248

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1673.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1036x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1036x2048): 79.855
Elapsed time for attention_prob_times_values (80x2048x2048x1036): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1036): 78.434

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1680.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1037x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1037x2048): 77.773
Elapsed time for attention_prob_times_values (80x2048x2048x1037): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1037): 76.666

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1641.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
========================================================================================================================
num_attention_heads: 40, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x640x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x640x2048): 80.632
Elapsed time for attention_prob_times_values (160x2048x2048x640): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x640): 90.872

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2221.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x641x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x641x2048): 69.286
Elapsed time for attention_prob_times_values (160x2048x2048x641): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x641): 60.456

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1681.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x642x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x642x2048): 74.199
Elapsed time for attention_prob_times_values (160x2048x2048x642): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x642): 61.946

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1760.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x643x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x643x2048): 71.023
Elapsed time for attention_prob_times_values (160x2048x2048x643): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x643): 60.387

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1704.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x644x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x644x2048): 72.599
Elapsed time for attention_prob_times_values (160x2048x2048x644): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x644): 64.610

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1788.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x645x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x645x2048): 72.848
Elapsed time for attention_prob_times_values (160x2048x2048x645): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x645): 59.018

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1708.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x646x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x646x2048): 72.019
Elapsed time for attention_prob_times_values (160x2048x2048x646): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x646): 63.383

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1768.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x647x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x647x2048): 72.773
Elapsed time for attention_prob_times_values (160x2048x2048x647): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x647): 61.555

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1752.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x648x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x648x2048): 76.051
Elapsed time for attention_prob_times_values (160x2048x2048x648): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x648): 77.714

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2022.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x649x2048): 0.0121
Attention throughput (in TFLOP/s): 1307.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1822x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1822x2048): 86.429
Elapsed time for attention_prob_times_values (32x2048x2048x1822): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1822): 84.211

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1299.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1823x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1823x2048): 88.276
Elapsed time for attention_prob_times_values (32x2048x2048x1823): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1823): 84.348

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1314.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1824x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1824x2048): 95.288
Elapsed time for attention_prob_times_values (32x2048x2048x1824): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1824): 87.311

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1389.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1825x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1825x2048): 87.322
Elapsed time for attention_prob_times_values (32x2048x2048x1825): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1825): 84.423

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1309.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1826x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1826x2048): 87.481
Elapsed time for attention_prob_times_values (32x2048x2048x1826): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1826): 86.299

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1326.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1827x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1827x2048): 89.666
Elapsed time for attention_prob_times_values (32x2048x2048x1827): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1827): 84.325

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1327.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1828x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1828x2048): 90.988
Elapsed time for attention_prob_times_values (32x2048x2048x1828): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1828): 86.706

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1356.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1829x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1829x2048): 89.277
Elapsed time for attention_prob_times_values (32x2048x2048x1829): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1829): 84.441

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1326.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1830x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1830x2048): 89.715
Elapsed time for attention_prob_times_values (32x2048x2048x1830): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1830): 86.779

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1349.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x339x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x339x2048): 62.801
Elapsed time for attention_prob_times_values (384x2048x2048x339): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x339): 62.493

Attention duration (in seconds): 0.0349
Attention throughput (in TFLOP/s): 2053.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x340x2048): 0.0171
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x340x2048): 64.131
Elapsed time for attention_prob_times_values (384x2048x2048x340): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x340): 65.307

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 2127.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x341x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x341x2048): 63.220
Elapsed time for attention_prob_times_values (384x2048x2048x341): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x341): 62.557

Attention duration (in seconds): 0.0349
Attention throughput (in TFLOP/s): 2073.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1038x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1038x2048): 80.737
Elapsed time for attention_prob_times_values (80x2048x2048x1038): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1038): 80.721

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1717.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1039x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1039x2048): 77.442
Elapsed time for attention_prob_times_values (80x2048x2048x1039): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1039): 76.233

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1635.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1040x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1040x2048): 79.890
Elapsed time for attention_prob_times_values (80x2048x2048x1040): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1040): 83.533

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1740.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1041x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1041x2048): 80.004
Elapsed time for attention_prob_times_values (80x2048x2048x1041): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1041): 78.765

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1693.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1042x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1042x2048): 79.274
Elapsed time for attention_prob_times_values (80x2048x2048x1042): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1042): 79.586

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1695.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1043x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1043x2048): 80.092
Elapsed time for attention_prob_times_values (80x2048x2048x1043): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1043): 78.932

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1699.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1044x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1044x2048): 81.917
Elapsed time for attention_prob_times_values (80x2048x2048x1044): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1044): 81.222

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1744.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1045x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1045x2048): 80.182
Elapsed time for attention_prob_times_values (80x2048x2048x1045): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1045): 78.987

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1703.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1046x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1046x2048): 81.001
Elapsed time for attention_prob_times_values (80x2048x2048x1046): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1046): 74.713

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1665.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
num_attention_heads: 8, hidden_size: 14648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1831x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1831x2048): 88.866
Elapsed time for attention_prob_times_values (32x2048x2048x1831): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1831): 84.492

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1325.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1832x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1832x2048): 90.690
Elapsed time for attention_prob_times_values (32x2048x2048x1832): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1832): 85.990

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1351.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1833x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1833x2048): 88.297
Elapsed time for attention_prob_times_values (32x2048x2048x1833): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1833): 84.532

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1323.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1834x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1834x2048): 89.147
Elapsed time for attention_prob_times_values (32x2048x2048x1834): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1834): 86.361

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1344.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1835x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1835x2048): 88.488
Elapsed time for attention_prob_times_values (32x2048x2048x1835): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1835): 84.514

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1325.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1836x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1836x2048): 90.072
Elapsed time for attention_prob_times_values (32x2048x2048x1836): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1836): 86.448

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1353.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1837x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1837x2048): 88.539
Elapsed time for attention_prob_times_values (32x2048x2048x1837): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1837): 84.547

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1327.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1838x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1838x2048): 89.503
Elapsed time for attention_prob_times_values (32x2048x2048x1838): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1838): 87.100

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1356.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1839x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1839x2048): 88.727
Elapsed time for attention_prob_times_values (32x2048x2048x1839): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1839): 84.757

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1332.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1840x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1840x2048): 90.790
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x649x2048): 71.790
Elapsed time for attention_prob_times_values (160x2048x2048x649): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x649): 62.494

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1760.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x650x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x650x2048): 72.844
Elapsed time for attention_prob_times_values (160x2048x2048x650): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x650): 65.128

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1814.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x651x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x651x2048): 72.357
Elapsed time for attention_prob_times_values (160x2048x2048x651): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x651): 60.004

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1733.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x652x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x652x2048): 72.971
Elapsed time for attention_prob_times_values (160x2048x2048x652): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x652): 65.488

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1827.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x653x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x653x2048): 71.181
Elapsed time for attention_prob_times_values (160x2048x2048x653): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x653): 62.534

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1764.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x654x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x654x2048): 73.440
Elapsed time for attention_prob_times_values (160x2048x2048x654): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x654): 64.897

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1829.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x655x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x655x2048): 71.562
Elapsed time for attention_prob_times_values (160x2048x2048x655): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x655): 62.438

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1773.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x656x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x656x2048): 75.545
Elapsed time for attention_prob_times_values (160x2048x2048x656): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x656): 75.155

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2006.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x657x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x657x2048): 70.260
Elapsed time for attention_prob_times_values (160x2048x2048x657): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x657): 59.821

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1723.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x658x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x658x2048): 73.582
Elapsed time for attention_prob_times_values (160x2048x2048x658): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x658): 64.605

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1837.235
MLP duration (in seconds): 0.0000
Elapsed time for attention_prob_times_values (32x2048x2048x1840): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1840): 86.833

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1364.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1841x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1841x2048): 88.143
Elapsed time for attention_prob_times_values (32x2048x2048x1841): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1841): 84.787

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1329.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1842x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1842x2048): 89.170
Elapsed time for attention_prob_times_values (32x2048x2048x1842): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1842): 87.238

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1357.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1843x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1843x2048): 88.243
Elapsed time for attention_prob_times_values (32x2048x2048x1843): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1843): 84.917

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1332.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1844x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1844x2048): 89.749
Elapsed time for attention_prob_times_values (32x2048x2048x1844): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1844): 87.443

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1364.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1845x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1845x2048): 88.463
Elapsed time for attention_prob_times_values (32x2048x2048x1845): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1845): 84.978

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1336.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1846x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1846x2048): 89.225
Elapsed time for attention_prob_times_values (32x2048x2048x1846): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1846): 87.442

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1362.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1847x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1847x2048): 88.397
Elapsed time for attention_prob_times_values (32x2048x2048x1847): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1847): 85.077

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1337.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1848x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1848x2048): 90.566
Elapsed time for attention_prob_times_values (32x2048x2048x1848): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1848): 86.712

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1367.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1849x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1849x2048): 88.133
Elapsed time for attention_prob_times_values (32x2048x2048x1849): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1849): 85.124

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1337.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
--------
Elapsed time for attention_key_query_prob (80x2048x1047x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1047x2048): 80.408
Elapsed time for attention_prob_times_values (80x2048x2048x1047): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1047): 74.084

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1654.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1048x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1048x2048): 79.618
Elapsed time for attention_prob_times_values (80x2048x2048x1048): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1048): 86.178

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1776.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1049x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1049x2048): 78.470
Elapsed time for attention_prob_times_values (80x2048x2048x1049): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1049): 76.759

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1667.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1050x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1050x2048): 80.694
Elapsed time for attention_prob_times_values (80x2048x2048x1050): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1050): 80.961

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1738.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1051x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1051x2048): 79.593
Elapsed time for attention_prob_times_values (80x2048x2048x1051): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1051): 79.078

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1707.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1052x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1052x2048): 81.564
Elapsed time for attention_prob_times_values (80x2048x2048x1052): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1052): 81.693

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1758.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1053x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1053x2048): 80.297
Elapsed time for attention_prob_times_values (80x2048x2048x1053): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1053): 79.646

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1724.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1054x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1054x2048): 80.428
Elapsed time for attention_prob_times_values (80x2048x2048x1054): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1054): 81.819

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1750.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1055x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1055x2048): 80.317
Elapsed time for attention_prob_times_values (80x2048x2048x1055): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1055): 79.273

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1723.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1056x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1056x2048): 93.500
Elapsed time for attention_prob_times_values (80x2048x2048x1056): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1056): 85.728

Attention duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1850x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1850x2048): 88.899
Elapsed time for attention_prob_times_values (32x2048x2048x1850): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1850): 87.580

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1363.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1851x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1851x2048): 88.236
Elapsed time for attention_prob_times_values (32x2048x2048x1851): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1851): 84.789

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1337.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1852x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1852x2048): 90.002
Elapsed time for attention_prob_times_values (32x2048x2048x1852): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1852): 87.550

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1372.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1853x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1853x2048): 88.466
Elapsed time for attention_prob_times_values (32x2048x2048x1853): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1853): 85.350

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1344.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1854x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1854x2048): 89.267
Elapsed time for attention_prob_times_values (32x2048x2048x1854): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1854): 87.329

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1367.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1855x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1855x2048): 84.563
Elapsed time for attention_prob_times_values (32x2048x2048x1855): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1855): 82.065

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1290.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1856x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1856x2048): 98.854
Elapsed time for attention_prob_times_values (32x2048x2048x1856): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1856): 87.026

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1434.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1857x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1857x2048): 89.622
Elapsed time for attention_prob_times_values (32x2048x2048x1857): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1857): 85.405

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1356.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1858x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1858x2048): 90.333
Elapsed time for attention_prob_times_values (32x2048x2048x1858): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1858): 85.082

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1359.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1859x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1859x2048): 85.486
Elapsed time for attention_prob_times_values (32x2048x2048x1859): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1859): 81.852

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1298.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1860x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1860x2048): 86.730
Elapsed time for attention_prob_times_values (32x2048x2048x1860): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1860): 83.720

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1323.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1861x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1861x2048): 85.979
Elapsed time for attention_prob_times_values (32x2048x2048x1861): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1861): 85.652

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1333.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1862x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1862x2048): 84.248
Elapsed time for attention_prob_times_values (32x2048x2048x1862): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1862): 87.728

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1336.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1863x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1863x2048): 86.216
Elapsed time for attention_prob_times_values (32x2048x2048x1863): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1863): 83.312

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1318.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1864x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1864x2048): 91.108
Elapsed time for attention_prob_times_values (32x2048x2048x1864): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1864): 86.640

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1382.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1865x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1865x2048): 89.005
Elapsed time for attention_prob_times_values (32x2048x2048x1865): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1865): 81.549

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1325.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1866x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1866x2048): 89.638
Elapsed time for attention_prob_times_values (32x2048x2048x1866): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1866): 83.616

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1347.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1867x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1867x2048): 83.023
Elapsed time for attention_prob_times_values (32x2048x2048x1867): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1867): 85.200

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1310.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1868x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1868x2048): 83.675
Elapsed time for attention_prob_times_values (32x2048x2048x1868): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1868): 85.277

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1934.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1057x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1057x2048): 80.648
Elapsed time for attention_prob_times_values (80x2048x2048x1057): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1057): 78.607

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1723.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1058x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1058x2048): 81.116
Elapsed time for attention_prob_times_values (80x2048x2048x1058): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1058): 81.650

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1763.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1059x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1059x2048): 80.255
Elapsed time for attention_prob_times_values (80x2048x2048x1059): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1059): 78.697

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1723.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1060x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1060x2048): 83.458
Elapsed time for attention_prob_times_values (80x2048x2048x1060): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1060): 82.387

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1799.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1061x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1061x2048): 81.812
Elapsed time for attention_prob_times_values (80x2048x2048x1061): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1061): 79.935

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1756.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1062x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1062x2048): 79.837
Elapsed time for attention_prob_times_values (80x2048x2048x1062): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1062): 82.372

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1762.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1063x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1063x2048): 76.929
Elapsed time for attention_prob_times_values (80x2048x2048x1063): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1063): 74.240

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1644.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1064x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1064x2048): 81.683
Elapsed time for attention_prob_times_values (80x2048x2048x1064): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1064): 83.607

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1799.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1065x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1065x2048): 80.975
Elapsed time for attention_prob_times_values (80x2048x2048x1065): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1065): 78.078

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1733.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x659x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x659x2048): 70.979
Elapsed time for attention_prob_times_values (160x2048x2048x659): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x659): 63.475

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1792.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x660x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x660x2048): 73.467
Elapsed time for attention_prob_times_values (160x2048x2048x660): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x660): 66.520

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1869.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x661x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x661x2048): 73.011
Elapsed time for attention_prob_times_values (160x2048x2048x661): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x661): 62.758

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1810.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x662x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x662x2048): 71.807
Elapsed time for attention_prob_times_values (160x2048x2048x662): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x662): 66.480

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1854.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x663x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x663x2048): 74.326
Elapsed time for attention_prob_times_values (160x2048x2048x663): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x663): 64.065

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1851.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x664x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x664x2048): 75.881
Elapsed time for attention_prob_times_values (160x2048x2048x664): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x664): 80.233

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2101.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x665x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x665x2048): 73.753
Elapsed time for attention_prob_times_values (160x2048x2048x665): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x665): 64.344

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1854.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x666x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x666x2048): 74.396
Elapsed time for attention_prob_times_values (160x2048x2048x666): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x666): 66.656

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1899.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x667x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x667x2048): 73.863
Elapsed time for attention_prob_times_values (160x2048x2048x667): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x667): 64.583

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1864.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 1317.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1869x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1869x2048): 88.951
Elapsed time for attention_prob_times_values (32x2048x2048x1869): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1869): 82.399

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1334.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1870x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1870x2048): 89.899
Elapsed time for attention_prob_times_values (32x2048x2048x1870): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1870): 87.800

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1386.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1871x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1871x2048): 85.747
Elapsed time for attention_prob_times_values (32x2048x2048x1871): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1871): 85.859

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1340.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1872x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1872x2048): 87.008
Elapsed time for attention_prob_times_values (32x2048x2048x1872): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1872): 88.413

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1370.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1873x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1873x2048): 84.868
Elapsed time for attention_prob_times_values (32x2048x2048x1873): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1873): 83.597

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1316.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 14992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1874x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1874x2048): 89.370
Elapsed time for attention_prob_times_values (32x2048x2048x1874): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1874): 87.921

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1386.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1875x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1875x2048): 88.496
Elapsed time for attention_prob_times_values (32x2048x2048x1875): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1875): 86.388

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1368.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1876x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1876x2048): 86.782
Elapsed time for attention_prob_times_values (32x2048x2048x1876): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1876): 88.247

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1370.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1877x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1877x2048): 85.876
Elapsed time for attention_prob_times_values (32x2048x2048x1877): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1877): 85.649

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1343.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 20, hidden_size: 21320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1066x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1066x2048): 78.787
Elapsed time for attention_prob_times_values (80x2048x2048x1066): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1066): 82.632

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1760.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1067x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1067x2048): 76.411
Elapsed time for attention_prob_times_values (80x2048x2048x1067): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1067): 76.065

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1665.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1068x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1068x2048): 78.463
Elapsed time for attention_prob_times_values (80x2048x2048x1068): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1068): 80.628

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1738.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1069x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1069x2048): 81.271
Elapsed time for attention_prob_times_values (80x2048x2048x1069): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1069): 76.304

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1722.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1070x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1070x2048): 78.369
Elapsed time for attention_prob_times_values (80x2048x2048x1070): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1070): 82.835

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1763.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1071x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1071x2048): 78.935
Elapsed time for attention_prob_times_values (80x2048x2048x1071): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1071): 78.685

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1727.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1072x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1072x2048): 83.435
Elapsed time for attention_prob_times_values (80x2048x2048x1072): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1072): 86.927

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1867.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1073x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1073x2048): 78.513
Elapsed time for attention_prob_times_values (80x2048x2048x1073): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1073): 79.265

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1732.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1074x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1074x2048): 79.656
Elapsed time for attention_prob_times_values (80x2048x2048x1074): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1074): 80.873

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1763.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1075x2048): 0.0091
num_attention_heads: 8, hidden_size: 15024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1878x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1878x2048): 89.432
Elapsed time for attention_prob_times_values (32x2048x2048x1878): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1878): 88.113

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1391.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1879x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1879x2048): 88.584
Elapsed time for attention_prob_times_values (32x2048x2048x1879): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1879): 85.960

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1368.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1880x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1880x2048): 90.888
Elapsed time for attention_prob_times_values (32x2048x2048x1880): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1880): 88.016

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1402.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1881x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1881x2048): 88.290
Elapsed time for attention_prob_times_values (32x2048x2048x1881): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1881): 86.125

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1368.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1882x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1882x2048): 89.061
Elapsed time for attention_prob_times_values (32x2048x2048x1882): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1882): 88.271

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1392.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1883x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1883x2048): 86.301
Elapsed time for attention_prob_times_values (32x2048x2048x1883): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1883): 85.670

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1350.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1884x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1884x2048): 88.192
Elapsed time for attention_prob_times_values (32x2048x2048x1884): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1884): 88.350

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1387.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1885x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1885x2048): 88.195
Elapsed time for attention_prob_times_values (32x2048x2048x1885): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1885): 86.168

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1370.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1886x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1886x2048): 89.123
Elapsed time for attention_prob_times_values (32x2048x2048x1886): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1886): 86.643

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1382.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1887x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1887x2048): 88.487
num_attention_heads: 40, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x668x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x668x2048): 75.096
Elapsed time for attention_prob_times_values (160x2048x2048x668): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x668): 67.154

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1921.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x669x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x669x2048): 74.002
Elapsed time for attention_prob_times_values (160x2048x2048x669): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x669): 62.672

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1841.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x670x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x670x2048): 73.174
Elapsed time for attention_prob_times_values (160x2048x2048x670): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x670): 66.799

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1897.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x671x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x671x2048): 72.088
Elapsed time for attention_prob_times_values (160x2048x2048x671): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x671): 63.788

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1841.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x672x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x672x2048): 89.433
Elapsed time for attention_prob_times_values (160x2048x2048x672): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x672): 80.493

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2308.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x673x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x673x2048): 73.736
Elapsed time for attention_prob_times_values (160x2048x2048x673): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x673): 62.861

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1851.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x674x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x674x2048): 77.217
Elapsed time for attention_prob_times_values (160x2048x2048x674): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x674): 65.914

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1943.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x675x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x675x2048): 73.352
Elapsed time for attention_prob_times_values (160x2048x2048x675): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x675): 62.839

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1852.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x676x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x676x2048): 77.751
Elapsed time for attention_prob_times_values (160x2048x2048x676): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x676): 66.564

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1965.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x677x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x677x2048): 73.713
Elapsed time for attention_prob_times_values (32x2048x2048x1887): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1887): 86.172

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1374.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1888x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1888x2048): 98.782
Elapsed time for attention_prob_times_values (32x2048x2048x1888): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1888): 90.194

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1485.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1889x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1889x2048): 89.573
Elapsed time for attention_prob_times_values (32x2048x2048x1889): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1889): 86.221

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1384.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1890x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1890x2048): 90.313
Elapsed time for attention_prob_times_values (32x2048x2048x1890): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1890): 88.264

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1407.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1891x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1891x2048): 89.314
Elapsed time for attention_prob_times_values (32x2048x2048x1891): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1891): 86.075

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1382.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1892x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1892x2048): 91.139
Elapsed time for attention_prob_times_values (32x2048x2048x1892): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1892): 88.442

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1416.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1893x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1893x2048): 89.237
Elapsed time for attention_prob_times_values (32x2048x2048x1893): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1893): 86.184

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1384.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1894x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1894x2048): 89.950
Elapsed time for attention_prob_times_values (32x2048x2048x1894): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1894): 88.363

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1408.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1895x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1895x2048): 88.958
Elapsed time for attention_prob_times_values (32x2048x2048x1895): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1895): 86.299

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1384.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1896x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1896x2048): 91.032
Elapsed time for attention_prob_times_values (32x2048x2048x1896): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1896): 88.463

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1418.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1075x2048): 79.128
Elapsed time for attention_prob_times_values (80x2048x2048x1075): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1075): 79.513

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1744.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1076x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1076x2048): 82.672
Elapsed time for attention_prob_times_values (80x2048x2048x1076): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1076): 82.238

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1815.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1077x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1077x2048): 79.527
Elapsed time for attention_prob_times_values (80x2048x2048x1077): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1077): 79.602

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1753.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1078x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1078x2048): 80.381
Elapsed time for attention_prob_times_values (80x2048x2048x1078): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1078): 82.242

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1793.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1079x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1079x2048): 80.262
Elapsed time for attention_prob_times_values (80x2048x2048x1079): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1079): 81.074

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1780.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1080x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1080x2048): 83.179
Elapsed time for attention_prob_times_values (80x2048x2048x1080): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1080): 88.496

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1894.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1081x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1081x2048): 79.213
Elapsed time for attention_prob_times_values (80x2048x2048x1081): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1081): 80.121

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1761.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1082x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1082x2048): 81.419
Elapsed time for attention_prob_times_values (80x2048x2048x1082): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1082): 83.532

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1825.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1083x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1083x2048): 80.764
Elapsed time for attention_prob_times_values (80x2048x2048x1083): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1083): 80.930

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1790.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1084x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1084x2048): 81.500
Elapsed time for attention_prob_times_values (80x2048x2048x1084): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1084): 83.105

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1824.621
MLP duration (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1897x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1897x2048): 88.867
Elapsed time for attention_prob_times_values (32x2048x2048x1897): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1897): 85.987

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1382.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1898x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1898x2048): 89.724
Elapsed time for attention_prob_times_values (32x2048x2048x1898): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1898): 86.576

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1394.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1899x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1899x2048): 88.761
Elapsed time for attention_prob_times_values (32x2048x2048x1899): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1899): 84.173

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1368.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1900x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1900x2048): 85.548
Elapsed time for attention_prob_times_values (32x2048x2048x1900): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1900): 87.633

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1371.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1901x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1901x2048): 88.977
Elapsed time for attention_prob_times_values (32x2048x2048x1901): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1901): 86.057

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1386.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1902x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1902x2048): 89.796
Elapsed time for attention_prob_times_values (32x2048x2048x1902): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1902): 83.235

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1370.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1903x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1903x2048): 88.841
Elapsed time for attention_prob_times_values (32x2048x2048x1903): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1903): 84.939

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1378.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1904x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1904x2048): 91.009
Elapsed time for attention_prob_times_values (32x2048x2048x1904): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1904): 89.654

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1433.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1905x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1905x2048): 88.494
Elapsed time for attention_prob_times_values (32x2048x2048x1905): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1905): 86.426

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1388.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1906x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1906x2048): 83.735
Elapsed time for attention_prob_times_values (32x2048x2048x1906): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1906): 85.439

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1344.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1907x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1907x2048): 88.693
Elapsed time for attention_prob_times_values (32x2048x2048x1907): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1907): 86.125

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1389.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1908x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1908x2048): 89.610
Elapsed time for attention_prob_times_values (32x2048x2048x1908): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1908): 88.448

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1416.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1909x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1909x2048): 85.693
Elapsed time for attention_prob_times_values (32x2048x2048x1909): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1909): 86.032

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1366.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1910x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1910x2048): 82.943
Elapsed time for attention_prob_times_values (32x2048x2048x1910): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1910): 88.008

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1359.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1911x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1911x2048): 88.536
Elapsed time for attention_prob_times_values (32x2048x2048x1911): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1911): 84.612

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1378.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1912x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1912x2048): 90.507
Elapsed time for attention_prob_times_values (32x2048x2048x1912): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1912): 86.261

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1407.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1913x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1913x2048): 87.952
Elapsed time for attention_prob_times_values (32x2048x2048x1913): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1913): 86.399

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1389.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1914x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1914x2048): 85.630
Elapsed time for attention_prob_times_values (32x2048x2048x1914): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1914): 88.055

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1385.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1915x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1915x2048): 88.180
Elapsed time for attention_prob_times_values (32x2048x2048x1915): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1915): 86.517

Attention duration (in seconds): 0.0118
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1085x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1085x2048): 80.963
Elapsed time for attention_prob_times_values (80x2048x2048x1085): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1085): 81.457

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1802.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1086x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1086x2048): 80.815
Elapsed time for attention_prob_times_values (80x2048x2048x1086): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1086): 83.769

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1827.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1087x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1087x2048): 78.462
Elapsed time for attention_prob_times_values (80x2048x2048x1087): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1087): 77.465

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1733.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1088x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1088x2048): 93.544
Elapsed time for attention_prob_times_values (80x2048x2048x1088): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1088): 86.888

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 2004.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1089x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1089x2048): 82.411
Elapsed time for attention_prob_times_values (80x2048x2048x1089): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1089): 81.436

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1824.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1090x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1090x2048): 80.749
Elapsed time for attention_prob_times_values (80x2048x2048x1090): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1090): 81.102

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1803.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1091x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1091x2048): 79.286
Elapsed time for attention_prob_times_values (80x2048x2048x1091): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1091): 76.901

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1741.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1092x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1092x2048): 83.999
Elapsed time for attention_prob_times_values (80x2048x2048x1092): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1092): 80.806

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1839.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1093x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1093x2048): 77.172
Elapsed time for attention_prob_times_values (80x2048x2048x1093): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1093): 81.768

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1774.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (160x2048x2048x677): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x677): 65.066

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1897.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x678x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x678x2048): 77.018
Elapsed time for attention_prob_times_values (160x2048x2048x678): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x678): 67.373

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1975.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x679x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x679x2048): 75.089
Elapsed time for attention_prob_times_values (160x2048x2048x679): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x679): 64.077

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1903.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x680x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x680x2048): 80.202
Elapsed time for attention_prob_times_values (160x2048x2048x680): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x680): 79.526

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2201.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x681x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x681x2048): 73.898
Elapsed time for attention_prob_times_values (160x2048x2048x681): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x681): 64.348

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1898.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x682x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x682x2048): 73.650
Elapsed time for attention_prob_times_values (160x2048x2048x682): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x682): 65.903

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1922.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x683x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x683x2048): 72.081
Elapsed time for attention_prob_times_values (160x2048x2048x683): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x683): 64.877

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1890.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x684x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x684x2048): 76.801
Elapsed time for attention_prob_times_values (160x2048x2048x684): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x684): 68.171

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2002.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x685x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x685x2048): 74.731
Elapsed time for attention_prob_times_values (160x2048x2048x685): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x685): 65.258

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1933.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x686x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x686x2048): 76.199
Elapsed time for attention_prob_times_values (160x2048x2048x686): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x686): 68.219

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2001.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 1394.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1916x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1916x2048): 89.869
Elapsed time for attention_prob_times_values (32x2048x2048x1916): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1916): 88.411

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1423.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1917x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1917x2048): 87.866
Elapsed time for attention_prob_times_values (32x2048x2048x1917): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1917): 86.365

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1391.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1918x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1918x2048): 89.102
Elapsed time for attention_prob_times_values (32x2048x2048x1918): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1918): 87.146

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1408.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1919x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1919x2048): 85.987
Elapsed time for attention_prob_times_values (32x2048x2048x1919): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1919): 85.640

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1372.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1920x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1920x2048): 96.713
Elapsed time for attention_prob_times_values (32x2048x2048x1920): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1920): 92.623

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1513.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1921x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1921x2048): 89.463
Elapsed time for attention_prob_times_values (32x2048x2048x1921): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1921): 82.038

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1370.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1922x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1922x2048): 90.358
Elapsed time for attention_prob_times_values (32x2048x2048x1922): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1922): 82.737

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1383.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1923x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1923x2048): 84.909
Elapsed time for attention_prob_times_values (32x2048x2048x1923): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1923): 79.711

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1317.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1924x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1924x2048): 86.383
Elapsed time for attention_prob_times_values (32x2048x2048x1924): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1924): 82.176

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1350.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1094x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1094x2048): 82.569
Elapsed time for attention_prob_times_values (80x2048x2048x1094): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1094): 84.128

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1864.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1095x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1095x2048): 81.889
Elapsed time for attention_prob_times_values (80x2048x2048x1095): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1095): 81.878

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1833.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1096x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1096x2048): 83.936
Elapsed time for attention_prob_times_values (80x2048x2048x1096): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1096): 89.878

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1944.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1097x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1097x2048): 77.477
Elapsed time for attention_prob_times_values (80x2048x2048x1097): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1097): 80.088

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1766.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1098x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1098x2048): 80.105
Elapsed time for attention_prob_times_values (80x2048x2048x1098): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1098): 82.462

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1824.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1099x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1099x2048): 81.513
Elapsed time for attention_prob_times_values (80x2048x2048x1099): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1099): 80.352

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1818.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1100x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1100x2048): 83.147
Elapsed time for attention_prob_times_values (80x2048x2048x1100): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1100): 82.504

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1862.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1101x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1101x2048): 79.573
Elapsed time for attention_prob_times_values (80x2048x2048x1101): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1101): 82.509

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1823.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1102x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1102x2048): 79.961
Elapsed time for attention_prob_times_values (80x2048x2048x1102): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1102): 82.926

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1833.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1103x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1103x2048): 80.263
num_attention_heads: 8, hidden_size: 15400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1925x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1925x2048): 89.129
Elapsed time for attention_prob_times_values (32x2048x2048x1925): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1925): 82.926

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1378.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1926x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1926x2048): 89.990
Elapsed time for attention_prob_times_values (32x2048x2048x1926): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1926): 85.404

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1406.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1927x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1927x2048): 85.502
Elapsed time for attention_prob_times_values (32x2048x2048x1927): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1927): 83.066

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1352.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1928x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1928x2048): 90.940
Elapsed time for attention_prob_times_values (32x2048x2048x1928): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1928): 77.737

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1346.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1929x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1929x2048): 88.532
Elapsed time for attention_prob_times_values (32x2048x2048x1929): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1929): 79.282

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1344.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1930x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1930x2048): 86.628
Elapsed time for attention_prob_times_values (32x2048x2048x1930): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1930): 82.332

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1357.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1931x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1931x2048): 84.307
Elapsed time for attention_prob_times_values (32x2048x2048x1931): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1931): 83.280

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1347.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1932x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1932x2048): 86.446
Elapsed time for attention_prob_times_values (32x2048x2048x1932): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1932): 83.573

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1367.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1933x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1933x2048): 88.795
Elapsed time for attention_prob_times_values (32x2048x2048x1933): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1933): 82.849

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1380.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1934x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1934x2048): 89.942
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x687x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x687x2048): 75.557
Elapsed time for attention_prob_times_values (160x2048x2048x687): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x687): 64.694

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1940.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x688x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x688x2048): 77.368
Elapsed time for attention_prob_times_values (160x2048x2048x688): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x688): 82.223

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2222.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x689x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x689x2048): 72.444
Elapsed time for attention_prob_times_values (160x2048x2048x689): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x689): 65.431

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1919.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x690x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x690x2048): 74.547
Elapsed time for attention_prob_times_values (160x2048x2048x690): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x690): 68.450

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1994.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x691x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x691x2048): 74.329
Elapsed time for attention_prob_times_values (160x2048x2048x691): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x691): 65.370

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1947.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x692x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x692x2048): 74.981
Elapsed time for attention_prob_times_values (160x2048x2048x692): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x692): 68.222

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2002.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x693x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x693x2048): 74.698
Elapsed time for attention_prob_times_values (160x2048x2048x693): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x693): 66.376

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1973.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x694x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x694x2048): 75.347
Elapsed time for attention_prob_times_values (160x2048x2048x694): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x694): 68.807

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2021.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x695x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x695x2048): 75.301
Elapsed time for attention_prob_times_values (160x2048x2048x695): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x695): 66.168

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1982.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_prob_times_values (32x2048x2048x1934): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1934): 84.602

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1404.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1935x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1935x2048): 88.064
Elapsed time for attention_prob_times_values (32x2048x2048x1935): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1935): 81.835

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1367.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1936x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1936x2048): 88.737
Elapsed time for attention_prob_times_values (32x2048x2048x1936): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1936): 84.984

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1399.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1937x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1937x2048): 88.430
Elapsed time for attention_prob_times_values (32x2048x2048x1937): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1937): 83.558

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1386.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1938x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1938x2048): 89.378
Elapsed time for attention_prob_times_values (32x2048x2048x1938): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1938): 82.827

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1387.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1939x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1939x2048): 84.808
Elapsed time for attention_prob_times_values (32x2048x2048x1939): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1939): 83.584

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1359.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1940x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1940x2048): 90.135
Elapsed time for attention_prob_times_values (32x2048x2048x1940): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1940): 85.596

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1418.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1941x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1941x2048): 88.688
Elapsed time for attention_prob_times_values (32x2048x2048x1941): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1941): 83.551

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1390.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1942x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1942x2048): 89.526
Elapsed time for attention_prob_times_values (32x2048x2048x1942): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1942): 85.907

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1417.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1943x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1943x2048): 85.796
Elapsed time for attention_prob_times_values (32x2048x2048x1943): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1943): 83.724

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1371.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Elapsed time for attention_prob_times_values (80x2048x2048x1103): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1103): 80.394

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1810.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1104x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1104x2048): 83.830
Elapsed time for attention_prob_times_values (80x2048x2048x1104): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1104): 89.096

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1949.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1105x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1105x2048): 81.018
Elapsed time for attention_prob_times_values (80x2048x2048x1105): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1105): 82.829

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1849.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1106x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1106x2048): 81.581
Elapsed time for attention_prob_times_values (80x2048x2048x1106): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1106): 85.048

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1882.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1107x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1107x2048): 81.028
Elapsed time for attention_prob_times_values (80x2048x2048x1107): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1107): 82.987

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1854.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1108x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1108x2048): 82.787
Elapsed time for attention_prob_times_values (80x2048x2048x1108): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1108): 85.209

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1901.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1109x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1109x2048): 81.380
Elapsed time for attention_prob_times_values (80x2048x2048x1109): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1109): 83.157

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1863.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1110x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1110x2048): 82.013
Elapsed time for attention_prob_times_values (80x2048x2048x1110): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1110): 85.172

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1895.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1111x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1111x2048): 81.278
Elapsed time for attention_prob_times_values (80x2048x2048x1111): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1111): 82.875

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1862.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1112x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1112x2048): 83.623
Elapsed time for attention_prob_times_values (80x2048x2048x1112): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1112): 91.142

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1981.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1944x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1944x2048): 90.594
Elapsed time for attention_prob_times_values (32x2048x2048x1944): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1944): 81.294

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1387.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1945x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1945x2048): 88.351
Elapsed time for attention_prob_times_values (32x2048x2048x1945): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1945): 83.653

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1391.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1946x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1946x2048): 89.276
Elapsed time for attention_prob_times_values (32x2048x2048x1946): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1946): 86.053

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1419.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1947x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1947x2048): 88.218
Elapsed time for attention_prob_times_values (32x2048x2048x1947): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1947): 83.887

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1394.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1948x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1948x2048): 88.882
Elapsed time for attention_prob_times_values (32x2048x2048x1948): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1948): 86.274

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1420.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1949x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1949x2048): 88.601
Elapsed time for attention_prob_times_values (32x2048x2048x1949): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1949): 80.084

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1365.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1950x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1950x2048): 89.375
Elapsed time for attention_prob_times_values (32x2048x2048x1950): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1950): 83.198

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1399.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1951x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1951x2048): 88.667
Elapsed time for attention_prob_times_values (32x2048x2048x1951): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1951): 83.937

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1400.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1952x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1952x2048): 98.856
Elapsed time for attention_prob_times_values (32x2048x2048x1952): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1952): 86.786

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1501.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1113x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1113x2048): 80.914
Elapsed time for attention_prob_times_values (80x2048x2048x1113): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1113): 83.382

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1867.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1114x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1114x2048): 78.511
Elapsed time for attention_prob_times_values (80x2048x2048x1114): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1114): 81.823

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1823.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1115x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1115x2048): 81.109
Elapsed time for attention_prob_times_values (80x2048x2048x1115): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1115): 83.427

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1873.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1116x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1116x2048): 82.868
Elapsed time for attention_prob_times_values (80x2048x2048x1116): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1116): 85.458

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1918.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1117x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1117x2048): 78.557
Elapsed time for attention_prob_times_values (80x2048x2048x1117): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1117): 83.413

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1846.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1118x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1118x2048): 81.899
Elapsed time for attention_prob_times_values (80x2048x2048x1118): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1118): 83.102

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1883.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1119x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1119x2048): 80.998
Elapsed time for attention_prob_times_values (80x2048x2048x1119): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1119): 81.044

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1851.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1120x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1120x2048): 91.814
Elapsed time for attention_prob_times_values (80x2048x2048x1120): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1120): 92.748

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2110.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1121x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1121x2048): 82.605
Elapsed time for attention_prob_times_values (80x2048x2048x1121): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1121): 81.409

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1877.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_key_query_prob (32x2048x1953x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1953x2048): 89.688
Elapsed time for attention_prob_times_values (32x2048x2048x1953): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1953): 83.892

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1409.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1954x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1954x2048): 90.651
Elapsed time for attention_prob_times_values (32x2048x2048x1954): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1954): 86.302

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1438.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1955x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1955x2048): 89.513
Elapsed time for attention_prob_times_values (32x2048x2048x1955): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1955): 84.099

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1411.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1956x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1956x2048): 91.217
Elapsed time for attention_prob_times_values (32x2048x2048x1956): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1956): 86.502

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1445.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1957x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1957x2048): 89.213
Elapsed time for attention_prob_times_values (32x2048x2048x1957): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1957): 84.115

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1410.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1958x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1958x2048): 90.279
Elapsed time for attention_prob_times_values (32x2048x2048x1958): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1958): 86.330

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1438.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1959x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1959x2048): 89.337
Elapsed time for attention_prob_times_values (32x2048x2048x1959): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1959): 84.108

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1412.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1960x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1960x2048): 91.241
Elapsed time for attention_prob_times_values (32x2048x2048x1960): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1960): 85.262

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1437.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1961x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1961x2048): 88.967
Elapsed time for attention_prob_times_values (32x2048x2048x1961): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1961): 84.245

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1412.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1962x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1962x2048): 89.710
Elapsed time for attention_prob_times_values (32x2048x2048x1962): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1962): 86.645

Attention duration (in seconds): 0.0119
--------
Elapsed time for attention_key_query_prob (160x2048x696x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x696x2048): 77.235
Elapsed time for attention_prob_times_values (160x2048x2048x696): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x696): 82.232

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2245.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x697x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x697x2048): 73.492
Elapsed time for attention_prob_times_values (160x2048x2048x697): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x697): 65.977

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1962.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x698x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x698x2048): 73.933
Elapsed time for attention_prob_times_values (160x2048x2048x698): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x698): 68.325

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2007.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x699x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x699x2048): 74.067
Elapsed time for attention_prob_times_values (160x2048x2048x699): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x699): 63.518

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1935.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x700x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x700x2048): 75.351
Elapsed time for attention_prob_times_values (160x2048x2048x700): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x700): 67.949

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2025.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x701x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x701x2048): 75.076
Elapsed time for attention_prob_times_values (160x2048x2048x701): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x701): 66.901

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2008.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x702x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x702x2048): 76.007
Elapsed time for attention_prob_times_values (160x2048x2048x702): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x702): 69.339

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2061.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x703x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x703x2048): 75.204
Elapsed time for attention_prob_times_values (160x2048x2048x703): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x703): 67.019

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2017.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x704x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x704x2048): 89.780
Elapsed time for attention_prob_times_values (160x2048x2048x704): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x704): 86.557

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2511.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x705x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x705x2048): 76.388
Elapsed time for attention_prob_times_values (160x2048x2048x705): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x705): 63.224

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1439.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1963x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1963x2048): 89.263
Elapsed time for attention_prob_times_values (32x2048x2048x1963): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1963): 84.222

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1415.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1964x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1964x2048): 90.838
Elapsed time for attention_prob_times_values (32x2048x2048x1964): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1964): 86.676

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1449.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1965x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1965x2048): 88.017
Elapsed time for attention_prob_times_values (32x2048x2048x1965): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1965): 84.262

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1407.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1966x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1966x2048): 89.853
Elapsed time for attention_prob_times_values (32x2048x2048x1966): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1966): 85.989

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1437.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1967x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1967x2048): 89.182
Elapsed time for attention_prob_times_values (32x2048x2048x1967): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1967): 83.276

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1409.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1968x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1968x2048): 91.527
Elapsed time for attention_prob_times_values (32x2048x2048x1968): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1968): 86.200

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1453.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1969x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1969x2048): 88.228
Elapsed time for attention_prob_times_values (32x2048x2048x1969): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1969): 84.473

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1413.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1970x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1970x2048): 88.799
Elapsed time for attention_prob_times_values (32x2048x2048x1970): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1970): 86.842

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1439.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1971x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1971x2048): 88.922
Elapsed time for attention_prob_times_values (32x2048x2048x1971): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1971): 83.493

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1412.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
--------
Elapsed time for attention_key_query_prob (80x2048x1122x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1122x2048): 83.503
Elapsed time for attention_prob_times_values (80x2048x2048x1122): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1122): 85.733

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1938.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1123x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1123x2048): 80.049
Elapsed time for attention_prob_times_values (80x2048x2048x1123): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1123): 79.254

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1826.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1124x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1124x2048): 84.031
Elapsed time for attention_prob_times_values (80x2048x2048x1124): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1124): 82.429

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1910.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1125x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1125x2048): 82.204
Elapsed time for attention_prob_times_values (80x2048x2048x1125): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1125): 83.783

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1906.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1126x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1126x2048): 78.977
Elapsed time for attention_prob_times_values (80x2048x2048x1126): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1126): 85.957

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1892.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1127x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1127x2048): 81.906
Elapsed time for attention_prob_times_values (80x2048x2048x1127): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1127): 80.819

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1872.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1128x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1128x2048): 83.812
Elapsed time for attention_prob_times_values (80x2048x2048x1128): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1128): 92.324

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2023.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1129x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1129x2048): 81.417
Elapsed time for attention_prob_times_values (80x2048x2048x1129): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1129): 84.117

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1907.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1130x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1130x2048): 82.052
Elapsed time for attention_prob_times_values (80x2048x2048x1130): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1130): 80.785

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1878.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1131x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1131x2048): 81.550
Elapsed time for attention_prob_times_values (80x2048x2048x1131): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1131): 84.183

Attention duration (in seconds): 0.0183
num_attention_heads: 8, hidden_size: 15776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1972x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1972x2048): 90.509
Elapsed time for attention_prob_times_values (32x2048x2048x1972): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1972): 86.442

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1450.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1973x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1973x2048): 89.095
Elapsed time for attention_prob_times_values (32x2048x2048x1973): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1973): 84.609

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1424.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1974x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1974x2048): 89.810
Elapsed time for attention_prob_times_values (32x2048x2048x1974): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1974): 87.059

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1451.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1975x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1975x2048): 88.387
Elapsed time for attention_prob_times_values (32x2048x2048x1975): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1975): 84.669

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1420.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1976x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1976x2048): 91.120
Elapsed time for attention_prob_times_values (32x2048x2048x1976): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1976): 82.666

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1424.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1977x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1977x2048): 88.841
Elapsed time for attention_prob_times_values (32x2048x2048x1977): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1977): 84.796

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1426.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1978x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1978x2048): 89.800
Elapsed time for attention_prob_times_values (32x2048x2048x1978): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1978): 85.820

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1444.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1979x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1979x2048): 88.722
Elapsed time for attention_prob_times_values (32x2048x2048x1979): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1979): 84.860

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1427.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1980x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1980x2048): 90.537
Elapsed time for attention_prob_times_values (32x2048x2048x1980): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1980): 87.519

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1465.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1981x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1981x2048): 86.732
Attention throughput (in TFLOP/s): 1974.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x706x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x706x2048): 77.104
Elapsed time for attention_prob_times_values (160x2048x2048x706): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x706): 65.307

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2020.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x707x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x707x2048): 75.168
Elapsed time for attention_prob_times_values (160x2048x2048x707): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x707): 63.082

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1963.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x708x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x708x2048): 77.375
Elapsed time for attention_prob_times_values (160x2048x2048x708): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x708): 66.146

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2043.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x709x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x709x2048): 76.000
Elapsed time for attention_prob_times_values (160x2048x2048x709): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x709): 60.644

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 1935.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x710x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x710x2048): 76.318
Elapsed time for attention_prob_times_values (160x2048x2048x710): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x710): 65.646

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2028.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x711x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x711x2048): 75.512
Elapsed time for attention_prob_times_values (160x2048x2048x711): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x711): 64.218

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1997.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x712x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x712x2048): 78.157
Elapsed time for attention_prob_times_values (160x2048x2048x712): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x712): 85.980

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2359.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x713x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x713x2048): 75.896
Elapsed time for attention_prob_times_values (160x2048x2048x713): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x713): 62.737

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1981.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x714x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x714x2048): 76.814
Elapsed time for attention_prob_times_values (160x2048x2048x714): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x714): 67.202

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2071.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (32x2048x2048x1981): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1981): 84.981

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1414.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1982x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1982x2048): 87.911
Elapsed time for attention_prob_times_values (32x2048x2048x1982): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1982): 87.480

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1445.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1983x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1983x2048): 88.968
Elapsed time for attention_prob_times_values (32x2048x2048x1983): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1983): 85.080

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1434.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1984x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1984x2048): 99.165
Elapsed time for attention_prob_times_values (32x2048x2048x1984): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1984): 94.764

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1599.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1985x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1985x2048): 90.067
Elapsed time for attention_prob_times_values (32x2048x2048x1985): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1985): 81.775

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1415.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1986x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1986x2048): 87.537
Elapsed time for attention_prob_times_values (32x2048x2048x1986): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1986): 87.671

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1446.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1987x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1987x2048): 88.886
Elapsed time for attention_prob_times_values (32x2048x2048x1987): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1987): 85.144

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1437.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1988x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1988x2048): 91.658
Elapsed time for attention_prob_times_values (32x2048x2048x1988): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1988): 83.509

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1444.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1989x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1989x2048): 89.886
Elapsed time for attention_prob_times_values (32x2048x2048x1989): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1989): 80.994

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1409.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1990x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1990x2048): 90.629
Elapsed time for attention_prob_times_values (32x2048x2048x1990): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1990): 87.439

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1472.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1912.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1132x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1132x2048): 82.929
Elapsed time for attention_prob_times_values (80x2048x2048x1132): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1132): 86.521

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1957.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1133x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1133x2048): 81.669
Elapsed time for attention_prob_times_values (80x2048x2048x1133): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1133): 84.270

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1918.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1134x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1134x2048): 82.423
Elapsed time for attention_prob_times_values (80x2048x2048x1134): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1134): 86.591

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1955.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1135x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1135x2048): 81.851
Elapsed time for attention_prob_times_values (80x2048x2048x1135): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1135): 84.403

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1925.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1136x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1136x2048): 82.085
Elapsed time for attention_prob_times_values (80x2048x2048x1136): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1136): 90.873

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2000.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1137x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1137x2048): 81.259
Elapsed time for attention_prob_times_values (80x2048x2048x1137): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1137): 82.137

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1895.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1138x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1138x2048): 82.200
Elapsed time for attention_prob_times_values (80x2048x2048x1138): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1138): 86.741

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1960.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1139x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1139x2048): 81.412
Elapsed time for attention_prob_times_values (80x2048x2048x1139): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1139): 84.506

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1927.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1140x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1140x2048): 82.797
Elapsed time for attention_prob_times_values (80x2048x2048x1140): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1140): 87.071

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1974.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1991x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1991x2048): 85.861
Elapsed time for attention_prob_times_values (32x2048x2048x1991): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1991): 85.273

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1416.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1992x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1992x2048): 86.208
Elapsed time for attention_prob_times_values (32x2048x2048x1992): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1992): 93.043

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1482.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1993x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1993x2048): 89.631
Elapsed time for attention_prob_times_values (32x2048x2048x1993): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1993): 85.019

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1445.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1994x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1994x2048): 90.446
Elapsed time for attention_prob_times_values (32x2048x2048x1994): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1994): 87.523

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1474.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1995x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1995x2048): 83.349
Elapsed time for attention_prob_times_values (32x2048x2048x1995): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1995): 85.572

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1400.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1996x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1996x2048): 91.080
Elapsed time for attention_prob_times_values (32x2048x2048x1996): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1996): 82.938

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1440.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1997x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1997x2048): 89.266
Elapsed time for attention_prob_times_values (32x2048x2048x1997): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1997): 82.541

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1423.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1998x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1998x2048): 90.445
Elapsed time for attention_prob_times_values (32x2048x2048x1998): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1998): 88.062

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1482.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 15992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1999x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1999x2048): 89.511
Elapsed time for attention_prob_times_values (32x2048x2048x1999): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1999): 85.681

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1454.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 20, hidden_size: 22820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1141x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1141x2048): 81.474
Elapsed time for attention_prob_times_values (80x2048x2048x1141): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1141): 84.577

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1932.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1142x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1142x2048): 81.915
Elapsed time for attention_prob_times_values (80x2048x2048x1142): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1142): 87.058

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1967.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1143x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1143x2048): 81.388
Elapsed time for attention_prob_times_values (80x2048x2048x1143): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1143): 84.792

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1937.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1144x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1144x2048): 83.465
Elapsed time for attention_prob_times_values (80x2048x2048x1144): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1144): 93.645

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2060.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1145x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1145x2048): 81.108
Elapsed time for attention_prob_times_values (80x2048x2048x1145): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1145): 84.849

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1937.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1146x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1146x2048): 81.794
Elapsed time for attention_prob_times_values (80x2048x2048x1146): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1146): 87.344

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1975.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1147x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1147x2048): 81.163
Elapsed time for attention_prob_times_values (80x2048x2048x1147): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1147): 85.127

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1944.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1148x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1148x2048): 79.069
Elapsed time for attention_prob_times_values (80x2048x2048x1148): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1148): 84.832

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1917.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1149x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1149x2048): 78.140
Elapsed time for attention_prob_times_values (80x2048x2048x1149): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1149): 85.190

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1910.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1150x2048): 0.0094
Elapsed time for attention_key_query_prob (32x2048x2000x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2000x2048): 87.370
Elapsed time for attention_prob_times_values (32x2048x2048x2000): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2000): 90.260

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1476.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2001x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2001x2048): 88.852
Elapsed time for attention_prob_times_values (32x2048x2048x2001): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2001): 85.855

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1452.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2002x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2002x2048): 90.072
Elapsed time for attention_prob_times_values (32x2048x2048x2002): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2002): 88.218

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1483.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2003x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2003x2048): 81.183
Elapsed time for attention_prob_times_values (32x2048x2048x2003): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2003): 85.817

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1389.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2004x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2004x2048): 87.257
Elapsed time for attention_prob_times_values (32x2048x2048x2004): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2004): 88.192

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1461.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2005x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2005x2048): 89.191
Elapsed time for attention_prob_times_values (32x2048x2048x2005): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2005): 85.938

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1458.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2006x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2006x2048): 90.180
Elapsed time for attention_prob_times_values (32x2048x2048x2006): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2006): 84.102

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1451.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2007x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2007x2048): 89.090
Elapsed time for attention_prob_times_values (32x2048x2048x2007): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2007): 85.955

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1459.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2008x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2008x2048): 91.538
Elapsed time for attention_prob_times_values (32x2048x2048x2008): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2008): 94.287

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1550.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2009x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2009x2048): 89.631
Elapsed time for attention_prob_times_values (32x2048x2048x2009): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2009): 85.943

Attention duration (in seconds): 0.0123
========================================================================================================================
num_attention_heads: 40, hidden_size: 28600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x715x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x715x2048): 76.282
Elapsed time for attention_prob_times_values (160x2048x2048x715): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x715): 63.979

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2013.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x716x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x716x2048): 76.930
Elapsed time for attention_prob_times_values (160x2048x2048x716): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x716): 66.230

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2062.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x717x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x717x2048): 73.671
Elapsed time for attention_prob_times_values (160x2048x2048x717): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x717): 64.052

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1987.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x718x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x718x2048): 76.242
Elapsed time for attention_prob_times_values (160x2048x2048x718): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x718): 65.462

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2046.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x719x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x719x2048): 74.609
Elapsed time for attention_prob_times_values (160x2048x2048x719): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x719): 62.488

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1978.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x720x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x720x2048): 76.970
Elapsed time for attention_prob_times_values (160x2048x2048x720): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x720): 84.652

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2348.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x721x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x721x2048): 73.281
Elapsed time for attention_prob_times_values (160x2048x2048x721): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x721): 64.289

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 1997.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x722x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x722x2048): 74.566
Elapsed time for attention_prob_times_values (160x2048x2048x722): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x722): 67.123

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2063.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x723x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x723x2048): 74.464
Elapsed time for attention_prob_times_values (160x2048x2048x723): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x723): 64.020

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 2013.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x724x2048): 0.0128
Attention throughput (in TFLOP/s): 1464.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2010x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2010x2048): 90.079
Elapsed time for attention_prob_times_values (32x2048x2048x2010): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2010): 88.101

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1487.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2011x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2011x2048): 88.816
Elapsed time for attention_prob_times_values (32x2048x2048x2011): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2011): 85.947

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1459.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2012x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2012x2048): 90.631
Elapsed time for attention_prob_times_values (32x2048x2048x2012): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2012): 88.358

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1495.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2013x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2013x2048): 86.383
Elapsed time for attention_prob_times_values (32x2048x2048x2013): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2013): 84.032

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1424.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2014x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2014x2048): 89.979
Elapsed time for attention_prob_times_values (32x2048x2048x2014): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2014): 88.163

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1490.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2015x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2015x2048): 89.111
Elapsed time for attention_prob_times_values (32x2048x2048x2015): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2015): 80.561

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1416.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2016x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2016x2048): 93.759
Elapsed time for attention_prob_times_values (32x2048x2048x2016): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2016): 95.656

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1586.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2017x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2017x2048): 87.039
Elapsed time for attention_prob_times_values (32x2048x2048x2017): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2017): 85.955

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1449.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2018x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2018x2048): 87.633
Elapsed time for attention_prob_times_values (32x2048x2048x2018): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2018): 85.221

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1448.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1150x2048): 81.863
Elapsed time for attention_prob_times_values (80x2048x2048x1150): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1150): 85.197

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1958.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1151x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1151x2048): 77.883
Elapsed time for attention_prob_times_values (80x2048x2048x1151): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1151): 85.253

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1911.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1152x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1152x2048): 90.816
Elapsed time for attention_prob_times_values (80x2048x2048x1152): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1152): 93.836

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2169.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1153x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1153x2048): 79.503
Elapsed time for attention_prob_times_values (80x2048x2048x1153): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1153): 76.253

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1830.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1154x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1154x2048): 83.637
Elapsed time for attention_prob_times_values (80x2048x2048x1154): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1154): 79.019

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1912.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1155x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1155x2048): 82.095
Elapsed time for attention_prob_times_values (80x2048x2048x1155): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1155): 78.817

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1894.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1156x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1156x2048): 84.275
Elapsed time for attention_prob_times_values (80x2048x2048x1156): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1156): 76.103

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1885.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1157x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1157x2048): 82.600
Elapsed time for attention_prob_times_values (80x2048x2048x1157): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1157): 78.871

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1904.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1158x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1158x2048): 77.023
Elapsed time for attention_prob_times_values (80x2048x2048x1158): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1158): 77.976

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1830.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1159x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1159x2048): 81.727
Elapsed time for attention_prob_times_values (80x2048x2048x1159): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1159): 79.021

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1899.236
MLP duration (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 16152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2019x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2019x2048): 90.160
Elapsed time for attention_prob_times_values (32x2048x2048x2019): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2019): 81.881

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1439.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2020x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2020x2048): 91.825
Elapsed time for attention_prob_times_values (32x2048x2048x2020): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2020): 86.005

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1490.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2021x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2021x2048): 85.678
Elapsed time for attention_prob_times_values (32x2048x2048x2021): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2021): 83.739

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1421.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2022x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2022x2048): 90.994
Elapsed time for attention_prob_times_values (32x2048x2048x2022): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2022): 88.483

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1507.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2023x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2023x2048): 90.705
Elapsed time for attention_prob_times_values (32x2048x2048x2023): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2023): 82.391

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1451.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2024x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2024x2048): 89.679
Elapsed time for attention_prob_times_values (32x2048x2048x2024): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2024): 90.663

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1515.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2025x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2025x2048): 88.291
Elapsed time for attention_prob_times_values (32x2048x2048x2025): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2025): 85.777

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1463.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2026x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2026x2048): 90.329
Elapsed time for attention_prob_times_values (32x2048x2048x2026): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2026): 88.232

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1502.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2027x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2027x2048): 89.459
Elapsed time for attention_prob_times_values (32x2048x2048x2027): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2027): 85.797

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1474.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2028x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2028x2048): 87.932
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x724x2048): 75.872
Elapsed time for attention_prob_times_values (160x2048x2048x724): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x724): 66.376

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2073.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x725x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x725x2048): 75.981
Elapsed time for attention_prob_times_values (160x2048x2048x725): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x725): 63.989

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2036.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x726x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x726x2048): 74.907
Elapsed time for attention_prob_times_values (160x2048x2048x726): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x726): 66.630

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2070.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x727x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x727x2048): 74.474
Elapsed time for attention_prob_times_values (160x2048x2048x727): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x727): 63.514

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2015.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x728x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x728x2048): 77.103
Elapsed time for attention_prob_times_values (160x2048x2048x728): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x728): 86.611

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2401.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x729x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x729x2048): 75.478
Elapsed time for attention_prob_times_values (160x2048x2048x729): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x729): 62.896

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2022.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x730x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x730x2048): 74.979
Elapsed time for attention_prob_times_values (160x2048x2048x730): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x730): 66.593

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2081.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x731x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x731x2048): 74.297
Elapsed time for attention_prob_times_values (160x2048x2048x731): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x731): 65.198

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2052.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x732x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x732x2048): 74.491
Elapsed time for attention_prob_times_values (160x2048x2048x732): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x732): 68.837

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2117.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x733x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x733x2048): 74.412
Elapsed time for attention_prob_times_values (160x2048x2048x733): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x733): 63.979

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 2038.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1160x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1160x2048): 83.455
Elapsed time for attention_prob_times_values (80x2048x2048x1160): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1160): 85.660

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1999.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1161x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1161x2048): 76.887
Elapsed time for attention_prob_times_values (80x2048x2048x1161): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1161): 79.236

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1847.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1162x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1162x2048): 79.323
Elapsed time for attention_prob_times_values (80x2048x2048x1162): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1162): 81.453

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1904.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1163x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1163x2048): 82.229
Elapsed time for attention_prob_times_values (80x2048x2048x1163): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1163): 79.317

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1914.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1164x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1164x2048): 83.685
Elapsed time for attention_prob_times_values (80x2048x2048x1164): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1164): 81.589

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1961.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1165x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1165x2048): 78.523
Elapsed time for attention_prob_times_values (80x2048x2048x1165): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1165): 78.253

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1862.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1166x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1166x2048): 84.012
Elapsed time for attention_prob_times_values (80x2048x2048x1166): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1166): 81.455

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1966.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1167x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1167x2048): 82.643
Elapsed time for attention_prob_times_values (80x2048x2048x1167): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1167): 79.555

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1928.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1168x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1168x2048): 84.526
Elapsed time for attention_prob_times_values (80x2048x2048x1168): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1168): 87.087

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2042.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (32x2048x2048x2028): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2028): 88.759

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1488.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2029x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2029x2048): 89.338
Elapsed time for attention_prob_times_values (32x2048x2048x2029): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2029): 83.907

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1458.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2030x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2030x2048): 90.518
Elapsed time for attention_prob_times_values (32x2048x2048x2030): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2030): 87.282

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1498.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2031x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2031x2048): 89.405
Elapsed time for attention_prob_times_values (32x2048x2048x2031): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2031): 85.940

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1478.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2032x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2032x2048): 92.277
Elapsed time for attention_prob_times_values (32x2048x2048x2032): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2032): 95.986

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1587.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2033x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2033x2048): 89.269
Elapsed time for attention_prob_times_values (32x2048x2048x2033): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2033): 86.288

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1481.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2034x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2034x2048): 90.510
Elapsed time for attention_prob_times_values (32x2048x2048x2034): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2034): 85.774

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1487.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2035x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2035x2048): 90.265
Elapsed time for attention_prob_times_values (32x2048x2048x2035): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2035): 85.701

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1485.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2036x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2036x2048): 91.230
Elapsed time for attention_prob_times_values (32x2048x2048x2036): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2036): 88.579

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1519.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2037x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2037x2048): 89.799
Elapsed time for attention_prob_times_values (32x2048x2048x2037): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2037): 85.695

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1483.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2038x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2038x2048): 91.421
Elapsed time for attention_prob_times_values (32x2048x2048x2038): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2038): 88.348

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1520.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2039x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2039x2048): 92.409
Elapsed time for attention_prob_times_values (32x2048x2048x2039): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2039): 77.889

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1431.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2040x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2040x2048): 89.755
Elapsed time for attention_prob_times_values (32x2048x2048x2040): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2040): 91.003

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1530.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2041x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2041x2048): 92.514
Elapsed time for attention_prob_times_values (32x2048x2048x2041): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2041): 80.602

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1459.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2042x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2042x2048): 91.010
Elapsed time for attention_prob_times_values (32x2048x2048x2042): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2042): 87.993

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1516.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2043x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2043x2048): 90.060
Elapsed time for attention_prob_times_values (32x2048x2048x2043): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2043): 85.427

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1487.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2044x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2044x2048): 91.052
Elapsed time for attention_prob_times_values (32x2048x2048x2044): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2044): 88.489

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1522.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2045x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2045x2048): 85.062
Elapsed time for attention_prob_times_values (32x2048x2048x2045): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2045): 85.473

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1447.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2046x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2046x2048): 90.128
Elapsed time for attention_prob_times_values (32x2048x2048x2046): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2046): 88.695

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1518.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 20, hidden_size: 23380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1169x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1169x2048): 81.848
Elapsed time for attention_prob_times_values (80x2048x2048x1169): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1169): 79.691

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1924.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1170x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1170x2048): 76.555
Elapsed time for attention_prob_times_values (80x2048x2048x1170): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1170): 81.452

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1882.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1171x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1171x2048): 80.502
Elapsed time for attention_prob_times_values (80x2048x2048x1171): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1171): 78.844

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1901.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1172x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1172x2048): 80.815
Elapsed time for attention_prob_times_values (80x2048x2048x1172): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1172): 81.984

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1944.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1173x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1173x2048): 80.726
Elapsed time for attention_prob_times_values (80x2048x2048x1173): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1173): 79.759

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1918.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1174x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1174x2048): 83.055
Elapsed time for attention_prob_times_values (80x2048x2048x1174): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1174): 78.440

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1930.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1175x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1175x2048): 81.421
Elapsed time for attention_prob_times_values (80x2048x2048x1175): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1175): 76.020

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1883.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1176x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1176x2048): 82.876
Elapsed time for attention_prob_times_values (80x2048x2048x1176): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1176): 83.176

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1990.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1177x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1177x2048): 81.682
Elapsed time for attention_prob_times_values (80x2048x2048x1177): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1177): 78.693

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1922.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1178x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1178x2048): 80.254
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x734x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x734x2048): 73.231
Elapsed time for attention_prob_times_values (160x2048x2048x734): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x734): 66.420

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2066.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x735x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x735x2048): 73.606
Elapsed time for attention_prob_times_values (160x2048x2048x735): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x735): 63.228

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 2021.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x736x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x736x2048): 87.206
Elapsed time for attention_prob_times_values (160x2048x2048x736): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x736): 89.148

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2622.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x737x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x737x2048): 75.356
Elapsed time for attention_prob_times_values (160x2048x2048x737): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x737): 65.704

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 2091.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x738x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x738x2048): 75.324
Elapsed time for attention_prob_times_values (160x2048x2048x738): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x738): 67.410

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2122.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x739x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x739x2048): 74.722
Elapsed time for attention_prob_times_values (160x2048x2048x739): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x739): 65.868

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2091.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x740x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x740x2048): 76.176
Elapsed time for attention_prob_times_values (160x2048x2048x740): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x740): 67.313

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2137.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x741x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x741x2048): 75.080
Elapsed time for attention_prob_times_values (160x2048x2048x741): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x741): 65.882

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2101.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x742x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x742x2048): 77.562
Elapsed time for attention_prob_times_values (160x2048x2048x742): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x742): 67.652

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2166.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (32x2048x2047x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2047x2048): 89.233
Elapsed time for attention_prob_times_values (32x2048x2048x2047): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2047): 83.155

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1462.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2048x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2048x2048): 98.209
Elapsed time for attention_prob_times_values (32x2048x2048x2048): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2048): 98.196

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1669.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2049x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2049x2048): 89.979
Elapsed time for attention_prob_times_values (32x2048x2048x2049): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2049): 81.720

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1456.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2050x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2050x2048): 86.365
Elapsed time for attention_prob_times_values (32x2048x2048x2050): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2050): 85.006

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1457.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2051x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2051x2048): 89.699
Elapsed time for attention_prob_times_values (32x2048x2048x2051): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2051): 82.109

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1459.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2052x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2052x2048): 91.972
Elapsed time for attention_prob_times_values (32x2048x2048x2052): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2052): 81.382

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1470.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2053x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2053x2048): 90.860
Elapsed time for attention_prob_times_values (32x2048x2048x2053): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2053): 80.618

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1455.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2054x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2054x2048): 91.690
Elapsed time for attention_prob_times_values (32x2048x2048x2054): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2054): 84.891

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1502.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2055x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2055x2048): 88.601
Elapsed time for attention_prob_times_values (32x2048x2048x2055): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2055): 81.800

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1450.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2056x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2056x2048): 89.761
Elapsed time for attention_prob_times_values (32x2048x2048x2056): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2056): 85.751

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1496.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2057x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2057x2048): 92.680
Elapsed time for attention_prob_times_values (32x2048x2048x2057): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2057): 78.187

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1447.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2058x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2058x2048): 91.868
Elapsed time for attention_prob_times_values (32x2048x2048x2058): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2058): 83.125

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1490.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2059x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2059x2048): 90.545
Elapsed time for attention_prob_times_values (32x2048x2048x2059): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2059): 82.061

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1471.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2060x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2060x2048): 85.324
Elapsed time for attention_prob_times_values (32x2048x2048x2060): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2060): 84.865

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1454.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2061x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2061x2048): 89.421
Elapsed time for attention_prob_times_values (32x2048x2048x2061): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2061): 73.107

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1375.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2062x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2062x2048): 90.936
Elapsed time for attention_prob_times_values (32x2048x2048x2062): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2062): 80.811

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1464.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2063x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2063x2048): 82.186
Elapsed time for attention_prob_times_values (32x2048x2048x2063): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2063): 82.798

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1412.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2064x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2064x2048): 92.407
Elapsed time for attention_prob_times_values (32x2048x2048x2064): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2064): 91.350

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1573.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2065x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2065x2048): 89.268
Elapsed time for attention_prob_times_values (32x2048x2048x2065): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2065): 78.573

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1431.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (80x2048x2048x1178): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1178): 82.425

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1952.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1179x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1179x2048): 79.007
Elapsed time for attention_prob_times_values (80x2048x2048x1179): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1179): 78.243

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1889.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1180x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1180x2048): 83.127
Elapsed time for attention_prob_times_values (80x2048x2048x1180): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1180): 79.988

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1960.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1181x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1181x2048): 81.666
Elapsed time for attention_prob_times_values (80x2048x2048x1181): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1181): 80.497

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1951.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1182x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1182x2048): 81.125
Elapsed time for attention_prob_times_values (80x2048x2048x1182): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1182): 82.698

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1972.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1183x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1183x2048): 81.686
Elapsed time for attention_prob_times_values (80x2048x2048x1183): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1183): 80.145

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1950.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1184x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1184x2048): 93.957
Elapsed time for attention_prob_times_values (80x2048x2048x1184): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1184): 87.940

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2191.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1185x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1185x2048): 81.939
Elapsed time for attention_prob_times_values (80x2048x2048x1185): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1185): 80.674

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1962.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1186x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1186x2048): 84.366
Elapsed time for attention_prob_times_values (80x2048x2048x1186): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1186): 81.695

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2005.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1187x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1187x2048): 83.369
Elapsed time for attention_prob_times_values (80x2048x2048x1187): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1187): 80.785

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1984.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
num_attention_heads: 8, hidden_size: 16528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2066x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2066x2048): 90.362
Elapsed time for attention_prob_times_values (32x2048x2048x2066): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2066): 79.314

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1448.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2067x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2067x2048): 89.187
Elapsed time for attention_prob_times_values (32x2048x2048x2067): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2067): 83.443

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1478.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2068x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2068x2048): 90.785
Elapsed time for attention_prob_times_values (32x2048x2048x2068): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2068): 85.750

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1513.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2069x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2069x2048): 85.130
Elapsed time for attention_prob_times_values (32x2048x2048x2069): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2069): 79.736

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1413.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2070x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2070x2048): 90.365
Elapsed time for attention_prob_times_values (32x2048x2048x2070): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2070): 82.722

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1483.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2071x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2071x2048): 90.252
Elapsed time for attention_prob_times_values (32x2048x2048x2071): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2071): 83.494

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1490.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2072x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2072x2048): 89.071
Elapsed time for attention_prob_times_values (32x2048x2048x2072): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2072): 91.292

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1549.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2073x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2073x2048): 86.534
Elapsed time for attention_prob_times_values (32x2048x2048x2073): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2073): 83.770

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1463.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2074x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2074x2048): 90.281
Elapsed time for attention_prob_times_values (32x2048x2048x2074): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2074): 81.162

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1470.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2075x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2075x2048): 89.323
num_attention_heads: 40, hidden_size: 29720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x743x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x743x2048): 74.642
Elapsed time for attention_prob_times_values (160x2048x2048x743): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x743): 65.975

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2102.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x744x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x744x2048): 79.042
Elapsed time for attention_prob_times_values (160x2048x2048x744): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x744): 89.854

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2528.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x745x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x745x2048): 76.193
Elapsed time for attention_prob_times_values (160x2048x2048x745): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x745): 65.935

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2127.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x746x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x746x2048): 77.155
Elapsed time for attention_prob_times_values (160x2048x2048x746): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x746): 67.541

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2170.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x747x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x747x2048): 75.517
Elapsed time for attention_prob_times_values (160x2048x2048x747): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x747): 66.142

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 2128.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x748x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x748x2048): 75.096
Elapsed time for attention_prob_times_values (160x2048x2048x748): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x748): 68.460

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2164.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x749x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x749x2048): 74.275
Elapsed time for attention_prob_times_values (160x2048x2048x749): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x749): 63.061

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 2063.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x750x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x750x2048): 72.553
Elapsed time for attention_prob_times_values (160x2048x2048x750): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x750): 68.132

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 2129.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x751x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x751x2048): 74.119
Elapsed time for attention_prob_times_values (160x2048x2048x751): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x751): 65.283

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 2105.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x752x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x752x2048): 74.819
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1188x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1188x2048): 82.517
Elapsed time for attention_prob_times_values (80x2048x2048x1188): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1188): 82.986

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2002.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1189x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1189x2048): 82.652
Elapsed time for attention_prob_times_values (80x2048x2048x1189): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1189): 75.187

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1907.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1190x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1190x2048): 83.970
Elapsed time for attention_prob_times_values (80x2048x2048x1190): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1190): 82.075

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2012.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1191x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1191x2048): 81.638
Elapsed time for attention_prob_times_values (80x2048x2048x1191): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1191): 80.987

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1972.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1192x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1192x2048): 80.700
Elapsed time for attention_prob_times_values (80x2048x2048x1192): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1192): 83.282

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1990.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1193x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1193x2048): 82.468
Elapsed time for attention_prob_times_values (80x2048x2048x1193): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1193): 81.138

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1987.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1194x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1194x2048): 80.256
Elapsed time for attention_prob_times_values (80x2048x2048x1194): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1194): 83.474

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1990.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1195x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1195x2048): 76.862
Elapsed time for attention_prob_times_values (80x2048x2048x1195): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1195): 78.035

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1884.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1196x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1196x2048): 84.435
Elapsed time for attention_prob_times_values (80x2048x2048x1196): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1196): 83.595

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2046.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_prob_times_values (32x2048x2048x2075): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2075): 78.075

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1434.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2076x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2076x2048): 81.305
Elapsed time for attention_prob_times_values (32x2048x2048x2076): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2076): 86.285

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1441.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2077x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2077x2048): 83.745
Elapsed time for attention_prob_times_values (32x2048x2048x2077): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2077): 77.459

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1386.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2078x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2078x2048): 90.239
Elapsed time for attention_prob_times_values (32x2048x2048x2078): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2078): 86.126

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1518.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2079x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2079x2048): 89.393
Elapsed time for attention_prob_times_values (32x2048x2048x2079): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2079): 80.657

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1462.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2080x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2080x2048): 92.509
Elapsed time for attention_prob_times_values (32x2048x2048x2080): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2080): 87.377

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1550.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2081x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2081x2048): 89.581
Elapsed time for attention_prob_times_values (32x2048x2048x2081): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2081): 84.217

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1498.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2082x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2082x2048): 86.614
Elapsed time for attention_prob_times_values (32x2048x2048x2082): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2082): 86.314

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1492.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2083x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2083x2048): 85.822
Elapsed time for attention_prob_times_values (32x2048x2048x2083): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2083): 83.774

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1464.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2084x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2084x2048): 90.966
Elapsed time for attention_prob_times_values (32x2048x2048x2084): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2084): 86.675

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1534.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2085x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2085x2048): 90.029
Elapsed time for attention_prob_times_values (32x2048x2048x2085): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2085): 81.772

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1481.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2086x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2086x2048): 91.043
Elapsed time for attention_prob_times_values (32x2048x2048x2086): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2086): 86.604

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1535.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2087x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2087x2048): 90.005
Elapsed time for attention_prob_times_values (32x2048x2048x2087): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2087): 84.293

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1506.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2088x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2088x2048): 92.653
Elapsed time for attention_prob_times_values (32x2048x2048x2088): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2088): 92.087

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1599.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2089x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2089x2048): 86.719
Elapsed time for attention_prob_times_values (32x2048x2048x2089): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2089): 84.547

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1482.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2090x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2090x2048): 88.172
Elapsed time for attention_prob_times_values (32x2048x2048x2090): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2090): 83.628

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1487.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2091x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2091x2048): 89.778
Elapsed time for attention_prob_times_values (32x2048x2048x2091): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2091): 82.617

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1491.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2092x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2092x2048): 91.418
Elapsed time for attention_prob_times_values (32x2048x2048x2092): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2092): 85.866

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1535.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2093x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2093x2048): 84.149
Elapsed time for attention_prob_times_values (32x2048x2048x2093): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2093): 84.593

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1463.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
--------
Elapsed time for attention_key_query_prob (80x2048x1197x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1197x2048): 82.736
Elapsed time for attention_prob_times_values (80x2048x2048x1197): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1197): 81.183

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1997.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1198x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1198x2048): 78.318
Elapsed time for attention_prob_times_values (80x2048x2048x1198): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1198): 80.177

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1933.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1199x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1199x2048): 81.954
Elapsed time for attention_prob_times_values (80x2048x2048x1199): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1199): 81.391

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1994.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1200x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1200x2048): 84.324
Elapsed time for attention_prob_times_values (80x2048x2048x1200): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1200): 89.086

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2117.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1201x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1201x2048): 80.278
Elapsed time for attention_prob_times_values (80x2048x2048x1201): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1201): 81.556

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1978.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1202x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1202x2048): 83.165
Elapsed time for attention_prob_times_values (80x2048x2048x1202): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1202): 78.851

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1981.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1203x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1203x2048): 76.466
Elapsed time for attention_prob_times_values (80x2048x2048x1203): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1203): 81.606

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1934.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1204x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1204x2048): 81.379
Elapsed time for attention_prob_times_values (80x2048x2048x1204): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1204): 84.106

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2027.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1205x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1205x2048): 82.579
Elapsed time for attention_prob_times_values (80x2048x2048x1205): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1205): 77.288

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1959.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1206x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1206x2048): 79.001
Elapsed time for attention_prob_times_values (80x2048x2048x1206): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1206): 83.985

Attention duration (in seconds): 0.0199
Elapsed time for attention_prob_times_values (160x2048x2048x752): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x752): 91.178

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2496.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x753x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x753x2048): 74.262
Elapsed time for attention_prob_times_values (160x2048x2048x753): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x753): 66.252

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 2129.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x754x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x754x2048): 74.368
Elapsed time for attention_prob_times_values (160x2048x2048x754): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x754): 68.048

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2164.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x755x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x755x2048): 76.088
Elapsed time for attention_prob_times_values (160x2048x2048x755): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x755): 66.905

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2171.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x756x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x756x2048): 77.286
Elapsed time for attention_prob_times_values (160x2048x2048x756): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x756): 65.799

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2170.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x757x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x757x2048): 73.370
Elapsed time for attention_prob_times_values (160x2048x2048x757): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x757): 66.555

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2133.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x758x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x758x2048): 73.104
Elapsed time for attention_prob_times_values (160x2048x2048x758): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x758): 67.199

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2143.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x759x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x759x2048): 74.457
Elapsed time for attention_prob_times_values (160x2048x2048x759): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x759): 64.633

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2120.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x760x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x760x2048): 74.672
Elapsed time for attention_prob_times_values (160x2048x2048x760): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x760): 90.298

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2508.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x761x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x761x2048): 75.111
Elapsed time for attention_prob_times_values (160x2048x2048x761): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x761): 65.864

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2156.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Elapsed time for attention_key_query_prob (32x2048x2094x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2094x2048): 87.139
Elapsed time for attention_prob_times_values (32x2048x2048x2094): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2094): 86.834

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1510.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2095x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2095x2048): 88.218
Elapsed time for attention_prob_times_values (32x2048x2048x2095): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2095): 84.747

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1501.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2096x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2096x2048): 91.957
Elapsed time for attention_prob_times_values (32x2048x2048x2096): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2096): 90.110

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1581.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2097x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2097x2048): 82.813
Elapsed time for attention_prob_times_values (32x2048x2048x2097): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2097): 82.947

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1440.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2098x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2098x2048): 90.268
Elapsed time for attention_prob_times_values (32x2048x2048x2098): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2098): 87.260

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1543.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2099x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2099x2048): 89.399
Elapsed time for attention_prob_times_values (32x2048x2048x2099): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2099): 82.470

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1492.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2100x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2100x2048): 91.243
Elapsed time for attention_prob_times_values (32x2048x2048x2100): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2100): 84.882

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1530.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2101x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2101x2048): 89.627
Elapsed time for attention_prob_times_values (32x2048x2048x2101): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2101): 84.959

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1519.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2102x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2102x2048): 90.316
Elapsed time for attention_prob_times_values (32x2048x2048x2102): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2102): 87.224

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1546.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2103x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2103x2048): 83.472
Elapsed time for attention_prob_times_values (32x2048x2048x2103): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2103): 79.012

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1414.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2104x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2104x2048): 91.651
Elapsed time for attention_prob_times_values (32x2048x2048x2104): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2104): 92.912

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1609.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2105x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2105x2048): 89.391
Elapsed time for attention_prob_times_values (32x2048x2048x2105): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2105): 85.124

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1521.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2106x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2106x2048): 87.088
Elapsed time for attention_prob_times_values (32x2048x2048x2106): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2106): 87.322

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1521.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2107x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2107x2048): 84.869
Elapsed time for attention_prob_times_values (32x2048x2048x2107): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2107): 85.291

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1485.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2108x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2108x2048): 91.067
Elapsed time for attention_prob_times_values (32x2048x2048x2108): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2108): 83.735

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1524.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2109x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2109x2048): 89.328
Elapsed time for attention_prob_times_values (32x2048x2048x2109): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2109): 79.325

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1468.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2110x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2110x2048): 90.205
Elapsed time for attention_prob_times_values (32x2048x2048x2110): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2110): 87.414

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1552.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2111x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2111x2048): 83.722
Elapsed time for attention_prob_times_values (32x2048x2048x2111): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2111): 85.400

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1479.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2112x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2112x2048): 99.565
Elapsed time for attention_prob_times_values (32x2048x2048x2112): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2112): 94.926

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1700.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 1999.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1207x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1207x2048): 80.580
Elapsed time for attention_prob_times_values (80x2048x2048x1207): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1207): 81.836

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1995.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1208x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1208x2048): 83.755
Elapsed time for attention_prob_times_values (80x2048x2048x1208): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1208): 89.454

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2127.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1209x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1209x2048): 77.930
Elapsed time for attention_prob_times_values (80x2048x2048x1209): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1209): 79.882

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1941.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1210x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1210x2048): 77.499
Elapsed time for attention_prob_times_values (80x2048x2048x1210): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1210): 83.591

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1981.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1211x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1211x2048): 79.194
Elapsed time for attention_prob_times_values (80x2048x2048x1211): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1211): 78.021

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1937.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1212x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1212x2048): 79.105
Elapsed time for attention_prob_times_values (80x2048x2048x1212): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1212): 81.158

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1976.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1213x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1213x2048): 80.031
Elapsed time for attention_prob_times_values (80x2048x2048x1213): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1213): 80.691

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1984.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1214x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1214x2048): 83.091
Elapsed time for attention_prob_times_values (80x2048x2048x1214): 0.0321
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1214): 25.400

Attention duration (in seconds): 0.0419
Attention throughput (in TFLOP/s): 961.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0419
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1215x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1215x2048): 77.625
Elapsed time for attention_prob_times_values (80x2048x2048x1215): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1215): 82.291

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1975.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 16904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2113x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2113x2048): 90.516
Elapsed time for attention_prob_times_values (32x2048x2048x2113): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2113): 83.191

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1517.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2114x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2114x2048): 91.397
Elapsed time for attention_prob_times_values (32x2048x2048x2114): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2114): 86.075

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1552.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2115x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2115x2048): 90.357
Elapsed time for attention_prob_times_values (32x2048x2048x2115): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2115): 85.587

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1540.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2116x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2116x2048): 89.235
Elapsed time for attention_prob_times_values (32x2048x2048x2116): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2116): 84.919

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1525.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2117x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2117x2048): 87.344
Elapsed time for attention_prob_times_values (32x2048x2048x2117): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2117): 85.625

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1516.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2118x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2118x2048): 91.101
Elapsed time for attention_prob_times_values (32x2048x2048x2118): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2118): 87.771

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1568.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2119x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2119x2048): 86.831
Elapsed time for attention_prob_times_values (32x2048x2048x2119): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2119): 82.442

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1484.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2120x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2120x2048): 89.214
Elapsed time for attention_prob_times_values (32x2048x2048x2120): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2120): 93.674

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1605.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2121x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2121x2048): 89.708
Elapsed time for attention_prob_times_values (32x2048x2048x2121): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2121): 85.863

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1541.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2122x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2122x2048): 87.081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x762x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x762x2048): 75.674
Elapsed time for attention_prob_times_values (160x2048x2048x762): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x762): 68.502

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 2212.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x763x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x763x2048): 75.350
Elapsed time for attention_prob_times_values (160x2048x2048x763): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x763): 65.795

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2164.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x764x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x764x2048): 76.183
Elapsed time for attention_prob_times_values (160x2048x2048x764): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x764): 67.997

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2216.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x765x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x765x2048): 74.957
Elapsed time for attention_prob_times_values (160x2048x2048x765): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x765): 66.522

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2176.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x766x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x766x2048): 75.827
Elapsed time for attention_prob_times_values (160x2048x2048x766): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x766): 64.675

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 2158.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x767x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x767x2048): 72.842
Elapsed time for attention_prob_times_values (160x2048x2048x767): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x767): 62.339

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2080.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x768x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x768x2048): 83.663
Elapsed time for attention_prob_times_values (160x2048x2048x768): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x768): 94.261

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2748.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x769x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x769x2048): 73.835
Elapsed time for attention_prob_times_values (160x2048x2048x769): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x769): 60.316

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2060.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x770x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x770x2048): 76.208
Elapsed time for attention_prob_times_values (160x2048x2048x770): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x770): 63.881

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 2159.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
========================================================================================================================
num_attention_heads: 20, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1216x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1216x2048): 91.770
Elapsed time for attention_prob_times_values (80x2048x2048x1216): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1216): 89.810

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2246.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1217x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1217x2048): 83.771
Elapsed time for attention_prob_times_values (80x2048x2048x1217): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1217): 79.479

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2020.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1218x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1218x2048): 81.636
Elapsed time for attention_prob_times_values (80x2048x2048x1218): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1218): 84.804

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2062.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1219x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1219x2048): 79.350
Elapsed time for attention_prob_times_values (80x2048x2048x1219): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1219): 78.200

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1954.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1220x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1220x2048): 85.602
Elapsed time for attention_prob_times_values (80x2048x2048x1220): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1220): 84.994

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2117.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1221x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1221x2048): 81.609
Elapsed time for attention_prob_times_values (80x2048x2048x1221): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1221): 82.604

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2040.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1222x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1222x2048): 82.072
Elapsed time for attention_prob_times_values (80x2048x2048x1222): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1222): 84.954

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2076.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1223x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1223x2048): 83.241
Elapsed time for attention_prob_times_values (80x2048x2048x1223): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1223): 82.622

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2063.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1224x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1224x2048): 83.621
Elapsed time for attention_prob_times_values (80x2048x2048x1224): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1224): 90.607

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2166.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1225x2048): 0.0104
Elapsed time for attention_prob_times_values (32x2048x2048x2122): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2122): 84.966

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1511.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2123x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2123x2048): 87.454
Elapsed time for attention_prob_times_values (32x2048x2048x2123): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2123): 85.806

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1523.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2124x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2124x2048): 91.380
Elapsed time for attention_prob_times_values (32x2048x2048x2124): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2124): 86.589

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1564.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2125x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2125x2048): 89.739
Elapsed time for attention_prob_times_values (32x2048x2048x2125): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2125): 82.467

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1512.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2126x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2126x2048): 90.700
Elapsed time for attention_prob_times_values (32x2048x2048x2126): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2126): 88.002

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1573.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2127x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2127x2048): 89.736
Elapsed time for attention_prob_times_values (32x2048x2048x2127): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2127): 86.113

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1548.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2128x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2128x2048): 91.468
Elapsed time for attention_prob_times_values (32x2048x2048x2128): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2128): 94.320

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1636.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2129x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2129x2048): 89.475
Elapsed time for attention_prob_times_values (32x2048x2048x2129): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2129): 86.224

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1548.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2130x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2130x2048): 90.446
Elapsed time for attention_prob_times_values (32x2048x2048x2130): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2130): 87.932

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1573.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2131x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2131x2048): 89.422
Elapsed time for attention_prob_times_values (32x2048x2048x2131): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2131): 86.337

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1550.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2132x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2132x2048): 91.073
Elapsed time for attention_prob_times_values (32x2048x2048x2132): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2132): 88.316

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1583.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2133x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2133x2048): 88.815
Elapsed time for attention_prob_times_values (32x2048x2048x2133): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2133): 86.411

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1547.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2134x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2134x2048): 90.333
Elapsed time for attention_prob_times_values (32x2048x2048x2134): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2134): 86.096

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1558.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2135x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2135x2048): 89.644
Elapsed time for attention_prob_times_values (32x2048x2048x2135): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2135): 86.074

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1552.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2136x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2136x2048): 91.539
Elapsed time for attention_prob_times_values (32x2048x2048x2136): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2136): 93.843

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1639.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2137x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2137x2048): 89.291
Elapsed time for attention_prob_times_values (32x2048x2048x2137): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2137): 86.533

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1555.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2138x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2138x2048): 88.832
Elapsed time for attention_prob_times_values (32x2048x2048x2138): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2138): 88.376

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1568.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2139x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2139x2048): 89.528
Elapsed time for attention_prob_times_values (32x2048x2048x2139): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2139): 84.635

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1541.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2140x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2140x2048): 91.090
Elapsed time for attention_prob_times_values (32x2048x2048x2140): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2140): 88.510

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1590.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1225x2048): 79.281
Elapsed time for attention_prob_times_values (80x2048x2048x1225): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1225): 82.892

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2020.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1226x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1226x2048): 84.236
Elapsed time for attention_prob_times_values (80x2048x2048x1226): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1226): 82.743

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2082.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1227x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1227x2048): 79.727
Elapsed time for attention_prob_times_values (80x2048x2048x1227): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1227): 82.898

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2029.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1228x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1228x2048): 81.666
Elapsed time for attention_prob_times_values (80x2048x2048x1228): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1228): 85.418

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2086.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1229x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1229x2048): 82.906
Elapsed time for attention_prob_times_values (80x2048x2048x1229): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1229): 83.047

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2074.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1230x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1230x2048): 83.772
Elapsed time for attention_prob_times_values (80x2048x2048x1230): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1230): 85.458

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2117.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1231x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1231x2048): 82.969
Elapsed time for attention_prob_times_values (80x2048x2048x1231): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1231): 83.260

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2081.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1232x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1232x2048): 85.468
Elapsed time for attention_prob_times_values (80x2048x2048x1232): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1232): 91.716

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2217.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1233x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1233x2048): 82.600
Elapsed time for attention_prob_times_values (80x2048x2048x1233): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1233): 83.434

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2082.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1234x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1234x2048): 83.519
Elapsed time for attention_prob_times_values (80x2048x2048x1234): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1234): 85.719

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2123.709
MLP duration (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (160x2048x771x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x771x2048): 75.601
Elapsed time for attention_prob_times_values (160x2048x2048x771): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x771): 62.090

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 2121.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x772x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x772x2048): 77.578
Elapsed time for attention_prob_times_values (160x2048x2048x772): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x772): 65.344

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2210.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x773x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x773x2048): 76.568
Elapsed time for attention_prob_times_values (160x2048x2048x773): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x773): 62.014

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 2137.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x774x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x774x2048): 77.528
Elapsed time for attention_prob_times_values (160x2048x2048x774): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x774): 65.364

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 2215.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x775x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x775x2048): 76.133
Elapsed time for attention_prob_times_values (160x2048x2048x775): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x775): 62.517

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 2147.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x776x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x776x2048): 77.717
Elapsed time for attention_prob_times_values (160x2048x2048x776): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x776): 80.352

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2474.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x777x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x777x2048): 75.880
Elapsed time for attention_prob_times_values (160x2048x2048x777): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x777): 60.886

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2118.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x778x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x778x2048): 76.254
Elapsed time for attention_prob_times_values (160x2048x2048x778): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x778): 65.195

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 2206.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x779x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x779x2048): 74.719
Elapsed time for attention_prob_times_values (160x2048x2048x779): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x779): 61.875

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2127.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x780x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x780x2048): 77.658
Elapsed time for attention_prob_times_values (160x2048x2048x780): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x780): 65.837

Attention duration (in seconds): 0.0294
Elapsed time for attention_key_query_prob (32x2048x2141x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2141x2048): 89.492
Elapsed time for attention_prob_times_values (32x2048x2048x2141): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2141): 86.573

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1560.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2142x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2142x2048): 87.162
Elapsed time for attention_prob_times_values (32x2048x2048x2142): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2142): 87.285

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1546.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2143x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2143x2048): 84.639
Elapsed time for attention_prob_times_values (32x2048x2048x2143): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2143): 86.586

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1518.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2144x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2144x2048): 96.058
Elapsed time for attention_prob_times_values (32x2048x2048x2144): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2144): 95.424

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1699.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2145x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2145x2048): 89.005
Elapsed time for attention_prob_times_values (32x2048x2048x2145): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2145): 84.031

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1535.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2146x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2146x2048): 86.505
Elapsed time for attention_prob_times_values (32x2048x2048x2146): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2146): 88.596

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1555.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2147x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2147x2048): 90.262
Elapsed time for attention_prob_times_values (32x2048x2048x2147): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2147): 86.419

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1569.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2148x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2148x2048): 91.886
Elapsed time for attention_prob_times_values (32x2048x2048x2148): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2148): 87.038

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1589.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2149x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2149x2048): 90.020
Elapsed time for attention_prob_times_values (32x2048x2048x2149): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2149): 86.493

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1569.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2150x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2150x2048): 91.072
Elapsed time for attention_prob_times_values (32x2048x2048x2150): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2150): 88.582

Attention duration (in seconds): 0.0129
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1235x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1235x2048): 82.631
Elapsed time for attention_prob_times_values (80x2048x2048x1235): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1235): 83.380

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2085.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1236x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1236x2048): 84.110
Elapsed time for attention_prob_times_values (80x2048x2048x1236): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1236): 85.513

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2132.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1237x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1237x2048): 82.722
Elapsed time for attention_prob_times_values (80x2048x2048x1237): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1237): 72.932

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1950.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1238x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1238x2048): 83.469
Elapsed time for attention_prob_times_values (80x2048x2048x1238): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1238): 78.819

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2041.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1239x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1239x2048): 82.724
Elapsed time for attention_prob_times_values (80x2048x2048x1239): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1239): 72.490

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1947.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1240x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1240x2048): 82.338
Elapsed time for attention_prob_times_values (80x2048x2048x1240): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1240): 88.487

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2151.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1241x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1241x2048): 81.938
Elapsed time for attention_prob_times_values (80x2048x2048x1241): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1241): 72.507

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1941.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1242x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1242x2048): 80.364
Elapsed time for attention_prob_times_values (80x2048x2048x1242): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1242): 79.179

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2014.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1243x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1243x2048): 78.735
Elapsed time for attention_prob_times_values (80x2048x2048x1243): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1243): 71.555

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1895.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 1598.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2151x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2151x2048): 90.013
Elapsed time for attention_prob_times_values (32x2048x2048x2151): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2151): 86.707

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1572.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2152x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2152x2048): 91.935
Elapsed time for attention_prob_times_values (32x2048x2048x2152): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2152): 94.776

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1662.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2153x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2153x2048): 89.674
Elapsed time for attention_prob_times_values (32x2048x2048x2153): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2153): 82.696

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1533.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2154x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2154x2048): 86.962
Elapsed time for attention_prob_times_values (32x2048x2048x2154): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2154): 88.955

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1567.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2155x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2155x2048): 89.780
Elapsed time for attention_prob_times_values (32x2048x2048x2155): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2155): 86.336

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1570.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2156x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2156x2048): 91.254
Elapsed time for attention_prob_times_values (32x2048x2048x2156): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2156): 88.943

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1607.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2157x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2157x2048): 89.957
Elapsed time for attention_prob_times_values (32x2048x2048x2157): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2157): 86.444

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1573.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2158x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2158x2048): 90.918
Elapsed time for attention_prob_times_values (32x2048x2048x2158): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2158): 86.104

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1579.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2159x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2159x2048): 87.355
Elapsed time for attention_prob_times_values (32x2048x2048x2159): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2159): 86.494

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1553.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2160x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2160x2048): 91.990
Elapsed time for attention_prob_times_values (32x2048x2048x2160): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2160): 95.576

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1675.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2161x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2161x2048): 89.225
Elapsed time for attention_prob_times_values (32x2048x2048x2161): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2161): 86.657

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1572.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2162x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2162x2048): 90.619
Elapsed time for attention_prob_times_values (32x2048x2048x2162): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2162): 89.077

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1607.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2163x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2163x2048): 89.511
Elapsed time for attention_prob_times_values (32x2048x2048x2163): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2163): 84.454

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1555.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2164x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2164x2048): 88.415
Elapsed time for attention_prob_times_values (32x2048x2048x2164): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2164): 89.241

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1590.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2165x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2165x2048): 89.693
Elapsed time for attention_prob_times_values (32x2048x2048x2165): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2165): 86.725

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1579.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2166x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2166x2048): 87.625
Elapsed time for attention_prob_times_values (32x2048x2048x2166): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2166): 84.925

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1545.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2167x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2167x2048): 89.627
Elapsed time for attention_prob_times_values (32x2048x2048x2167): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2167): 86.719

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1580.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2168x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2168x2048): 86.372
Elapsed time for attention_prob_times_values (32x2048x2048x2168): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2168): 95.469

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1626.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2169x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2169x2048): 84.662
Attention throughput (in TFLOP/s): 2242.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x781x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x781x2048): 76.128
Elapsed time for attention_prob_times_values (160x2048x2048x781): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x781): 61.952

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2152.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x782x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x782x2048): 76.643
Elapsed time for attention_prob_times_values (160x2048x2048x782): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x782): 64.206

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2204.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x783x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x783x2048): 73.993
Elapsed time for attention_prob_times_values (160x2048x2048x783): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x783): 60.968

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 2111.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x784x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x784x2048): 76.170
Elapsed time for attention_prob_times_values (160x2048x2048x784): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x784): 82.253

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2501.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x785x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x785x2048): 73.873
Elapsed time for attention_prob_times_values (160x2048x2048x785): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x785): 62.057

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2135.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x786x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x786x2048): 77.261
Elapsed time for attention_prob_times_values (160x2048x2048x786): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x786): 66.046

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 2257.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x787x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x787x2048): 76.684
Elapsed time for attention_prob_times_values (160x2048x2048x787): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x787): 62.979

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2195.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x788x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x788x2048): 77.935
Elapsed time for attention_prob_times_values (160x2048x2048x788): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x788): 65.944

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 2270.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x789x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x789x2048): 76.806
Elapsed time for attention_prob_times_values (160x2048x2048x789): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x789): 62.464

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2192.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 20, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1244x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1244x2048): 83.894
Elapsed time for attention_prob_times_values (80x2048x2048x1244): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1244): 79.521

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2065.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1245x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1245x2048): 81.998
Elapsed time for attention_prob_times_values (80x2048x2048x1245): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1245): 74.276

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1973.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1246x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1246x2048): 83.108
Elapsed time for attention_prob_times_values (80x2048x2048x1246): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1246): 79.423

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2057.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1247x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1247x2048): 82.213
Elapsed time for attention_prob_times_values (80x2048x2048x1247): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1247): 74.830

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1986.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1248x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1248x2048): 94.104
Elapsed time for attention_prob_times_values (80x2048x2048x1248): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1248): 90.487

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2341.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1249x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1249x2048): 81.561
Elapsed time for attention_prob_times_values (80x2048x2048x1249): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1249): 74.558

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1978.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1250x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1250x2048): 83.482
Elapsed time for attention_prob_times_values (80x2048x2048x1250): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1250): 77.943

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2048.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1251x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1251x2048): 82.900
Elapsed time for attention_prob_times_values (80x2048x2048x1251): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1251): 74.355

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1993.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1252x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1252x2048): 78.921
Elapsed time for attention_prob_times_values (80x2048x2048x1252): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1252): 74.910

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1956.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1253x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1253x2048): 82.771
Elapsed time for attention_prob_times_values (32x2048x2048x2169): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2169): 86.897

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1539.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2170x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2170x2048): 88.107
Elapsed time for attention_prob_times_values (32x2048x2048x2170): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2170): 85.111

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1554.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2171x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2171x2048): 89.369
Elapsed time for attention_prob_times_values (32x2048x2048x2171): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2171): 84.958

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1564.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2172x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2172x2048): 90.875
Elapsed time for attention_prob_times_values (32x2048x2048x2172): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2172): 89.630

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1621.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2173x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2173x2048): 86.574
Elapsed time for attention_prob_times_values (32x2048x2048x2173): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2173): 86.857

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1558.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2174x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2174x2048): 87.499
Elapsed time for attention_prob_times_values (32x2048x2048x2174): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2174): 89.594

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1592.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2175x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2175x2048): 89.593
Elapsed time for attention_prob_times_values (32x2048x2048x2175): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2175): 86.863

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1587.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2176x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2176x2048): 97.526
Elapsed time for attention_prob_times_values (32x2048x2048x2176): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2176): 95.270

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1734.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2177x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2177x2048): 90.466
Elapsed time for attention_prob_times_values (32x2048x2048x2177): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2177): 82.953

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1558.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2178x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2178x2048): 89.631
Elapsed time for attention_prob_times_values (32x2048x2048x2178): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2178): 85.859

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1580.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2179x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2179x2048): 89.803
Elapsed time for attention_prob_times_values (32x2048x2048x2179): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2179): 83.201

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1556.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2180x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2180x2048): 91.766
Elapsed time for attention_prob_times_values (32x2048x2048x2180): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2180): 84.140

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1582.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2181x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2181x2048): 90.090
Elapsed time for attention_prob_times_values (32x2048x2048x2181): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2181): 83.384

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1562.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2182x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2182x2048): 89.413
Elapsed time for attention_prob_times_values (32x2048x2048x2182): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2182): 85.866

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1580.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2183x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2183x2048): 87.924
Elapsed time for attention_prob_times_values (32x2048x2048x2183): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2183): 81.146

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1523.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2184x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2184x2048): 91.914
Elapsed time for attention_prob_times_values (32x2048x2048x2184): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2184): 88.980

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1633.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2185x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2185x2048): 89.667
Elapsed time for attention_prob_times_values (32x2048x2048x2185): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2185): 82.176

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1549.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2186x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2186x2048): 90.586
Elapsed time for attention_prob_times_values (32x2048x2048x2186): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2186): 85.924

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1594.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2187x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2187x2048): 87.853
Elapsed time for attention_prob_times_values (32x2048x2048x2187): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2187): 83.639

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1549.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_prob_times_values (80x2048x2048x1253): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1253): 74.270

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1994.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1254x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1254x2048): 83.817
Elapsed time for attention_prob_times_values (80x2048x2048x1254): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1254): 79.843

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2084.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1255x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1255x2048): 78.728
Elapsed time for attention_prob_times_values (80x2048x2048x1255): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1255): 70.660

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1900.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1256x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1256x2048): 85.246
Elapsed time for attention_prob_times_values (80x2048x2048x1256): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1256): 92.764

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2268.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1257x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1257x2048): 79.934
Elapsed time for attention_prob_times_values (80x2048x2048x1257): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1257): 74.102

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1965.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1258x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1258x2048): 83.232
Elapsed time for attention_prob_times_values (80x2048x2048x1258): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1258): 73.751

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1999.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1259x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1259x2048): 82.373
Elapsed time for attention_prob_times_values (80x2048x2048x1259): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1259): 74.615

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2003.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1260x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1260x2048): 80.744
Elapsed time for attention_prob_times_values (80x2048x2048x1260): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1260): 80.367

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2062.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1261x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1261x2048): 82.592
Elapsed time for attention_prob_times_values (80x2048x2048x1261): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1261): 73.662

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1995.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1262x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1262x2048): 82.667
Elapsed time for attention_prob_times_values (80x2048x2048x1262): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1262): 78.172

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2061.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
========================================================================================================================
num_attention_heads: 40, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x790x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x790x2048): 74.922
Elapsed time for attention_prob_times_values (160x2048x2048x790): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x790): 64.387

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2206.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x791x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x791x2048): 75.459
Elapsed time for attention_prob_times_values (160x2048x2048x791): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x791): 60.803

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 2148.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x792x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x792x2048): 77.587
Elapsed time for attention_prob_times_values (160x2048x2048x792): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x792): 81.789

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2543.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x793x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x793x2048): 75.301
Elapsed time for attention_prob_times_values (160x2048x2048x793): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x793): 62.826

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2190.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x794x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x794x2048): 75.321
Elapsed time for attention_prob_times_values (160x2048x2048x794): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x794): 63.774

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2211.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x795x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x795x2048): 73.568
Elapsed time for attention_prob_times_values (160x2048x2048x795): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x795): 62.635

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 2168.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x796x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x796x2048): 74.162
Elapsed time for attention_prob_times_values (160x2048x2048x796): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x796): 66.816

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 2256.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x797x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x797x2048): 75.494
Elapsed time for attention_prob_times_values (160x2048x2048x797): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x797): 62.800

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2203.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x798x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x798x2048): 74.917
Elapsed time for attention_prob_times_values (160x2048x2048x798): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x798): 65.483

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2248.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x799x2048): 0.0142
Elapsed time for attention_key_query_prob (32x2048x2188x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2188x2048): 91.250
Elapsed time for attention_prob_times_values (32x2048x2048x2188): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2188): 83.536

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1578.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2189x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2189x2048): 89.657
Elapsed time for attention_prob_times_values (32x2048x2048x2189): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2189): 81.094

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1541.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2190x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2190x2048): 90.785
Elapsed time for attention_prob_times_values (32x2048x2048x2190): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2190): 86.112

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1600.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2191x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2191x2048): 87.048
Elapsed time for attention_prob_times_values (32x2048x2048x2191): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2191): 83.892

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1547.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2192x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2192x2048): 89.380
Elapsed time for attention_prob_times_values (32x2048x2048x2192): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2192): 91.471

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1638.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2193x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2193x2048): 89.844
Elapsed time for attention_prob_times_values (32x2048x2048x2193): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2193): 83.886

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1573.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2194x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2194x2048): 90.757
Elapsed time for attention_prob_times_values (32x2048x2048x2194): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2194): 86.233

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1604.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2195x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2195x2048): 89.665
Elapsed time for attention_prob_times_values (32x2048x2048x2195): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2195): 84.060

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1574.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2196x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2196x2048): 91.340
Elapsed time for attention_prob_times_values (32x2048x2048x2196): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2196): 86.426

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1612.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2197x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2197x2048): 89.845
Elapsed time for attention_prob_times_values (32x2048x2048x2197): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2197): 84.085

Attention duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1263x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1263x2048): 82.610
Elapsed time for attention_prob_times_values (80x2048x2048x1263): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1263): 71.510

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1967.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1264x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1264x2048): 84.672
Elapsed time for attention_prob_times_values (80x2048x2048x1264): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1264): 93.768

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2285.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1265x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1265x2048): 81.149
Elapsed time for attention_prob_times_values (80x2048x2048x1265): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1265): 72.699

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1971.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1266x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1266x2048): 83.117
Elapsed time for attention_prob_times_values (80x2048x2048x1266): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1266): 80.389

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2102.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1267x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1267x2048): 82.398
Elapsed time for attention_prob_times_values (80x2048x2048x1267): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1267): 74.489

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2014.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1268x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1268x2048): 83.578
Elapsed time for attention_prob_times_values (80x2048x2048x1268): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1268): 80.785

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2116.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1269x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1269x2048): 82.449
Elapsed time for attention_prob_times_values (80x2048x2048x1269): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1269): 72.965

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1996.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1270x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1270x2048): 83.422
Elapsed time for attention_prob_times_values (80x2048x2048x1270): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1270): 76.561

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2060.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1271x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1271x2048): 82.768
Elapsed time for attention_prob_times_values (80x2048x2048x1271): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1271): 74.459

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2024.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Attention throughput (in TFLOP/s): 1577.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2198x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2198x2048): 90.842
Elapsed time for attention_prob_times_values (32x2048x2048x2198): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2198): 86.321

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1608.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2199x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2199x2048): 89.653
Elapsed time for attention_prob_times_values (32x2048x2048x2199): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2199): 84.135

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1578.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2200x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2200x2048): 88.457
Elapsed time for attention_prob_times_values (32x2048x2048x2200): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2200): 91.378

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1634.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2201x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2201x2048): 86.505
Elapsed time for attention_prob_times_values (32x2048x2048x2201): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2201): 80.760

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1519.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2202x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2202x2048): 90.460
Elapsed time for attention_prob_times_values (32x2048x2048x2202): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2202): 86.330

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1608.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2203x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2203x2048): 89.456
Elapsed time for attention_prob_times_values (32x2048x2048x2203): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2203): 84.337

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1581.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2204x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2204x2048): 90.827
Elapsed time for attention_prob_times_values (32x2048x2048x2204): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2204): 86.695

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1616.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2205x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2205x2048): 89.454
Elapsed time for attention_prob_times_values (32x2048x2048x2205): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2205): 84.331

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1582.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2206x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2206x2048): 90.509
Elapsed time for attention_prob_times_values (32x2048x2048x2206): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2206): 86.067

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1608.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2207x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2207x2048): 89.713
Elapsed time for attention_prob_times_values (32x2048x2048x2207): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2207): 80.214

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1545.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2208x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2208x2048): 96.776
Elapsed time for attention_prob_times_values (32x2048x2048x2208): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2208): 92.771

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1728.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2209x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2209x2048): 90.388
Elapsed time for attention_prob_times_values (32x2048x2048x2209): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2209): 84.573

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1595.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2210x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2210x2048): 91.111
Elapsed time for attention_prob_times_values (32x2048x2048x2210): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2210): 86.695

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1622.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2211x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2211x2048): 90.625
Elapsed time for attention_prob_times_values (32x2048x2048x2211): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2211): 84.342

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1596.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2212x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2212x2048): 92.340
Elapsed time for attention_prob_times_values (32x2048x2048x2212): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2212): 86.188

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1629.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2213x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2213x2048): 90.902
Elapsed time for attention_prob_times_values (32x2048x2048x2213): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2213): 83.418

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1591.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2214x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2214x2048): 91.346
Elapsed time for attention_prob_times_values (32x2048x2048x2214): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2214): 86.949

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1630.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2215x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2215x2048): 90.453
Elapsed time for attention_prob_times_values (32x2048x2048x2215): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2215): 84.595

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1600.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2216x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2216x2048): 91.931
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x799x2048): 75.613
Elapsed time for attention_prob_times_values (160x2048x2048x799): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x799): 63.114

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2216.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x800x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x800x2048): 87.853
Elapsed time for attention_prob_times_values (160x2048x2048x800): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x800): 85.966

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2802.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x801x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x801x2048): 75.918
Elapsed time for attention_prob_times_values (160x2048x2048x801): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x801): 65.198

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2265.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x802x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x802x2048): 75.968
Elapsed time for attention_prob_times_values (160x2048x2048x802): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x802): 67.038

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 2302.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x803x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x803x2048): 74.896
Elapsed time for attention_prob_times_values (160x2048x2048x803): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x803): 64.303

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2239.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x804x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x804x2048): 80.376
Elapsed time for attention_prob_times_values (160x2048x2048x804): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x804): 67.397

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2375.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x805x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x805x2048): 77.932
Elapsed time for attention_prob_times_values (160x2048x2048x805): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x805): 64.435

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2288.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x806x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x806x2048): 79.409
Elapsed time for attention_prob_times_values (160x2048x2048x806): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x806): 67.309

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 2366.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x807x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x807x2048): 78.363
Elapsed time for attention_prob_times_values (160x2048x2048x807): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x807): 64.423

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2299.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x808x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x808x2048): 80.321
Elapsed time for attention_prob_times_values (160x2048x2048x808): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x808): 86.206

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2707.868
MLP duration (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (80x2048x1272x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1272x2048): 80.958
Elapsed time for attention_prob_times_values (80x2048x2048x1272): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1272): 93.712

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2245.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1273x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1273x2048): 80.757
Elapsed time for attention_prob_times_values (80x2048x2048x1273): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1273): 73.250

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1986.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1274x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1274x2048): 83.335
Elapsed time for attention_prob_times_values (80x2048x2048x1274): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1274): 79.857

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2110.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1275x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1275x2048): 81.325
Elapsed time for attention_prob_times_values (80x2048x2048x1275): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1275): 74.287

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2011.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1276x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1276x2048): 83.501
Elapsed time for attention_prob_times_values (80x2048x2048x1276): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1276): 80.769

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2128.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1277x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1277x2048): 81.923
Elapsed time for attention_prob_times_values (80x2048x2048x1277): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1277): 73.065

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2003.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1278x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1278x2048): 82.995
Elapsed time for attention_prob_times_values (80x2048x2048x1278): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1278): 79.350

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2106.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1279x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1279x2048): 79.973
Elapsed time for attention_prob_times_values (80x2048x2048x1279): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1279): 74.517

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2004.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1280x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1280x2048): 91.472
Elapsed time for attention_prob_times_values (80x2048x2048x1280): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1280): 96.300

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2439.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1281x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1281x2048): 82.730
Elapsed time for attention_prob_times_values (80x2048x2048x1281): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1281): 65.481

Attention duration (in seconds): 0.0235
Elapsed time for attention_prob_times_values (32x2048x2048x2216): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2216): 92.032

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1684.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2217x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2217x2048): 90.070
Elapsed time for attention_prob_times_values (32x2048x2048x2217): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2217): 84.752

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1599.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2218x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2218x2048): 91.074
Elapsed time for attention_prob_times_values (32x2048x2048x2218): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2218): 87.167

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1632.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2219x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2219x2048): 87.641
Elapsed time for attention_prob_times_values (32x2048x2048x2219): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2219): 82.438

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1557.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2220x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2220x2048): 91.789
Elapsed time for attention_prob_times_values (32x2048x2048x2220): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2220): 83.300

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1602.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2221x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2221x2048): 87.224
Elapsed time for attention_prob_times_values (32x2048x2048x2221): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2221): 84.664

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1576.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2222x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2222x2048): 87.324
Elapsed time for attention_prob_times_values (32x2048x2048x2222): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2222): 87.275

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1602.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2223x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2223x2048): 90.295
Elapsed time for attention_prob_times_values (32x2048x2048x2223): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2223): 81.989

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1578.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2224x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2224x2048): 92.720
Elapsed time for attention_prob_times_values (32x2048x2048x2224): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2224): 89.336

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1672.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2225x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2225x2048): 90.171
Elapsed time for attention_prob_times_values (32x2048x2048x2225): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2225): 80.670

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1565.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2226x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2226x2048): 88.062
Elapsed time for attention_prob_times_values (32x2048x2048x2226): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2226): 87.384

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1613.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2227x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2227x2048): 90.019
Elapsed time for attention_prob_times_values (32x2048x2048x2227): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2227): 84.958

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1608.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2228x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2228x2048): 88.640
Elapsed time for attention_prob_times_values (32x2048x2048x2228): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2228): 87.219

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1618.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2229x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2229x2048): 87.258
Elapsed time for attention_prob_times_values (32x2048x2048x2229): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2229): 84.905

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1584.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2230x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2230x2048): 91.290
Elapsed time for attention_prob_times_values (32x2048x2048x2230): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2230): 87.577

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1646.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2231x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2231x2048): 87.464
Elapsed time for attention_prob_times_values (32x2048x2048x2231): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2231): 85.053

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1589.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2232x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2232x2048): 92.197
Elapsed time for attention_prob_times_values (32x2048x2048x2232): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2232): 92.622

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1703.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2233x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2233x2048): 89.746
Elapsed time for attention_prob_times_values (32x2048x2048x2233): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2233): 81.148

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1572.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2234x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2234x2048): 90.643
Elapsed time for attention_prob_times_values (32x2048x2048x2234): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2234): 83.083

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1599.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Attention throughput (in TFLOP/s): 1902.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1282x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1282x2048): 83.950
Elapsed time for attention_prob_times_values (80x2048x2048x1282): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1282): 75.426

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2069.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1283x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1283x2048): 77.033
Elapsed time for attention_prob_times_values (80x2048x2048x1283): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1283): 69.950

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1910.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1284x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1284x2048): 84.409
Elapsed time for attention_prob_times_values (80x2048x2048x1284): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1284): 75.657

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2080.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1285x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1285x2048): 83.093
Elapsed time for attention_prob_times_values (80x2048x2048x1285): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1285): 70.054

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1983.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1286x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1286x2048): 84.232
Elapsed time for attention_prob_times_values (80x2048x2048x1286): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1286): 74.898

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2070.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1287x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1287x2048): 83.354
Elapsed time for attention_prob_times_values (80x2048x2048x1287): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1287): 68.598

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1967.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1288x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1288x2048): 82.624
Elapsed time for attention_prob_times_values (80x2048x2048x1288): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1288): 83.722

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2175.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1289x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1289x2048): 82.306
Elapsed time for attention_prob_times_values (80x2048x2048x1289): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1289): 66.830

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1930.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1290x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1290x2048): 82.768
Elapsed time for attention_prob_times_values (80x2048x2048x1290): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1290): 75.691

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2071.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x809x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x809x2048): 77.713
Elapsed time for attention_prob_times_values (160x2048x2048x809): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x809): 63.883

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 2286.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x810x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x810x2048): 77.049
Elapsed time for attention_prob_times_values (160x2048x2048x810): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x810): 66.322

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2326.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x811x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x811x2048): 76.200
Elapsed time for attention_prob_times_values (160x2048x2048x811): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x811): 63.369

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 2261.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x812x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x812x2048): 78.025
Elapsed time for attention_prob_times_values (160x2048x2048x812): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x812): 67.915

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2376.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x813x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x813x2048): 78.082
Elapsed time for attention_prob_times_values (160x2048x2048x813): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x813): 64.863

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 2321.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x814x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x814x2048): 79.063
Elapsed time for attention_prob_times_values (160x2048x2048x814): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x814): 80.733

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2620.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x815x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x815x2048): 78.447
Elapsed time for attention_prob_times_values (160x2048x2048x815): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x815): 78.455

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2576.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x816x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x816x2048): 80.085
Elapsed time for attention_prob_times_values (160x2048x2048x816): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x816): 87.277

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2745.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x817x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x817x2048): 78.067
Elapsed time for attention_prob_times_values (160x2048x2048x817): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x817): 77.837

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2565.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x818x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x818x2048): 76.602
Elapsed time for attention_prob_times_values (160x2048x2048x818): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x818): 79.775

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2575.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x819x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x819x2048): 76.956
Elapsed time for attention_prob_times_values (160x2048x2048x819): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x819): 77.079

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2540.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (32x2048x2235x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2235x2048): 89.906
Elapsed time for attention_prob_times_values (32x2048x2048x2235): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2235): 85.255

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1615.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2236x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2236x2048): 91.629
Elapsed time for attention_prob_times_values (32x2048x2048x2236): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2236): 87.899

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1657.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2237x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2237x2048): 90.029
Elapsed time for attention_prob_times_values (32x2048x2048x2237): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2237): 85.366

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1619.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2238x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2238x2048): 91.180
Elapsed time for attention_prob_times_values (32x2048x2048x2238): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2238): 87.830

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1653.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2239x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2239x2048): 90.183
Elapsed time for attention_prob_times_values (32x2048x2048x2239): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2239): 85.335

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1621.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2240x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2240x2048): 99.680
Elapsed time for attention_prob_times_values (32x2048x2048x2240): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2240): 94.638

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1796.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2241x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2241x2048): 91.055
Elapsed time for attention_prob_times_values (32x2048x2048x2241): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2241): 85.322

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1630.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2242x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2242x2048): 91.925
Elapsed time for attention_prob_times_values (32x2048x2048x2242): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2242): 88.036

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1665.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2243x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2243x2048): 91.094
Elapsed time for attention_prob_times_values (32x2048x2048x2243): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2243): 85.525

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1634.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2244x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2244x2048): 92.764
Elapsed time for attention_prob_times_values (32x2048x2048x2244): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2244): 88.176

Attention duration (in seconds): 0.0133
========================================================================================================================
num_attention_heads: 20, hidden_size: 25820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1291x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1291x2048): 83.090
Elapsed time for attention_prob_times_values (80x2048x2048x1291): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1291): 69.630

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1986.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1292x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1292x2048): 84.436
Elapsed time for attention_prob_times_values (80x2048x2048x1292): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1292): 76.105

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2100.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1293x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1293x2048): 82.429
Elapsed time for attention_prob_times_values (80x2048x2048x1293): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1293): 70.686

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1998.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1294x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1294x2048): 84.363
Elapsed time for attention_prob_times_values (80x2048x2048x1294): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1294): 75.997

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2100.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1295x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1295x2048): 83.233
Elapsed time for attention_prob_times_values (80x2048x2048x1295): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1295): 71.139

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2016.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1296x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1296x2048): 85.417
Elapsed time for attention_prob_times_values (80x2048x2048x1296): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1296): 87.713

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2277.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1297x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1297x2048): 82.517
Elapsed time for attention_prob_times_values (80x2048x2048x1297): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1297): 71.155

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2012.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1298x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1298x2048): 83.999
Elapsed time for attention_prob_times_values (80x2048x2048x1298): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1298): 75.877

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2101.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1299x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1299x2048): 82.843
Elapsed time for attention_prob_times_values (80x2048x2048x1299): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1299): 71.307

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2021.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1300x2048): 0.0103
Attention throughput (in TFLOP/s): 1675.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2245x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2245x2048): 90.873
Elapsed time for attention_prob_times_values (32x2048x2048x2245): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2245): 85.596

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1634.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2246x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2246x2048): 91.582
Elapsed time for attention_prob_times_values (32x2048x2048x2246): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2246): 88.120

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1665.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2247x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2247x2048): 90.573
Elapsed time for attention_prob_times_values (32x2048x2048x2247): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2247): 83.785

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1615.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2248x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2248x2048): 92.711
Elapsed time for attention_prob_times_values (32x2048x2048x2248): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2248): 93.558

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1728.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 17992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2249x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2249x2048): 90.421
Elapsed time for attention_prob_times_values (32x2048x2048x2249): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2249): 85.885

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1635.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2250x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2250x2048): 88.086
Elapsed time for attention_prob_times_values (32x2048x2048x2250): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2250): 88.203

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1637.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2251x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2251x2048): 88.144
Elapsed time for attention_prob_times_values (32x2048x2048x2251): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2251): 85.822

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1616.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2252x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2252x2048): 92.000
Elapsed time for attention_prob_times_values (32x2048x2048x2252): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2252): 86.386

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1656.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2253x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2253x2048): 90.417
Elapsed time for attention_prob_times_values (32x2048x2048x2253): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2253): 83.815

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1618.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2254x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2254x2048): 91.349
Elapsed time for attention_prob_times_values (32x2048x2048x2254): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2254): 88.295

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1671.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2255x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2255x2048): 88.153
Elapsed time for attention_prob_times_values (32x2048x2048x2255): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2255): 86.164

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1622.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2256x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2256x2048): 90.287
Elapsed time for attention_prob_times_values (32x2048x2048x2256): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2256): 94.176

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1717.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2257x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2257x2048): 90.134
Elapsed time for attention_prob_times_values (32x2048x2048x2257): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2257): 83.759

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1617.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2258x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2258x2048): 91.069
Elapsed time for attention_prob_times_values (32x2048x2048x2258): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2258): 86.856

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1657.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2259x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2259x2048): 88.546
Elapsed time for attention_prob_times_values (32x2048x2048x2259): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2259): 86.261

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1629.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2260x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2260x2048): 91.655
Elapsed time for attention_prob_times_values (32x2048x2048x2260): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2260): 88.684

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1681.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2261x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2261x2048): 90.300
Elapsed time for attention_prob_times_values (32x2048x2048x2261): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2261): 86.405

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1648.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2262x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2262x2048): 91.044
Elapsed time for attention_prob_times_values (32x2048x2048x2262): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2262): 88.679

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1677.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2263x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2263x2048): 90.351
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1300x2048): 84.529
Elapsed time for attention_prob_times_values (80x2048x2048x1300): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1300): 75.995

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2112.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1301x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1301x2048): 82.772
Elapsed time for attention_prob_times_values (80x2048x2048x1301): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1301): 71.118

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2020.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1302x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1302x2048): 84.185
Elapsed time for attention_prob_times_values (80x2048x2048x1302): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1302): 76.318

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2115.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1303x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1303x2048): 82.965
Elapsed time for attention_prob_times_values (80x2048x2048x1303): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1303): 71.140

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2025.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1304x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1304x2048): 85.208
Elapsed time for attention_prob_times_values (80x2048x2048x1304): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1304): 87.591

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2286.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1305x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1305x2048): 82.445
Elapsed time for attention_prob_times_values (80x2048x2048x1305): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1305): 71.361

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2026.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1306x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1306x2048): 83.879
Elapsed time for attention_prob_times_values (80x2048x2048x1306): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1306): 76.540

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2121.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1307x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1307x2048): 82.766
Elapsed time for attention_prob_times_values (80x2048x2048x1307): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1307): 71.949

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2042.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1308x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1308x2048): 84.580
Elapsed time for attention_prob_times_values (80x2048x2048x1308): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1308): 76.836

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2137.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1309x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1309x2048): 82.869
Elapsed time for attention_prob_times_values (80x2048x2048x1309): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1309): 72.026

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2047.421
MLP duration (in seconds): 0.0000
Elapsed time for attention_prob_times_values (32x2048x2048x2263): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2263): 86.320

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1649.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2264x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2264x2048): 92.086
Elapsed time for attention_prob_times_values (32x2048x2048x2264): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2264): 94.196

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1740.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2265x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2265x2048): 90.415
Elapsed time for attention_prob_times_values (32x2048x2048x2265): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2265): 86.446

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1652.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2266x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2266x2048): 91.173
Elapsed time for attention_prob_times_values (32x2048x2048x2266): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2266): 88.842

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1683.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2267x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2267x2048): 91.200
Elapsed time for attention_prob_times_values (32x2048x2048x2267): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2267): 86.455

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1660.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2268x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2268x2048): 91.768
Elapsed time for attention_prob_times_values (32x2048x2048x2268): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2268): 89.008

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1691.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2269x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2269x2048): 90.981
Elapsed time for attention_prob_times_values (32x2048x2048x2269): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2269): 86.502

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1660.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2270x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2270x2048): 91.158
Elapsed time for attention_prob_times_values (32x2048x2048x2270): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2270): 88.882

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1686.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2271x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2271x2048): 90.354
Elapsed time for attention_prob_times_values (32x2048x2048x2271): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2271): 86.625

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1657.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2272x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2272x2048): 99.756
Elapsed time for attention_prob_times_values (32x2048x2048x2272): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2272): 95.407

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1828.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1310x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1310x2048): 84.185
Elapsed time for attention_prob_times_values (80x2048x2048x1310): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1310): 76.781

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2135.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1311x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1311x2048): 83.020
Elapsed time for attention_prob_times_values (80x2048x2048x1311): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1311): 72.350

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2057.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1312x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1312x2048): 96.123
Elapsed time for attention_prob_times_values (80x2048x2048x1312): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1312): 89.243

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2464.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1313x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1313x2048): 84.403
Elapsed time for attention_prob_times_values (80x2048x2048x1313): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1313): 72.362

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2076.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1314x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1314x2048): 85.449
Elapsed time for attention_prob_times_values (80x2048x2048x1314): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1314): 76.908

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2158.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1315x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1315x2048): 84.216
Elapsed time for attention_prob_times_values (80x2048x2048x1315): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1315): 72.307

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2076.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1316x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1316x2048): 85.833
Elapsed time for attention_prob_times_values (80x2048x2048x1316): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1316): 77.168

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2170.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1317x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1317x2048): 83.272
Elapsed time for attention_prob_times_values (80x2048x2048x1317): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1317): 70.734

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2044.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1318x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1318x2048): 85.047
Elapsed time for attention_prob_times_values (80x2048x2048x1318): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1318): 75.411

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2137.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2273x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2273x2048): 91.174
Elapsed time for attention_prob_times_values (32x2048x2048x2273): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2273): 86.725

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1667.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2274x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2274x2048): 91.991
Elapsed time for attention_prob_times_values (32x2048x2048x2274): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2274): 89.165

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1699.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2275x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2275x2048): 90.969
Elapsed time for attention_prob_times_values (32x2048x2048x2275): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2275): 86.660

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1666.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2276x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2276x2048): 92.537
Elapsed time for attention_prob_times_values (32x2048x2048x2276): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2276): 89.141

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1705.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2277x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2277x2048): 90.840
Elapsed time for attention_prob_times_values (32x2048x2048x2277): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2277): 86.713

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1667.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2278x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2278x2048): 91.724
Elapsed time for attention_prob_times_values (32x2048x2048x2278): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2278): 89.148

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1699.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2279x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2279x2048): 90.694
Elapsed time for attention_prob_times_values (32x2048x2048x2279): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2279): 86.363

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1663.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2280x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2280x2048): 92.543
Elapsed time for attention_prob_times_values (32x2048x2048x2280): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2280): 94.806

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1761.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2281x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2281x2048): 90.451
Elapsed time for attention_prob_times_values (32x2048x2048x2281): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2281): 86.745

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1666.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2282x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2282x2048): 91.574
Elapsed time for attention_prob_times_values (32x2048x2048x2282): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2282): 89.156

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1701.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2283x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2283x2048): 90.908
Elapsed time for attention_prob_times_values (32x2048x2048x2283): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2283): 86.722

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1671.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2284x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2284x2048): 92.200
Elapsed time for attention_prob_times_values (32x2048x2048x2284): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2284): 89.467

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1711.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2285x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2285x2048): 90.702
Elapsed time for attention_prob_times_values (32x2048x2048x2285): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2285): 86.689

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1671.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2286x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2286x2048): 91.604
Elapsed time for attention_prob_times_values (32x2048x2048x2286): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2286): 89.427

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1706.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2287x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2287x2048): 90.600
Elapsed time for attention_prob_times_values (32x2048x2048x2287): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2287): 86.859

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1673.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2288x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2288x2048): 93.002
Elapsed time for attention_prob_times_values (32x2048x2048x2288): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2288): 95.509

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1778.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2289x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2289x2048): 90.320
Elapsed time for attention_prob_times_values (32x2048x2048x2289): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2289): 86.634

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1669.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2290x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2290x2048): 91.274
Elapsed time for attention_prob_times_values (32x2048x2048x2290): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2290): 88.987

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1702.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2291x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2291x2048): 90.510
Elapsed time for attention_prob_times_values (32x2048x2048x2291): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2291): 86.955

Attention duration (in seconds): 0.0139
num_attention_heads: 20, hidden_size: 26380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1319x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1319x2048): 81.703
Elapsed time for attention_prob_times_values (80x2048x2048x1319): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1319): 69.850

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2015.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1320x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1320x2048): 85.307
Elapsed time for attention_prob_times_values (80x2048x2048x1320): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1320): 86.446

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2299.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1321x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1321x2048): 83.443
Elapsed time for attention_prob_times_values (80x2048x2048x1321): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1321): 70.725

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2051.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1322x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1322x2048): 83.440
Elapsed time for attention_prob_times_values (80x2048x2048x1322): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1322): 75.450

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2125.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1323x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1323x2048): 82.079
Elapsed time for attention_prob_times_values (80x2048x2048x1323): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1323): 70.732

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2039.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1324x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1324x2048): 85.231
Elapsed time for attention_prob_times_values (80x2048x2048x1324): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1324): 75.702

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2153.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1325x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1325x2048): 81.245
Elapsed time for attention_prob_times_values (80x2048x2048x1325): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1325): 70.413

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2027.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1326x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1326x2048): 82.765
Elapsed time for attention_prob_times_values (80x2048x2048x1326): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1326): 75.374

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2122.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1327x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1327x2048): 83.662
Elapsed time for attention_prob_times_values (80x2048x2048x1327): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1327): 72.365

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2088.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1328x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1328x2048): 83.018
Attention throughput (in TFLOP/s): 1676.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2292x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2292x2048): 91.766
Elapsed time for attention_prob_times_values (32x2048x2048x2292): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2292): 89.324

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1711.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2293x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2293x2048): 90.392
Elapsed time for attention_prob_times_values (32x2048x2048x2293): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2293): 87.075

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1677.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2294x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2294x2048): 91.105
Elapsed time for attention_prob_times_values (32x2048x2048x2294): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2294): 89.312

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1706.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2295x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2295x2048): 90.266
Elapsed time for attention_prob_times_values (32x2048x2048x2295): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2295): 86.846

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1675.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2296x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2296x2048): 92.126
Elapsed time for attention_prob_times_values (32x2048x2048x2296): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2296): 95.305

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1774.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2297x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2297x2048): 89.955
Elapsed time for attention_prob_times_values (32x2048x2048x2297): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2297): 87.007

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1675.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2298x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2298x2048): 90.815
Elapsed time for attention_prob_times_values (32x2048x2048x2298): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2298): 89.576

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1709.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2299x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2299x2048): 89.986
Elapsed time for attention_prob_times_values (32x2048x2048x2299): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2299): 87.056

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1677.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2300x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2300x2048): 91.533
Elapsed time for attention_prob_times_values (32x2048x2048x2300): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2300): 89.766

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1719.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2301x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2301x2048): 89.740
Elapsed time for attention_prob_times_values (32x2048x2048x2301): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2301): 87.076

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1677.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2302x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2302x2048): 90.944
Elapsed time for attention_prob_times_values (32x2048x2048x2302): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2302): 89.673

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1714.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2303x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2303x2048): 89.914
Elapsed time for attention_prob_times_values (32x2048x2048x2303): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2303): 87.205

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1681.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2304x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2304x2048): 98.023
Elapsed time for attention_prob_times_values (32x2048x2048x2304): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2304): 97.610

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1858.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2305x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2305x2048): 90.926
Elapsed time for attention_prob_times_values (32x2048x2048x2305): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2305): 83.536

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1655.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2306x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2306x2048): 91.873
Elapsed time for attention_prob_times_values (32x2048x2048x2306): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2306): 86.011

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1689.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2307x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2307x2048): 90.500
Elapsed time for attention_prob_times_values (32x2048x2048x2307): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2307): 83.962

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1657.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2308x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2308x2048): 92.575
Elapsed time for attention_prob_times_values (32x2048x2048x2308): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2308): 86.360

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1700.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2309x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2309x2048): 90.908
Elapsed time for attention_prob_times_values (32x2048x2048x2309): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2309): 83.956

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1661.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2310x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2310x2048): 91.834
Elapsed time for attention_prob_times_values (80x2048x2048x1328): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1328): 87.459

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2294.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1329x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1329x2048): 81.398
Elapsed time for attention_prob_times_values (80x2048x2048x1329): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1329): 69.772

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2025.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1330x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1330x2048): 84.230
Elapsed time for attention_prob_times_values (80x2048x2048x1330): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1330): 77.572

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2178.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1331x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1331x2048): 80.962
Elapsed time for attention_prob_times_values (80x2048x2048x1331): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1331): 70.210

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2030.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1332x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1332x2048): 84.973
Elapsed time for attention_prob_times_values (80x2048x2048x1332): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1332): 75.730

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2163.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1333x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1333x2048): 82.584
Elapsed time for attention_prob_times_values (80x2048x2048x1333): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1333): 72.165

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2082.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1334x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1334x2048): 81.598
Elapsed time for attention_prob_times_values (80x2048x2048x1334): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1334): 75.901

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2127.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1335x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1335x2048): 83.378
Elapsed time for attention_prob_times_values (80x2048x2048x1335): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1335): 69.644

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2054.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1336x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1336x2048): 83.872
Elapsed time for attention_prob_times_values (80x2048x2048x1336): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1336): 89.745

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2349.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1337x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1337x2048): 80.692
Elapsed time for attention_prob_times_values (80x2048x2048x1337): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1337): 70.624

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2042.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Elapsed time for attention_prob_times_values (32x2048x2048x2310): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2310): 86.398

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1695.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2311x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2311x2048): 90.803
Elapsed time for attention_prob_times_values (32x2048x2048x2311): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2311): 84.013

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1663.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2312x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2312x2048): 92.597
Elapsed time for attention_prob_times_values (32x2048x2048x2312): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2312): 91.742

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1756.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2313x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2313x2048): 90.446
Elapsed time for attention_prob_times_values (32x2048x2048x2313): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2313): 84.202

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1663.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2314x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2314x2048): 91.268
Elapsed time for attention_prob_times_values (32x2048x2048x2314): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2314): 86.579

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1695.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2315x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2315x2048): 90.508
Elapsed time for attention_prob_times_values (32x2048x2048x2315): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2315): 84.270

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1665.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2316x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2316x2048): 91.868
Elapsed time for attention_prob_times_values (32x2048x2048x2316): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2316): 86.639

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1702.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2317x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2317x2048): 90.495
Elapsed time for attention_prob_times_values (32x2048x2048x2317): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2317): 84.304

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1667.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2318x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2318x2048): 91.419
Elapsed time for attention_prob_times_values (32x2048x2048x2318): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2318): 86.703

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1700.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2319x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2319x2048): 90.470
Elapsed time for attention_prob_times_values (32x2048x2048x2319): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2319): 84.410

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1669.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1338x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1338x2048): 83.884
Elapsed time for attention_prob_times_values (80x2048x2048x1338): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1338): 76.296

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2168.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1339x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1339x2048): 81.524
Elapsed time for attention_prob_times_values (80x2048x2048x1339): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1339): 72.220

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2079.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1340x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1340x2048): 82.940
Elapsed time for attention_prob_times_values (80x2048x2048x1340): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1340): 76.494

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2162.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1341x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1341x2048): 81.251
Elapsed time for attention_prob_times_values (80x2048x2048x1341): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1341): 70.400

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2051.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1342x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1342x2048): 82.541
Elapsed time for attention_prob_times_values (80x2048x2048x1342): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1342): 78.037

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2183.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1343x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1343x2048): 80.857
Elapsed time for attention_prob_times_values (80x2048x2048x1343): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1343): 70.828

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2056.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1344x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1344x2048): 93.385
Elapsed time for attention_prob_times_values (80x2048x2048x1344): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1344): 89.788

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2494.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1345x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1345x2048): 84.390
Elapsed time for attention_prob_times_values (80x2048x2048x1345): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1345): 70.720

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2098.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1346x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1346x2048): 87.220
Elapsed time for attention_prob_times_values (80x2048x2048x1346): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1346): 78.268

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2251.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2320x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2320x2048): 92.954
Elapsed time for attention_prob_times_values (32x2048x2048x2320): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2320): 82.992

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1677.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2321x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2321x2048): 90.201
Elapsed time for attention_prob_times_values (32x2048x2048x2321): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2321): 84.521

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1669.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2322x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2322x2048): 91.082
Elapsed time for attention_prob_times_values (32x2048x2048x2322): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2322): 86.668

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1700.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2323x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2323x2048): 90.226
Elapsed time for attention_prob_times_values (32x2048x2048x2323): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2323): 84.569

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1671.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2324x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2324x2048): 91.846
Elapsed time for attention_prob_times_values (32x2048x2048x2324): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2324): 86.743

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1709.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2325x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2325x2048): 90.356
Elapsed time for attention_prob_times_values (32x2048x2048x2325): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2325): 84.563

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1674.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2326x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2326x2048): 91.475
Elapsed time for attention_prob_times_values (32x2048x2048x2326): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2326): 86.892

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1708.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2327x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2327x2048): 90.475
Elapsed time for attention_prob_times_values (32x2048x2048x2327): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2327): 84.652

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1677.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2328x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2328x2048): 92.133
Elapsed time for attention_prob_times_values (32x2048x2048x2328): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2328): 68.844

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1512.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2329x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2329x2048): 90.851
Elapsed time for attention_prob_times_values (32x2048x2048x2329): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2329): 84.631

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1682.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2330x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2330x2048): 91.225
Elapsed time for attention_prob_times_values (32x2048x2048x2330): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2330): 86.602

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1706.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2331x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2331x2048): 91.292
Elapsed time for attention_prob_times_values (32x2048x2048x2331): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2331): 84.391

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1684.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2332x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2332x2048): 92.142
Elapsed time for attention_prob_times_values (32x2048x2048x2332): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2332): 86.802

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1718.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2333x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2333x2048): 90.427
Elapsed time for attention_prob_times_values (32x2048x2048x2333): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2333): 84.730

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1682.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2334x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2334x2048): 91.292
Elapsed time for attention_prob_times_values (32x2048x2048x2334): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2334): 86.891

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1712.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2335x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2335x2048): 90.231
Elapsed time for attention_prob_times_values (32x2048x2048x2335): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2335): 84.740

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1681.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2336x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2336x2048): 99.754
Elapsed time for attention_prob_times_values (32x2048x2048x2336): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2336): 84.913

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1765.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2337x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2337x2048): 89.049
Elapsed time for attention_prob_times_values (32x2048x2048x2337): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2337): 85.010

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1675.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2338x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2338x2048): 92.073
Elapsed time for attention_prob_times_values (32x2048x2048x2338): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2338): 87.080

Attention duration (in seconds): 0.0140
--------
Elapsed time for attention_key_query_prob (80x2048x1347x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1347x2048): 82.669
Elapsed time for attention_prob_times_values (80x2048x2048x1347): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1347): 68.575

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2047.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1348x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1348x2048): 82.113
Elapsed time for attention_prob_times_values (80x2048x2048x1348): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1348): 77.360

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2177.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1349x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1349x2048): 84.109
Elapsed time for attention_prob_times_values (80x2048x2048x1349): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1349): 70.518

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2098.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1350x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1350x2048): 84.962
Elapsed time for attention_prob_times_values (80x2048x2048x1350): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1350): 78.228

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2229.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1351x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1351x2048): 83.762
Elapsed time for attention_prob_times_values (80x2048x2048x1351): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1351): 71.431

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2111.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1352x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1352x2048): 86.250
Elapsed time for attention_prob_times_values (80x2048x2048x1352): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1352): 89.315

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2405.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1353x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1353x2048): 83.317
Elapsed time for attention_prob_times_values (80x2048x2048x1353): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1353): 72.268

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2122.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1354x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1354x2048): 83.430
Elapsed time for attention_prob_times_values (80x2048x2048x1354): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1354): 78.430

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2219.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1355x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1355x2048): 81.963
Elapsed time for attention_prob_times_values (80x2048x2048x1355): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1355): 71.118

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2091.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1356x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1356x2048): 87.363
Elapsed time for attention_prob_times_values (80x2048x2048x1356): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1356): 78.537

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1724.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2339x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2339x2048): 91.342
Elapsed time for attention_prob_times_values (32x2048x2048x2339): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2339): 84.926

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1696.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2340x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2340x2048): 92.657
Elapsed time for attention_prob_times_values (32x2048x2048x2340): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2340): 83.875

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1697.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2341x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2341x2048): 90.887
Elapsed time for attention_prob_times_values (32x2048x2048x2341): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2341): 84.952

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1693.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2342x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2342x2048): 88.903
Elapsed time for attention_prob_times_values (32x2048x2048x2342): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2342): 87.252

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1699.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2343x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2343x2048): 90.849
Elapsed time for attention_prob_times_values (32x2048x2048x2343): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2343): 82.574

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1670.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2344x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2344x2048): 92.825
Elapsed time for attention_prob_times_values (32x2048x2048x2344): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2344): 67.141

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1504.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2345x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2345x2048): 90.621
Elapsed time for attention_prob_times_values (32x2048x2048x2345): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2345): 85.045

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1695.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2346x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2346x2048): 91.371
Elapsed time for attention_prob_times_values (32x2048x2048x2346): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2346): 87.122

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1723.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2347x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2347x2048): 91.064
Elapsed time for attention_prob_times_values (32x2048x2048x2347): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2347): 85.033

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1700.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2348x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2348x2048): 92.470
Elapsed time for attention_prob_times_values (32x2048x2048x2348): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2348): 87.647

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1740.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2349x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2349x2048): 90.841
Elapsed time for attention_prob_times_values (32x2048x2048x2349): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2349): 85.010

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1699.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2350x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2350x2048): 93.141
Elapsed time for attention_prob_times_values (32x2048x2048x2350): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2350): 87.256

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1744.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2351x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2351x2048): 90.862
Elapsed time for attention_prob_times_values (32x2048x2048x2351): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2351): 84.614

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1697.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2352x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2352x2048): 93.981
Elapsed time for attention_prob_times_values (32x2048x2048x2352): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2352): 71.936

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1578.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2353x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2353x2048): 90.759
Elapsed time for attention_prob_times_values (32x2048x2048x2353): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2353): 85.239

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1703.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2354x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2354x2048): 91.436
Elapsed time for attention_prob_times_values (32x2048x2048x2354): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2354): 87.610

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1735.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2355x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2355x2048): 90.575
Elapsed time for attention_prob_times_values (32x2048x2048x2355): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2355): 85.232

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1703.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2356x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2356x2048): 92.124
Elapsed time for attention_prob_times_values (32x2048x2048x2356): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2356): 87.476

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1741.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2357x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2357x2048): 90.627
Attention throughput (in TFLOP/s): 2273.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1357x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1357x2048): 87.345
Elapsed time for attention_prob_times_values (80x2048x2048x1357): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1357): 72.202

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2174.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1358x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1358x2048): 85.480
Elapsed time for attention_prob_times_values (80x2048x2048x1358): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1358): 78.488

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2252.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1359x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1359x2048): 86.115
Elapsed time for attention_prob_times_values (80x2048x2048x1359): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1359): 72.470

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2167.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1360x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1360x2048): 88.705
Elapsed time for attention_prob_times_values (80x2048x2048x1360): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1360): 91.712

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2485.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1361x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1361x2048): 88.542
Elapsed time for attention_prob_times_values (80x2048x2048x1361): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1361): 73.068

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2208.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1362x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1362x2048): 85.650
Elapsed time for attention_prob_times_values (80x2048x2048x1362): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1362): 78.675

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2263.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1363x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1363x2048): 84.306
Elapsed time for attention_prob_times_values (80x2048x2048x1363): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1363): 72.546

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2154.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1364x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1364x2048): 86.707
Elapsed time for attention_prob_times_values (80x2048x2048x1364): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1364): 78.950

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2284.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1365x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1365x2048): 83.284
Elapsed time for attention_prob_times_values (80x2048x2048x1365): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1365): 73.114

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2153.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (32x2048x2048x2357): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2357): 85.244

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1705.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2358x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2358x2048): 91.428
Elapsed time for attention_prob_times_values (32x2048x2048x2358): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2358): 87.469

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1736.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2359x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2359x2048): 90.475
Elapsed time for attention_prob_times_values (32x2048x2048x2359): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2359): 84.908

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1702.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2360x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2360x2048): 92.515
Elapsed time for attention_prob_times_values (32x2048x2048x2360): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2360): 67.981

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1523.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2361x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2361x2048): 90.266
Elapsed time for attention_prob_times_values (32x2048x2048x2361): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2361): 85.408

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1706.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2362x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2362x2048): 91.078
Elapsed time for attention_prob_times_values (32x2048x2048x2362): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2362): 87.543

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1736.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2363x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2363x2048): 90.318
Elapsed time for attention_prob_times_values (32x2048x2048x2363): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2363): 85.313

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1707.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2364x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2364x2048): 91.856
Elapsed time for attention_prob_times_values (32x2048x2048x2364): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2364): 87.877

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1748.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2365x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2365x2048): 90.339
Elapsed time for attention_prob_times_values (32x2048x2048x2365): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2365): 85.126

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1707.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2366x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2366x2048): 91.382
Elapsed time for attention_prob_times_values (32x2048x2048x2366): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2366): 87.885

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1745.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
========================================================================================================================
num_attention_heads: 20, hidden_size: 27320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1366x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1366x2048): 83.735
Elapsed time for attention_prob_times_values (80x2048x2048x1366): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1366): 78.872

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2248.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1367x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1367x2048): 84.930
Elapsed time for attention_prob_times_values (80x2048x2048x1367): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1367): 73.025

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2175.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1368x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1368x2048): 85.782
Elapsed time for attention_prob_times_values (80x2048x2048x1368): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1368): 91.987

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2460.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1369x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1369x2048): 84.964
Elapsed time for attention_prob_times_values (80x2048x2048x1369): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1369): 73.531

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2186.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1370x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1370x2048): 88.791
Elapsed time for attention_prob_times_values (80x2048x2048x1370): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1370): 79.118

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2322.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1371x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1371x2048): 86.921
Elapsed time for attention_prob_times_values (80x2048x2048x1371): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1371): 73.529

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2212.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1372x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1372x2048): 85.931
Elapsed time for attention_prob_times_values (80x2048x2048x1372): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1372): 75.005

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2226.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1373x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1373x2048): 83.896
Elapsed time for attention_prob_times_values (80x2048x2048x1373): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1373): 69.975

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2122.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1374x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1374x2048): 82.044
Elapsed time for attention_prob_times_values (80x2048x2048x1374): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1374): 75.432

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2187.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1375x2048): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2367x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2367x2048): 90.502
Elapsed time for attention_prob_times_values (32x2048x2048x2367): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2367): 85.645

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1715.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2368x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2368x2048): 99.870
Elapsed time for attention_prob_times_values (32x2048x2048x2368): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2368): 89.690

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1842.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2369x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2369x2048): 91.438
Elapsed time for attention_prob_times_values (32x2048x2048x2369): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2369): 85.268

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1721.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2370x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2370x2048): 92.373
Elapsed time for attention_prob_times_values (32x2048x2048x2370): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2370): 88.063

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1759.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2371x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2371x2048): 91.390
Elapsed time for attention_prob_times_values (32x2048x2048x2371): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2371): 85.795

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1727.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2372x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2372x2048): 92.901
Elapsed time for attention_prob_times_values (32x2048x2048x2372): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2372): 88.067

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1766.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2373x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2373x2048): 91.183
Elapsed time for attention_prob_times_values (32x2048x2048x2373): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2373): 85.455

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1723.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 18992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2374x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2374x2048): 92.293
Elapsed time for attention_prob_times_values (32x2048x2048x2374): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2374): 87.883

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1759.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2375x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2375x2048): 91.049
Elapsed time for attention_prob_times_values (32x2048x2048x2375): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2375): 85.538

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1724.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2376x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2376x2048): 92.890
Elapsed time for attention_prob_times_values (32x2048x2048x2376): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2376): 71.108

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1575.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2377x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2377x2048): 90.767
Elapsed time for attention_prob_times_values (32x2048x2048x2377): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2377): 85.542

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1723.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2378x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2378x2048): 91.666
Elapsed time for attention_prob_times_values (32x2048x2048x2378): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2378): 88.220

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1760.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2379x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2379x2048): 90.964
Elapsed time for attention_prob_times_values (32x2048x2048x2379): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2379): 82.906

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1699.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2380x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2380x2048): 90.823
Elapsed time for attention_prob_times_values (32x2048x2048x2380): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2380): 88.300

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1754.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2381x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2381x2048): 87.955
Elapsed time for attention_prob_times_values (32x2048x2048x2381): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2381): 85.911

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1703.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2382x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2382x2048): 91.767
Elapsed time for attention_prob_times_values (32x2048x2048x2382): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2382): 85.722

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1738.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2383x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2383x2048): 90.888
Elapsed time for attention_prob_times_values (32x2048x2048x2383): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2383): 83.962

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1712.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2384x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2384x2048): 93.083
Elapsed time for attention_prob_times_values (32x2048x2048x2384): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2384): 84.071

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1733.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2385x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2385x2048): 88.265
Elapsed time for attention_prob_times_values (32x2048x2048x2385): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2385): 86.124

Attention duration (in seconds): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1375x2048): 87.428
Elapsed time for attention_prob_times_values (80x2048x2048x1375): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1375): 70.897

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2181.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1376x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1376x2048): 95.952
Elapsed time for attention_prob_times_values (80x2048x2048x1376): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1376): 93.339

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2637.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1377x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1377x2048): 84.655
Elapsed time for attention_prob_times_values (80x2048x2048x1377): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1377): 70.880

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2152.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1378x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1378x2048): 82.293
Elapsed time for attention_prob_times_values (80x2048x2048x1378): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1378): 79.760

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2261.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1379x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1379x2048): 77.505
Elapsed time for attention_prob_times_values (80x2048x2048x1379): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1379): 69.335

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2044.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1380x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1380x2048): 78.372
Elapsed time for attention_prob_times_values (80x2048x2048x1380): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1380): 76.761

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2167.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1381x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1381x2048): 83.974
Elapsed time for attention_prob_times_values (80x2048x2048x1381): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1381): 69.645

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2129.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1382x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1382x2048): 83.868
Elapsed time for attention_prob_times_values (80x2048x2048x1382): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1382): 80.041

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2292.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1383x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1383x2048): 80.799
Elapsed time for attention_prob_times_values (80x2048x2048x1383): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1383): 71.683

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2127.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1384x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1384x2048): 86.066
Elapsed time for attention_prob_times_values (80x2048x2048x1384): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1384): 87.856

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2437.367
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 1711.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2386x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2386x2048): 89.630
Elapsed time for attention_prob_times_values (32x2048x2048x2386): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2386): 88.514

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1749.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2387x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2387x2048): 90.657
Elapsed time for attention_prob_times_values (32x2048x2048x2387): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2387): 86.163

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1735.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2388x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2388x2048): 92.210
Elapsed time for attention_prob_times_values (32x2048x2048x2388): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2388): 88.144

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1771.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2389x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2389x2048): 88.631
Elapsed time for attention_prob_times_values (32x2048x2048x2389): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2389): 86.327

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1719.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2390x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2390x2048): 91.512
Elapsed time for attention_prob_times_values (32x2048x2048x2390): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2390): 88.592

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1771.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2391x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2391x2048): 90.729
Elapsed time for attention_prob_times_values (32x2048x2048x2391): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2391): 86.007

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1737.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2392x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2392x2048): 92.787
Elapsed time for attention_prob_times_values (32x2048x2048x2392): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2392): 70.227

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1573.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2393x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2393x2048): 90.729
Elapsed time for attention_prob_times_values (32x2048x2048x2393): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2393): 86.402

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1743.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2394x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2394x2048): 91.351
Elapsed time for attention_prob_times_values (32x2048x2048x2394): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2394): 88.691

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1773.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2395x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2395x2048): 89.033
Elapsed time for attention_prob_times_values (32x2048x2048x2395): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2395): 84.377

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1707.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2396x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2396x2048): 92.098
Elapsed time for attention_prob_times_values (32x2048x2048x2396): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2396): 88.823

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1783.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2397x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2397x2048): 90.475
Elapsed time for attention_prob_times_values (32x2048x2048x2397): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2397): 86.450

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1744.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2398x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2398x2048): 89.540
Elapsed time for attention_prob_times_values (32x2048x2048x2398): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2398): 88.898

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1760.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2399x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2399x2048): 89.909
Elapsed time for attention_prob_times_values (32x2048x2048x2399): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2399): 86.444

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1740.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2400x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2400x2048): 100.113
Elapsed time for attention_prob_times_values (32x2048x2048x2400): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2400): 87.074

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1839.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2401x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2401x2048): 91.643
Elapsed time for attention_prob_times_values (32x2048x2048x2401): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2401): 86.279

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1756.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2402x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2402x2048): 92.519
Elapsed time for attention_prob_times_values (32x2048x2048x2402): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2402): 88.885

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1792.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2403x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2403x2048): 88.764
Elapsed time for attention_prob_times_values (32x2048x2048x2403): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2403): 86.491

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1732.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2404x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2404x2048): 92.987
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1385x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1385x2048): 83.882
Elapsed time for attention_prob_times_values (80x2048x2048x1385): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1385): 74.355

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2211.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1386x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1386x2048): 84.970
Elapsed time for attention_prob_times_values (80x2048x2048x1386): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1386): 80.249

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2316.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1387x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1387x2048): 82.503
Elapsed time for attention_prob_times_values (80x2048x2048x1387): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1387): 72.880

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2173.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1388x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1388x2048): 85.212
Elapsed time for attention_prob_times_values (80x2048x2048x1388): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1388): 78.877

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2302.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1389x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1389x2048): 79.822
Elapsed time for attention_prob_times_values (80x2048x2048x1389): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1389): 75.043

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2176.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1390x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1390x2048): 82.931
Elapsed time for attention_prob_times_values (80x2048x2048x1390): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1390): 78.916

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2276.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1391x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1391x2048): 78.228
Elapsed time for attention_prob_times_values (80x2048x2048x1391): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1391): 70.102

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2082.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1392x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1392x2048): 86.516
Elapsed time for attention_prob_times_values (80x2048x2048x1392): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1392): 88.060

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2460.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1393x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1393x2048): 80.014
Elapsed time for attention_prob_times_values (80x2048x2048x1393): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1393): 75.058

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2184.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (32x2048x2048x2404): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2404): 89.064

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1799.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2405x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2405x2048): 91.424
Elapsed time for attention_prob_times_values (32x2048x2048x2405): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2405): 86.505

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1759.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2406x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2406x2048): 92.137
Elapsed time for attention_prob_times_values (32x2048x2048x2406): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2406): 86.533

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1766.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2407x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2407x2048): 88.810
Elapsed time for attention_prob_times_values (32x2048x2048x2407): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2407): 86.619

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1736.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2408x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2408x2048): 92.950
Elapsed time for attention_prob_times_values (32x2048x2048x2408): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2408): 69.606

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1577.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2409x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2409x2048): 90.509
Elapsed time for attention_prob_times_values (32x2048x2048x2409): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2409): 86.577

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1754.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2410x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2410x2048): 91.716
Elapsed time for attention_prob_times_values (32x2048x2048x2410): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2410): 89.143

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1792.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2411x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2411x2048): 91.028
Elapsed time for attention_prob_times_values (32x2048x2048x2411): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2411): 86.552

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1760.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2412x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2412x2048): 92.348
Elapsed time for attention_prob_times_values (32x2048x2048x2412): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2412): 89.417

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1802.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2413x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2413x2048): 90.866
Elapsed time for attention_prob_times_values (32x2048x2048x2413): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2413): 86.682

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1761.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
num_attention_heads: 20, hidden_size: 27880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1394x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1394x2048): 80.852
Elapsed time for attention_prob_times_values (80x2048x2048x1394): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1394): 75.551

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2204.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1395x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1395x2048): 83.471
Elapsed time for attention_prob_times_values (80x2048x2048x1395): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1395): 74.684

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2226.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1396x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1396x2048): 81.597
Elapsed time for attention_prob_times_values (80x2048x2048x1396): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1396): 81.079

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2299.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1397x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1397x2048): 78.492
Elapsed time for attention_prob_times_values (80x2048x2048x1397): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1397): 69.974

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2092.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1398x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1398x2048): 84.579
Elapsed time for attention_prob_times_values (80x2048x2048x1398): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1398): 80.019

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2327.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1399x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1399x2048): 83.661
Elapsed time for attention_prob_times_values (80x2048x2048x1399): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1399): 73.833

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2221.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1400x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1400x2048): 83.595
Elapsed time for attention_prob_times_values (80x2048x2048x1400): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1400): 93.890

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2506.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1401x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1401x2048): 80.774
Elapsed time for attention_prob_times_values (80x2048x2048x1401): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1401): 74.481

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2198.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1402x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1402x2048): 83.689
Elapsed time for attention_prob_times_values (80x2048x2048x1402): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1402): 80.320

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2326.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1403x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1403x2048): 83.293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2414x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2414x2048): 91.980
Elapsed time for attention_prob_times_values (32x2048x2048x2414): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2414): 89.226

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1798.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2415x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2415x2048): 91.072
Elapsed time for attention_prob_times_values (32x2048x2048x2415): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2415): 86.660

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1764.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2416x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2416x2048): 93.458
Elapsed time for attention_prob_times_values (32x2048x2048x2416): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2416): 85.088

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1770.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2417x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2417x2048): 90.819
Elapsed time for attention_prob_times_values (32x2048x2048x2417): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2417): 86.790

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1764.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2418x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2418x2048): 92.077
Elapsed time for attention_prob_times_values (32x2048x2048x2418): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2418): 89.492

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1805.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2419x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2419x2048): 90.940
Elapsed time for attention_prob_times_values (32x2048x2048x2419): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2419): 86.892

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1768.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2420x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2420x2048): 92.228
Elapsed time for attention_prob_times_values (32x2048x2048x2420): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2420): 89.560

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1808.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2421x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2421x2048): 90.969
Elapsed time for attention_prob_times_values (32x2048x2048x2421): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2421): 86.903

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1770.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2422x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2422x2048): 91.736
Elapsed time for attention_prob_times_values (32x2048x2048x2422): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2422): 89.578

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1805.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2423x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2423x2048): 90.887
Elapsed time for attention_prob_times_values (32x2048x2048x2423): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2423): 86.823

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1769.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2424x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2424x2048): 92.675
Elapsed time for attention_prob_times_values (32x2048x2048x2424): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2424): 71.299

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1606.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2425x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2425x2048): 90.443
Elapsed time for attention_prob_times_values (32x2048x2048x2425): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2425): 87.099

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1769.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2426x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2426x2048): 91.453
Elapsed time for attention_prob_times_values (32x2048x2048x2426): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2426): 89.705

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1807.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2427x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2427x2048): 90.494
Elapsed time for attention_prob_times_values (32x2048x2048x2427): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2427): 87.055

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1771.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2428x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2428x2048): 92.158
Elapsed time for attention_prob_times_values (32x2048x2048x2428): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2428): 89.782

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1816.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2429x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2429x2048): 90.243
Elapsed time for attention_prob_times_values (32x2048x2048x2429): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2429): 87.240

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1772.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2430x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2430x2048): 91.662
Elapsed time for attention_prob_times_values (32x2048x2048x2430): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2430): 89.788

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1812.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2431x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2431x2048): 90.715
Elapsed time for attention_prob_times_values (32x2048x2048x2431): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2431): 87.176

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1777.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2432x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2432x2048): 98.404
Elapsed time for attention_prob_times_values (32x2048x2048x2432): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2432): 92.757

Attention duration (in seconds): 0.0137
Elapsed time for attention_prob_times_values (80x2048x2048x1403): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1403): 75.026

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2242.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1404x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1404x2048): 84.834
Elapsed time for attention_prob_times_values (80x2048x2048x1404): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1404): 81.560

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2363.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1405x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1405x2048): 83.217
Elapsed time for attention_prob_times_values (80x2048x2048x1405): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1405): 74.568

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2237.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1406x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1406x2048): 83.835
Elapsed time for attention_prob_times_values (80x2048x2048x1406): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1406): 81.414

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2351.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1407x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1407x2048): 82.669
Elapsed time for attention_prob_times_values (80x2048x2048x1407): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1407): 75.313

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2244.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1408x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1408x2048): 92.877
Elapsed time for attention_prob_times_values (80x2048x2048x1408): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1408): 96.474

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2697.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1409x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1409x2048): 83.869
Elapsed time for attention_prob_times_values (80x2048x2048x1409): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1409): 66.890

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2122.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1410x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1410x2048): 84.823
Elapsed time for attention_prob_times_values (80x2048x2048x1410): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1410): 73.790

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2252.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1411x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1411x2048): 78.342
Elapsed time for attention_prob_times_values (80x2048x2048x1411): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1411): 68.777

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2091.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1412x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1412x2048): 80.751
Elapsed time for attention_prob_times_values (80x2048x2048x1412): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1412): 71.267

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2163.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1909.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2433x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2433x2048): 91.594
Elapsed time for attention_prob_times_values (32x2048x2048x2433): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2433): 83.951

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1752.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2434x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2434x2048): 92.642
Elapsed time for attention_prob_times_values (32x2048x2048x2434): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2434): 86.877

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1794.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2435x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2435x2048): 91.067
Elapsed time for attention_prob_times_values (32x2048x2048x2435): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2435): 84.408

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1754.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2436x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2436x2048): 93.362
Elapsed time for attention_prob_times_values (32x2048x2048x2436): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2436): 86.923

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1803.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2437x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2437x2048): 92.487
Elapsed time for attention_prob_times_values (32x2048x2048x2437): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2437): 84.386

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1768.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2438x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2438x2048): 92.488
Elapsed time for attention_prob_times_values (32x2048x2048x2438): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2438): 86.865

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1795.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2439x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2439x2048): 91.402
Elapsed time for attention_prob_times_values (32x2048x2048x2439): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2439): 84.468

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1760.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2440x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2440x2048): 93.560
Elapsed time for attention_prob_times_values (32x2048x2048x2440): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2440): 84.231

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1778.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2441x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2441x2048): 91.516
Elapsed time for attention_prob_times_values (32x2048x2048x2441): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2441): 84.551

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1764.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1413x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1413x2048): 84.324
Elapsed time for attention_prob_times_values (80x2048x2048x1413): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1413): 68.072

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2154.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1414x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1414x2048): 82.921
Elapsed time for attention_prob_times_values (80x2048x2048x1414): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1414): 74.962

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2253.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1415x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1415x2048): 84.233
Elapsed time for attention_prob_times_values (80x2048x2048x1415): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1415): 69.387

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2179.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1416x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1416x2048): 86.315
Elapsed time for attention_prob_times_values (80x2048x2048x1416): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1416): 87.442

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2489.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1417x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1417x2048): 83.894
Elapsed time for attention_prob_times_values (80x2048x2048x1417): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1417): 69.757

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2184.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1418x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1418x2048): 84.283
Elapsed time for attention_prob_times_values (80x2048x2048x1418): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1418): 74.603

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2271.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1419x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1419x2048): 83.366
Elapsed time for attention_prob_times_values (80x2048x2048x1419): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1419): 69.913

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2183.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1420x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1420x2048): 85.562
Elapsed time for attention_prob_times_values (80x2048x2048x1420): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1420): 75.398

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2303.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1421x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1421x2048): 79.327
Elapsed time for attention_prob_times_values (80x2048x2048x1421): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1421): 69.537

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2130.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
num_attention_heads: 8, hidden_size: 19536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2442x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2442x2048): 91.896
Elapsed time for attention_prob_times_values (32x2048x2048x2442): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2442): 86.950

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1794.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2443x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2443x2048): 91.097
Elapsed time for attention_prob_times_values (32x2048x2048x2443): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2443): 84.667

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1762.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2444x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2444x2048): 93.041
Elapsed time for attention_prob_times_values (32x2048x2048x2444): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2444): 87.107

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1807.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2445x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2445x2048): 91.365
Elapsed time for attention_prob_times_values (32x2048x2048x2445): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2445): 84.580

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1765.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2446x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2446x2048): 92.192
Elapsed time for attention_prob_times_values (32x2048x2048x2446): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2446): 87.004

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1800.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2447x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2447x2048): 91.494
Elapsed time for attention_prob_times_values (32x2048x2048x2447): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2447): 84.772

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1770.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2448x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2448x2048): 94.569
Elapsed time for attention_prob_times_values (32x2048x2048x2448): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2448): 83.337

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1783.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2449x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2449x2048): 91.402
Elapsed time for attention_prob_times_values (32x2048x2048x2449): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2449): 84.870

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1771.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2450x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2450x2048): 91.890
Elapsed time for attention_prob_times_values (32x2048x2048x2450): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2450): 87.160

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1801.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2451x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2451x2048): 91.431
Elapsed time for attention_prob_times_values (32x2048x2048x2451): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2451): 84.892

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1773.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2452x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2452x2048): 93.953
Elapsed time for attention_prob_times_values (32x2048x2048x2452): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2452): 87.273

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1823.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2453x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2453x2048): 91.258
Elapsed time for attention_prob_times_values (32x2048x2048x2453): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2453): 84.951

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1774.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2454x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2454x2048): 91.924
Elapsed time for attention_prob_times_values (32x2048x2048x2454): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2454): 87.242

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1805.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2455x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2455x2048): 91.110
Elapsed time for attention_prob_times_values (32x2048x2048x2455): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2455): 84.965

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1774.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2456x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2456x2048): 93.211
Elapsed time for attention_prob_times_values (32x2048x2048x2456): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2456): 82.529

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1767.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2457x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2457x2048): 90.602
Elapsed time for attention_prob_times_values (32x2048x2048x2457): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2457): 85.011

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1771.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2458x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2458x2048): 91.514
Elapsed time for attention_prob_times_values (32x2048x2048x2458): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2458): 87.310

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1805.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2459x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2459x2048): 90.750
Elapsed time for attention_prob_times_values (32x2048x2048x2459): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2459): 85.049

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1774.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2460x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2460x2048): 92.490
Elapsed time for attention_prob_times_values (32x2048x2048x2460): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2460): 87.494

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1818.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
--------
Elapsed time for attention_key_query_prob (80x2048x1422x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1422x2048): 85.038
Elapsed time for attention_prob_times_values (80x2048x2048x1422): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1422): 74.437

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2284.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1423x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1423x2048): 83.986
Elapsed time for attention_prob_times_values (80x2048x2048x1423): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1423): 69.884

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2196.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1424x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1424x2048): 86.887
Elapsed time for attention_prob_times_values (80x2048x2048x1424): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1424): 88.215

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2522.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1425x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1425x2048): 83.737
Elapsed time for attention_prob_times_values (80x2048x2048x1425): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1425): 70.460

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2206.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1426x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1426x2048): 84.967
Elapsed time for attention_prob_times_values (80x2048x2048x1426): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1426): 75.295

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2303.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1427x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1427x2048): 83.719
Elapsed time for attention_prob_times_values (80x2048x2048x1427): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1427): 70.369

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2207.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1428x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1428x2048): 85.171
Elapsed time for attention_prob_times_values (80x2048x2048x1428): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1428): 74.848

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2301.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1429x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1429x2048): 83.836
Elapsed time for attention_prob_times_values (80x2048x2048x1429): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1429): 70.336

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2211.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1430x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1430x2048): 84.782
Elapsed time for attention_prob_times_values (80x2048x2048x1430): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1430): 72.638

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2263.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1431x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1431x2048): 82.913
Elapsed time for attention_prob_times_values (80x2048x2048x1431): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1431): 70.138

Attention duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2461x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2461x2048): 90.770
Elapsed time for attention_prob_times_values (32x2048x2048x2461): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2461): 85.099

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1776.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2462x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2462x2048): 91.860
Elapsed time for attention_prob_times_values (32x2048x2048x2462): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2462): 87.502

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1813.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2463x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2463x2048): 92.292
Elapsed time for attention_prob_times_values (32x2048x2048x2463): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2463): 85.195

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1793.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2464x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2464x2048): 100.127
Elapsed time for attention_prob_times_values (32x2048x2048x2464): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2464): 87.803

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1894.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2465x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2465x2048): 92.204
Elapsed time for attention_prob_times_values (32x2048x2048x2465): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2465): 85.272

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1794.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2466x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2466x2048): 93.267
Elapsed time for attention_prob_times_values (32x2048x2048x2466): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2466): 87.572

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1830.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2467x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2467x2048): 93.336
Elapsed time for attention_prob_times_values (32x2048x2048x2467): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2467): 85.257

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1806.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2468x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2468x2048): 94.251
Elapsed time for attention_prob_times_values (32x2048x2048x2468): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2468): 87.676

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1842.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2469x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2469x2048): 91.521
Elapsed time for attention_prob_times_values (32x2048x2048x2469): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2469): 85.198

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1790.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2470x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2470x2048): 92.545
Elapsed time for attention_prob_times_values (32x2048x2048x2470): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2470): 87.631

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1827.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2471x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2471x2048): 92.113
Elapsed time for attention_prob_times_values (32x2048x2048x2471): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2471): 85.207

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1797.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2472x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2472x2048): 93.365
Elapsed time for attention_prob_times_values (32x2048x2048x2472): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2472): 81.641

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1769.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2473x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2473x2048): 91.007
Elapsed time for attention_prob_times_values (32x2048x2048x2473): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2473): 85.274

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1789.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2474x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2474x2048): 91.915
Elapsed time for attention_prob_times_values (32x2048x2048x2474): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2474): 87.734

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1824.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2475x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2475x2048): 91.334
Elapsed time for attention_prob_times_values (32x2048x2048x2475): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2475): 85.322

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1794.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2476x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2476x2048): 92.760
Elapsed time for attention_prob_times_values (32x2048x2048x2476): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2476): 87.977

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1837.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2477x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2477x2048): 91.145
Elapsed time for attention_prob_times_values (32x2048x2048x2477): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2477): 85.359

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1794.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2478x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2478x2048): 92.289
Elapsed time for attention_prob_times_values (32x2048x2048x2478): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2478): 87.882

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1832.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2479x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2479x2048): 91.831
Elapsed time for attention_prob_times_values (32x2048x2048x2479): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2479): 85.394

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 2199.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1432x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1432x2048): 82.265
Elapsed time for attention_prob_times_values (80x2048x2048x1432): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1432): 87.064

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2450.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1433x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1433x2048): 83.496
Elapsed time for attention_prob_times_values (80x2048x2048x1433): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1433): 70.118

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2209.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1434x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1434x2048): 83.853
Elapsed time for attention_prob_times_values (80x2048x2048x1434): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1434): 75.555

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2305.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1435x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1435x2048): 83.357
Elapsed time for attention_prob_times_values (80x2048x2048x1435): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1435): 70.489

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2217.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1436x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1436x2048): 85.128
Elapsed time for attention_prob_times_values (80x2048x2048x1436): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1436): 74.802

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2313.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1437x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1437x2048): 83.021
Elapsed time for attention_prob_times_values (80x2048x2048x1437): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1437): 70.959

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2224.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1438x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1438x2048): 84.311
Elapsed time for attention_prob_times_values (80x2048x2048x1438): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1438): 75.559

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2318.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1439x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1439x2048): 83.509
Elapsed time for attention_prob_times_values (80x2048x2048x1439): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1439): 71.308

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2239.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1440x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1440x2048): 96.949
Elapsed time for attention_prob_times_values (80x2048x2048x1440): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1440): 89.745

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2714.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention throughput (in TFLOP/s): 1802.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2480x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2480x2048): 93.636
Elapsed time for attention_prob_times_values (32x2048x2048x2480): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2480): 87.078

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1838.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2481x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2481x2048): 90.900
Elapsed time for attention_prob_times_values (32x2048x2048x2481): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2481): 85.618

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1797.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2482x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2482x2048): 92.060
Elapsed time for attention_prob_times_values (32x2048x2048x2482): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2482): 88.054

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1835.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2483x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2483x2048): 90.926
Elapsed time for attention_prob_times_values (32x2048x2048x2483): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2483): 85.427

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1796.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2484x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2484x2048): 92.622
Elapsed time for attention_prob_times_values (32x2048x2048x2484): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2484): 88.172

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1843.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2485x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2485x2048): 90.751
Elapsed time for attention_prob_times_values (32x2048x2048x2485): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2485): 85.546

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1797.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2486x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2486x2048): 88.542
Elapsed time for attention_prob_times_values (32x2048x2048x2486): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2486): 87.930

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1801.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2487x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2487x2048): 87.586
Elapsed time for attention_prob_times_values (32x2048x2048x2487): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2487): 85.487

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1767.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2488x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2488x2048): 92.907
Elapsed time for attention_prob_times_values (32x2048x2048x2488): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2488): 76.293

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1712.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 20, hidden_size: 28820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1441x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1441x2048): 84.655
Elapsed time for attention_prob_times_values (80x2048x2048x1441): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1441): 71.251

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2255.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1442x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1442x2048): 86.143
Elapsed time for attention_prob_times_values (80x2048x2048x1442): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1442): 75.877

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2353.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1443x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1443x2048): 84.946
Elapsed time for attention_prob_times_values (80x2048x2048x1443): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1443): 71.230

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2261.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1444x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1444x2048): 86.615
Elapsed time for attention_prob_times_values (80x2048x2048x1444): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1444): 76.178

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2367.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1445x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1445x2048): 84.716
Elapsed time for attention_prob_times_values (80x2048x2048x1445): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1445): 71.093

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2259.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1446x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1446x2048): 85.855
Elapsed time for attention_prob_times_values (80x2048x2048x1446): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1446): 75.995

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2357.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1447x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1447x2048): 84.652
Elapsed time for attention_prob_times_values (80x2048x2048x1447): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1447): 70.909

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2258.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1448x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1448x2048): 86.857
Elapsed time for attention_prob_times_values (80x2048x2048x1448): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1448): 89.261

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2577.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1449x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1449x2048): 84.275
Elapsed time for attention_prob_times_values (80x2048x2048x1449): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1449): 70.995

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2258.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1450x2048): 0.0115
num_attention_heads: 8, hidden_size: 19912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2489x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2489x2048): 90.724
Elapsed time for attention_prob_times_values (32x2048x2048x2489): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2489): 83.615

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1779.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2490x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2490x2048): 91.584
Elapsed time for attention_prob_times_values (32x2048x2048x2490): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2490): 88.324

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1839.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2491x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2491x2048): 88.488
Elapsed time for attention_prob_times_values (32x2048x2048x2491): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2491): 85.813

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1782.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2492x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2492x2048): 88.995
Elapsed time for attention_prob_times_values (32x2048x2048x2492): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2492): 88.439

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1815.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2493x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2493x2048): 90.820
Elapsed time for attention_prob_times_values (32x2048x2048x2493): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2493): 82.664

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1772.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2494x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2494x2048): 91.879
Elapsed time for attention_prob_times_values (32x2048x2048x2494): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2494): 84.877

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1807.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2495x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2495x2048): 91.063
Elapsed time for attention_prob_times_values (32x2048x2048x2495): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2495): 85.995

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1812.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2496x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2496x2048): 96.251
Elapsed time for attention_prob_times_values (32x2048x2048x2496): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2496): 90.767

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1915.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2497x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2497x2048): 91.893
Elapsed time for attention_prob_times_values (32x2048x2048x2497): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2497): 82.978

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1788.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2498x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2498x2048): 92.895
Elapsed time for attention_prob_times_values (32x2048x2048x2498): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2498): 88.599

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1860.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 19992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2499x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2499x2048): 91.840
Elapsed time for attention_prob_times_values (32x2048x2048x2499): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2499): 86.072

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1823.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2500x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2500x2048): 90.472
Elapsed time for attention_prob_times_values (32x2048x2048x2500): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2500): 88.648

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1838.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2501x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2501x2048): 88.885
Elapsed time for attention_prob_times_values (32x2048x2048x2501): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2501): 86.110

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1796.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2502x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2502x2048): 92.525
Elapsed time for attention_prob_times_values (32x2048x2048x2502): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2502): 86.308

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1835.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2503x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2503x2048): 91.471
Elapsed time for attention_prob_times_values (32x2048x2048x2503): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2503): 83.691

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1796.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2504x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2504x2048): 93.337
Elapsed time for attention_prob_times_values (32x2048x2048x2504): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2504): 74.910

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1709.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2505x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2505x2048): 88.178
Elapsed time for attention_prob_times_values (32x2048x2048x2505): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2505): 86.358

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1794.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2506x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2506x2048): 88.962
Elapsed time for attention_prob_times_values (32x2048x2048x2506): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2506): 88.699

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1827.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2507x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2507x2048): 91.309
Elapsed time for attention_prob_times_values (32x2048x2048x2507): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2507): 86.279

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1826.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1450x2048): 84.829
Elapsed time for attention_prob_times_values (80x2048x2048x1450): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1450): 76.118

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2352.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1451x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1451x2048): 83.753
Elapsed time for attention_prob_times_values (80x2048x2048x1451): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1451): 71.043

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2255.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1452x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1452x2048): 85.692
Elapsed time for attention_prob_times_values (80x2048x2048x1452): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1452): 76.484

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2373.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1453x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1453x2048): 84.537
Elapsed time for attention_prob_times_values (80x2048x2048x1453): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1453): 71.241

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2271.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1454x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1454x2048): 85.534
Elapsed time for attention_prob_times_values (80x2048x2048x1454): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1454): 76.375

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2372.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1455x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1455x2048): 84.530
Elapsed time for attention_prob_times_values (80x2048x2048x1455): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1455): 71.367

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2276.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1456x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1456x2048): 86.668
Elapsed time for attention_prob_times_values (80x2048x2048x1456): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1456): 89.959

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2598.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1457x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1457x2048): 84.258
Elapsed time for attention_prob_times_values (80x2048x2048x1457): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1457): 71.242

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2274.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1458x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1458x2048): 84.993
Elapsed time for attention_prob_times_values (80x2048x2048x1458): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1458): 76.429

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2372.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1459x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1459x2048): 84.011
Elapsed time for attention_prob_times_values (80x2048x2048x1459): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1459): 71.280

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2274.847
MLP duration (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2508x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2508x2048): 92.789
Elapsed time for attention_prob_times_values (32x2048x2048x2508): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2508): 86.056

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1838.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2509x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2509x2048): 88.872
Elapsed time for attention_prob_times_values (32x2048x2048x2509): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2509): 86.343

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1804.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2510x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2510x2048): 92.302
Elapsed time for attention_prob_times_values (32x2048x2048x2510): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2510): 88.840

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1865.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2511x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2511x2048): 89.184
Elapsed time for attention_prob_times_values (32x2048x2048x2511): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2511): 86.565

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1811.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2512x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2512x2048): 93.794
Elapsed time for attention_prob_times_values (32x2048x2048x2512): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2512): 88.605

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1879.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2513x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2513x2048): 90.945
Elapsed time for attention_prob_times_values (32x2048x2048x2513): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2513): 85.005

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1813.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2514x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2514x2048): 89.111
Elapsed time for attention_prob_times_values (32x2048x2048x2514): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2514): 89.015

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1838.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2515x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2515x2048): 90.918
Elapsed time for attention_prob_times_values (32x2048x2048x2515): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2515): 86.669

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1832.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2516x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2516x2048): 92.507
Elapsed time for attention_prob_times_values (32x2048x2048x2516): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2516): 86.991

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1852.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2517x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2517x2048): 91.066
Elapsed time for attention_prob_times_values (32x2048x2048x2517): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2517): 86.793

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1836.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2518x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2518x2048): 91.858
Elapsed time for attention_prob_times_values (32x2048x2048x2518): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2518): 87.379

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1851.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2519x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2519x2048): 88.621
Elapsed time for attention_prob_times_values (32x2048x2048x2519): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2519): 86.812

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1813.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2520x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2520x2048): 92.970
Elapsed time for attention_prob_times_values (32x2048x2048x2520): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2520): 87.901

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1869.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2521x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2521x2048): 90.695
Elapsed time for attention_prob_times_values (32x2048x2048x2521): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2521): 83.946

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1804.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2522x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2522x2048): 91.491
Elapsed time for attention_prob_times_values (32x2048x2048x2522): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2522): 86.793

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1844.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2523x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2523x2048): 90.802
Elapsed time for attention_prob_times_values (32x2048x2048x2523): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2523): 86.910

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1839.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2524x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2524x2048): 92.310
Elapsed time for attention_prob_times_values (32x2048x2048x2524): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2524): 89.240

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1880.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2525x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2525x2048): 90.752
Elapsed time for attention_prob_times_values (32x2048x2048x2525): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2525): 86.984

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1841.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2526x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2526x2048): 91.775
Elapsed time for attention_prob_times_values (32x2048x2048x2526): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2526): 89.238

Attention duration (in seconds): 0.0150
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1460x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1460x2048): 85.783
Elapsed time for attention_prob_times_values (80x2048x2048x1460): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1460): 76.830

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2392.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1461x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1461x2048): 84.328
Elapsed time for attention_prob_times_values (80x2048x2048x1461): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1461): 71.191

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2280.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1462x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1462x2048): 84.998
Elapsed time for attention_prob_times_values (80x2048x2048x1462): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1462): 76.621

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2381.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1463x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1463x2048): 84.430
Elapsed time for attention_prob_times_values (80x2048x2048x1463): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1463): 71.245

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2285.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1464x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1464x2048): 86.330
Elapsed time for attention_prob_times_values (80x2048x2048x1464): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1464): 90.135

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2609.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1465x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1465x2048): 83.977
Elapsed time for attention_prob_times_values (80x2048x2048x1465): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1465): 71.183

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2281.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1466x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1466x2048): 84.829
Elapsed time for attention_prob_times_values (80x2048x2048x1466): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1466): 76.811

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2389.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1467x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1467x2048): 83.957
Elapsed time for attention_prob_times_values (80x2048x2048x1467): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1467): 71.314

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2286.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1468x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1468x2048): 85.318
Elapsed time for attention_prob_times_values (80x2048x2048x1468): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1468): 77.201

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2405.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 1876.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2527x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2527x2048): 90.908
Elapsed time for attention_prob_times_values (32x2048x2048x2527): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2527): 86.936

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1843.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2528x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2528x2048): 100.136
Elapsed time for attention_prob_times_values (32x2048x2048x2528): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2528): 90.172

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1969.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2529x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2529x2048): 91.914
Elapsed time for attention_prob_times_values (32x2048x2048x2529): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2529): 87.138

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1857.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2530x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2530x2048): 92.992
Elapsed time for attention_prob_times_values (32x2048x2048x2530): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2530): 89.399

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1892.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2531x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2531x2048): 92.183
Elapsed time for attention_prob_times_values (32x2048x2048x2531): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2531): 86.980

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1859.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2532x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2532x2048): 93.214
Elapsed time for attention_prob_times_values (32x2048x2048x2532): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2532): 89.296

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1895.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2533x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2533x2048): 91.507
Elapsed time for attention_prob_times_values (32x2048x2048x2533): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2533): 87.054

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1854.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2534x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2534x2048): 92.361
Elapsed time for attention_prob_times_values (32x2048x2048x2534): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2534): 89.518

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1890.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2535x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2535x2048): 91.146
Elapsed time for attention_prob_times_values (32x2048x2048x2535): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2535): 86.916

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1851.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1469x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1469x2048): 83.965
Elapsed time for attention_prob_times_values (80x2048x2048x1469): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1469): 71.334

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2290.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1470x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1470x2048): 85.280
Elapsed time for attention_prob_times_values (80x2048x2048x1470): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1470): 77.152

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2406.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1471x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1471x2048): 84.360
Elapsed time for attention_prob_times_values (80x2048x2048x1471): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1471): 71.462

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2300.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1472x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1472x2048): 92.739
Elapsed time for attention_prob_times_values (80x2048x2048x1472): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1472): 84.781

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2635.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1473x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1473x2048): 85.712
Elapsed time for attention_prob_times_values (80x2048x2048x1473): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1473): 70.061

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2295.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1474x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1474x2048): 83.976
Elapsed time for attention_prob_times_values (80x2048x2048x1474): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1474): 77.384

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2399.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1475x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1475x2048): 83.442
Elapsed time for attention_prob_times_values (80x2048x2048x1475): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1475): 71.203

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2290.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1476x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1476x2048): 87.147
Elapsed time for attention_prob_times_values (80x2048x2048x1476): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1476): 77.297

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2443.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1477x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1477x2048): 82.831
Elapsed time for attention_prob_times_values (80x2048x2048x1477): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1477): 71.700

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2294.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1478x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1478x2048): 86.440
num_attention_heads: 8, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2536x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2536x2048): 93.362
Elapsed time for attention_prob_times_values (32x2048x2048x2536): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2536): 82.575

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1823.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2537x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2537x2048): 91.143
Elapsed time for attention_prob_times_values (32x2048x2048x2537): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2537): 86.966

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1853.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2538x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2538x2048): 91.882
Elapsed time for attention_prob_times_values (32x2048x2048x2538): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2538): 89.699

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1890.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2539x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2539x2048): 91.235
Elapsed time for attention_prob_times_values (32x2048x2048x2539): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2539): 87.168

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1857.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2540x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2540x2048): 92.772
Elapsed time for attention_prob_times_values (32x2048x2048x2540): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2540): 89.587

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1899.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2541x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2541x2048): 91.251
Elapsed time for attention_prob_times_values (32x2048x2048x2541): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2541): 87.271

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1860.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2542x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2542x2048): 92.267
Elapsed time for attention_prob_times_values (32x2048x2048x2542): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2542): 89.809

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1898.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2543x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2543x2048): 91.140
Elapsed time for attention_prob_times_values (32x2048x2048x2543): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2543): 87.276

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1860.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2544x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2544x2048): 93.779
Elapsed time for attention_prob_times_values (32x2048x2048x2544): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2544): 89.086

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1907.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2545x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2545x2048): 90.925
Elapsed time for attention_prob_times_values (32x2048x2048x2545): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2545): 86.414

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1850.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2546x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2546x2048): 92.124
Elapsed time for attention_prob_times_values (32x2048x2048x2546): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2546): 89.810

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1900.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2547x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2547x2048): 91.138
Elapsed time for attention_prob_times_values (32x2048x2048x2547): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2547): 87.322

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1863.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2548x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2548x2048): 89.815
Elapsed time for attention_prob_times_values (32x2048x2048x2548): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2548): 89.754

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1877.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2549x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2549x2048): 89.200
Elapsed time for attention_prob_times_values (32x2048x2048x2549): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2549): 87.463

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1847.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2550x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2550x2048): 92.629
Elapsed time for attention_prob_times_values (32x2048x2048x2550): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2550): 87.695

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1884.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2551x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2551x2048): 91.824
Elapsed time for attention_prob_times_values (32x2048x2048x2551): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2551): 84.720

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1844.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2552x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2552x2048): 93.089
Elapsed time for attention_prob_times_values (32x2048x2048x2552): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2552): 81.954

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1825.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2553x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2553x2048): 88.175
Elapsed time for attention_prob_times_values (32x2048x2048x2553): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2553): 87.616

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1840.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2554x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2554x2048): 89.361
Elapsed time for attention_prob_times_values (32x2048x2048x2554): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2554): 90.199

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1881.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Elapsed time for attention_prob_times_values (80x2048x2048x1478): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1478): 77.559

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2441.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1479x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1479x2048): 85.182
Elapsed time for attention_prob_times_values (80x2048x2048x1479): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1479): 71.960

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2331.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1480x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1480x2048): 84.532
Elapsed time for attention_prob_times_values (80x2048x2048x1480): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1480): 84.563

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2528.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1481x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1481x2048): 83.488
Elapsed time for attention_prob_times_values (80x2048x2048x1481): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1481): 71.154

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2299.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1482x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1482x2048): 84.862
Elapsed time for attention_prob_times_values (80x2048x2048x1482): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1482): 76.037

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2401.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1483x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1483x2048): 84.772
Elapsed time for attention_prob_times_values (80x2048x2048x1483): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1483): 72.351

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2339.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1484x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1484x2048): 84.495
Elapsed time for attention_prob_times_values (80x2048x2048x1484): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1484): 77.900

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2430.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1485x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1485x2048): 83.850
Elapsed time for attention_prob_times_values (80x2048x2048x1485): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1485): 70.459

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2297.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1486x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1486x2048): 85.985
Elapsed time for attention_prob_times_values (80x2048x2048x1486): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1486): 77.999

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2455.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1487x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1487x2048): 83.863
Elapsed time for attention_prob_times_values (80x2048x2048x1487): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1487): 72.204

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2331.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2555x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2555x2048): 91.092
Elapsed time for attention_prob_times_values (32x2048x2048x2555): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2555): 84.858

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1841.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2556x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2556x2048): 92.605
Elapsed time for attention_prob_times_values (32x2048x2048x2556): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2556): 87.732

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1889.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2557x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2557x2048): 90.752
Elapsed time for attention_prob_times_values (32x2048x2048x2557): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2557): 87.775

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1871.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2558x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2558x2048): 91.856
Elapsed time for attention_prob_times_values (32x2048x2048x2558): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2558): 90.372

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1911.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2559x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2559x2048): 90.961
Elapsed time for attention_prob_times_values (32x2048x2048x2559): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2559): 87.806

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1875.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2560x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2560x2048): 99.606
Elapsed time for attention_prob_times_values (32x2048x2048x2560): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2560): 90.414

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1990.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2561x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2561x2048): 90.279
Elapsed time for attention_prob_times_values (32x2048x2048x2561): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2561): 83.387

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1821.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2562x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2562x2048): 92.173
Elapsed time for attention_prob_times_values (32x2048x2048x2562): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2562): 86.842

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1879.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2563x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2563x2048): 90.338
Elapsed time for attention_prob_times_values (32x2048x2048x2563): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2563): 84.251

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1833.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2564x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2564x2048): 92.130
Elapsed time for attention_prob_times_values (32x2048x2048x2564): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2564): 86.824

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1880.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2565x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2565x2048): 90.287
Elapsed time for attention_prob_times_values (32x2048x2048x2565): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2565): 84.245

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1833.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2566x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2566x2048): 92.067
Elapsed time for attention_prob_times_values (32x2048x2048x2566): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2566): 86.636

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1878.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2567x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2567x2048): 90.121
Elapsed time for attention_prob_times_values (32x2048x2048x2567): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2567): 84.385

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1835.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2568x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2568x2048): 93.473
Elapsed time for attention_prob_times_values (32x2048x2048x2568): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2568): 71.812

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1710.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2569x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2569x2048): 90.661
Elapsed time for attention_prob_times_values (32x2048x2048x2569): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2569): 82.104

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1815.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2570x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2570x2048): 93.631
Elapsed time for attention_prob_times_values (32x2048x2048x2570): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2570): 83.874

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1865.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2571x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2571x2048): 91.332
Elapsed time for attention_prob_times_values (32x2048x2048x2571): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2571): 84.613

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1852.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2572x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2572x2048): 90.027
Elapsed time for attention_prob_times_values (32x2048x2048x2572): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2572): 87.137

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1868.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2573x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2573x2048): 88.335
Elapsed time for attention_prob_times_values (32x2048x2048x2573): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2573): 84.654

Attention duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1488x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1488x2048): 84.962
Elapsed time for attention_prob_times_values (80x2048x2048x1488): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1488): 82.892

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2522.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1489x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1489x2048): 84.797
Elapsed time for attention_prob_times_values (80x2048x2048x1489): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1489): 71.481

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2333.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1490x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1490x2048): 85.264
Elapsed time for attention_prob_times_values (80x2048x2048x1490): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1490): 78.773

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2465.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1491x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1491x2048): 83.502
Elapsed time for attention_prob_times_values (80x2048x2048x1491): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1491): 71.242

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2315.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1492x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1492x2048): 86.296
Elapsed time for attention_prob_times_values (80x2048x2048x1492): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1492): 78.080

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2471.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1493x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1493x2048): 84.592
Elapsed time for attention_prob_times_values (80x2048x2048x1493): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1493): 73.205

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2367.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1494x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1494x2048): 83.356
Elapsed time for attention_prob_times_values (80x2048x2048x1494): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1494): 78.405

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2438.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1495x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1495x2048): 83.928
Elapsed time for attention_prob_times_values (80x2048x2048x1495): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1495): 71.842

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2337.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1496x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1496x2048): 86.922
Elapsed time for attention_prob_times_values (80x2048x2048x1496): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1496): 83.663

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2576.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Attention throughput (in TFLOP/s): 1824.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2574x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2574x2048): 92.087
Elapsed time for attention_prob_times_values (32x2048x2048x2574): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2574): 85.035

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1866.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2575x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2575x2048): 90.177
Elapsed time for attention_prob_times_values (32x2048x2048x2575): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2575): 84.752

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1845.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2576x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2576x2048): 94.464
Elapsed time for attention_prob_times_values (32x2048x2048x2576): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2576): 78.049

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1805.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2577x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2577x2048): 89.786
Elapsed time for attention_prob_times_values (32x2048x2048x2577): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2577): 84.929

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1844.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2578x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2578x2048): 91.526
Elapsed time for attention_prob_times_values (32x2048x2048x2578): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2578): 85.508

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1869.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2579x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2579x2048): 89.945
Elapsed time for attention_prob_times_values (32x2048x2048x2579): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2579): 80.148

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1792.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2580x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2580x2048): 91.986
Elapsed time for attention_prob_times_values (32x2048x2048x2580): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2580): 87.368

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1895.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2581x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2581x2048): 84.586
Elapsed time for attention_prob_times_values (32x2048x2048x2581): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2581): 85.019

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1794.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2582x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2582x2048): 89.769
Elapsed time for attention_prob_times_values (32x2048x2048x2582): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2582): 87.315

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1874.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
--------
Elapsed time for attention_key_query_prob (80x2048x1497x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1497x2048): 82.379
Elapsed time for attention_prob_times_values (80x2048x2048x1497): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1497): 73.662

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2351.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1498x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1498x2048): 84.200
Elapsed time for attention_prob_times_values (80x2048x2048x1498): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1498): 77.108

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2435.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1499x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1499x2048): 84.571
Elapsed time for attention_prob_times_values (80x2048x2048x1499): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1499): 73.915

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2388.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1500x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1500x2048): 86.069
Elapsed time for attention_prob_times_values (80x2048x2048x1500): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1500): 79.875

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2510.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1501x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1501x2048): 80.301
Elapsed time for attention_prob_times_values (80x2048x2048x1501): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1501): 73.247

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2322.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1502x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1502x2048): 81.436
Elapsed time for attention_prob_times_values (80x2048x2048x1502): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1502): 77.810

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2414.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1503x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1503x2048): 84.822
Elapsed time for attention_prob_times_values (80x2048x2048x1503): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1503): 73.867

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2397.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1504x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1504x2048): 95.224
Elapsed time for attention_prob_times_values (80x2048x2048x1504): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1504): 85.995

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2745.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1505x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1505x2048): 81.537
Elapsed time for attention_prob_times_values (80x2048x2048x1505): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1505): 74.674

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2369.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1506x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1506x2048): 82.348
Elapsed time for attention_prob_times_values (80x2048x2048x1506): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1506): 77.014

Attention duration (in seconds): 0.0254
num_attention_heads: 8, hidden_size: 20664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2583x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2583x2048): 90.082
Elapsed time for attention_prob_times_values (32x2048x2048x2583): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2583): 84.956

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1852.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2584x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2584x2048): 92.908
Elapsed time for attention_prob_times_values (32x2048x2048x2584): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2584): 70.695

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1701.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2585x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2585x2048): 85.148
Elapsed time for attention_prob_times_values (32x2048x2048x2585): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2585): 85.136

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1804.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2586x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2586x2048): 91.061
Elapsed time for attention_prob_times_values (32x2048x2048x2586): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2586): 87.432

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1891.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2587x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2587x2048): 89.435
Elapsed time for attention_prob_times_values (32x2048x2048x2587): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2587): 81.790

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1812.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2588x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2588x2048): 91.791
Elapsed time for attention_prob_times_values (32x2048x2048x2588): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2588): 82.435

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1843.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2589x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2589x2048): 85.198
Elapsed time for attention_prob_times_values (32x2048x2048x2589): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2589): 85.223

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1808.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2590x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2590x2048): 88.908
Elapsed time for attention_prob_times_values (32x2048x2048x2590): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2590): 87.575

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1873.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2591x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2591x2048): 89.672
Elapsed time for attention_prob_times_values (32x2048x2048x2591): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2591): 81.797

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1817.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2592x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2592x2048): 101.278
Elapsed time for attention_prob_times_values (32x2048x2048x2592): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2592): 83.550

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1945.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2593x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2593x2048): 91.276
Elapsed time for attention_prob_times_values (32x2048x2048x2593): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2593): 85.404

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1875.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2594x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2594x2048): 91.915
Elapsed time for attention_prob_times_values (32x2048x2048x2594): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2594): 87.705

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1908.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2595x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2595x2048): 90.554
Elapsed time for attention_prob_times_values (32x2048x2048x2595): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2595): 85.348

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1869.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2596x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2596x2048): 93.271
Elapsed time for attention_prob_times_values (32x2048x2048x2596): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2596): 87.857

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1925.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2597x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2597x2048): 90.621
Elapsed time for attention_prob_times_values (32x2048x2048x2597): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2597): 84.022

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1856.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2598x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2598x2048): 91.759
Elapsed time for attention_prob_times_values (32x2048x2048x2598): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2598): 87.806

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1911.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2599x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2599x2048): 90.730
Elapsed time for attention_prob_times_values (32x2048x2048x2599): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2599): 85.412

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1874.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2600x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2600x2048): 93.762
Elapsed time for attention_prob_times_values (32x2048x2048x2600): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2600): 71.566

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1730.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2601x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2601x2048): 90.427
Elapsed time for attention_prob_times_values (32x2048x2048x2601): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2601): 84.589

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1863.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 2420.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1507x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1507x2048): 80.642
Elapsed time for attention_prob_times_values (80x2048x2048x1507): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1507): 71.162

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2300.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1508x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1508x2048): 87.437
Elapsed time for attention_prob_times_values (80x2048x2048x1508): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1508): 76.012

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2476.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1509x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1509x2048): 83.012
Elapsed time for attention_prob_times_values (80x2048x2048x1509): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1509): 73.916

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2382.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1510x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1510x2048): 81.824
Elapsed time for attention_prob_times_values (80x2048x2048x1510): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1510): 75.656

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2397.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1511x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1511x2048): 85.355
Elapsed time for attention_prob_times_values (80x2048x2048x1511): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1511): 70.888

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2363.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1512x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1512x2048): 85.694
Elapsed time for attention_prob_times_values (80x2048x2048x1512): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1512): 86.558

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2629.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1513x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1513x2048): 82.264
Elapsed time for attention_prob_times_values (80x2048x2048x1513): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1513): 70.863

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2326.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1514x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1514x2048): 83.134
Elapsed time for attention_prob_times_values (80x2048x2048x1514): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1514): 75.633

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2421.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1515x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1515x2048): 84.917
Elapsed time for attention_prob_times_values (80x2048x2048x1515): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1515): 72.982

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2401.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2602x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2602x2048): 91.900
Elapsed time for attention_prob_times_values (32x2048x2048x2602): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2602): 87.902

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1916.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2603x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2603x2048): 90.245
Elapsed time for attention_prob_times_values (32x2048x2048x2603): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2603): 85.399

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1872.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2604x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2604x2048): 91.881
Elapsed time for attention_prob_times_values (32x2048x2048x2604): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2604): 86.411

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1900.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2605x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2605x2048): 90.390
Elapsed time for attention_prob_times_values (32x2048x2048x2605): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2605): 85.477

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1876.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2606x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2606x2048): 92.076
Elapsed time for attention_prob_times_values (32x2048x2048x2606): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2606): 86.700

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1907.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2607x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2607x2048): 89.663
Elapsed time for attention_prob_times_values (32x2048x2048x2607): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2607): 85.523

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1870.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2608x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2608x2048): 94.032
Elapsed time for attention_prob_times_values (32x2048x2048x2608): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2608): 73.412

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1762.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2609x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2609x2048): 89.988
Elapsed time for attention_prob_times_values (32x2048x2048x2609): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2609): 84.739

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1866.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2610x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2610x2048): 91.591
Elapsed time for attention_prob_times_values (32x2048x2048x2610): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2610): 84.862

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1884.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2611x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2611x2048): 90.010
Elapsed time for attention_prob_times_values (32x2048x2048x2611): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2611): 85.606

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1877.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2612x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2612x2048): 92.219
Elapsed time for attention_prob_times_values (32x2048x2048x2612): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2612): 88.237

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1930.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2613x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2613x2048): 84.783
Elapsed time for attention_prob_times_values (32x2048x2048x2613): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2613): 85.640

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1824.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2614x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2614x2048): 91.477
Elapsed time for attention_prob_times_values (32x2048x2048x2614): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2614): 82.606

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1859.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2615x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2615x2048): 90.002
Elapsed time for attention_prob_times_values (32x2048x2048x2615): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2615): 85.633

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1880.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2616x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2616x2048): 90.568
Elapsed time for attention_prob_times_values (32x2048x2048x2616): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2616): 69.120

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1680.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2617x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2617x2048): 88.637
Elapsed time for attention_prob_times_values (32x2048x2048x2617): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2617): 85.601

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1867.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2618x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2618x2048): 91.311
Elapsed time for attention_prob_times_values (32x2048x2048x2618): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2618): 88.289

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1925.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2619x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2619x2048): 89.535
Elapsed time for attention_prob_times_values (32x2048x2048x2619): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2619): 84.946

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1870.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2620x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2620x2048): 91.287
Elapsed time for attention_prob_times_values (32x2048x2048x2620): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2620): 88.383

Attention duration (in seconds): 0.0157
========================================================================================================================
num_attention_heads: 20, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1516x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1516x2048): 80.963
Elapsed time for attention_prob_times_values (80x2048x2048x1516): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1516): 80.683

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2473.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1517x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1517x2048): 82.386
Elapsed time for attention_prob_times_values (80x2048x2048x1517): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1517): 72.620

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2364.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1518x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1518x2048): 80.031
Elapsed time for attention_prob_times_values (80x2048x2048x1518): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1518): 74.462

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2364.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1519x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1519x2048): 85.243
Elapsed time for attention_prob_times_values (80x2048x2048x1519): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1519): 71.241

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2380.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1520x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1520x2048): 85.527
Elapsed time for attention_prob_times_values (80x2048x2048x1520): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1520): 87.538

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2655.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1521x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1521x2048): 79.146
Elapsed time for attention_prob_times_values (80x2048x2048x1521): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1521): 73.305

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2337.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1522x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1522x2048): 85.134
Elapsed time for attention_prob_times_values (80x2048x2048x1522): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1522): 78.452

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2509.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1523x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1523x2048): 84.808
Elapsed time for attention_prob_times_values (80x2048x2048x1523): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1523): 71.370

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2383.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1524x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1524x2048): 81.103
Elapsed time for attention_prob_times_values (80x2048x2048x1524): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1524): 80.872

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2491.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1525x2048): 0.0128
Attention throughput (in TFLOP/s): 1928.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2621x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2621x2048): 88.084
Elapsed time for attention_prob_times_values (32x2048x2048x2621): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2621): 85.802

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1866.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2622x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2622x2048): 91.109
Elapsed time for attention_prob_times_values (32x2048x2048x2622): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2622): 87.032

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1912.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2623x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2623x2048): 89.759
Elapsed time for attention_prob_times_values (32x2048x2048x2623): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2623): 85.186

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1878.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2624x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2624x2048): 100.949
Elapsed time for attention_prob_times_values (32x2048x2048x2624): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2624): 89.834

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 2043.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2625x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2625x2048): 90.457
Elapsed time for attention_prob_times_values (32x2048x2048x2625): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2625): 85.821

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1894.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2626x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2626x2048): 92.600
Elapsed time for attention_prob_times_values (32x2048x2048x2626): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2626): 88.566

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1947.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2627x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2627x2048): 91.122
Elapsed time for attention_prob_times_values (32x2048x2048x2627): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2627): 85.971

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1904.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2628x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2628x2048): 93.502
Elapsed time for attention_prob_times_values (32x2048x2048x2628): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2628): 85.223

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1919.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2629x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2629x2048): 84.239
Elapsed time for attention_prob_times_values (32x2048x2048x2629): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2629): 85.964

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1832.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1525x2048): 80.240
Elapsed time for attention_prob_times_values (80x2048x2048x1525): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1525): 72.537

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2345.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1526x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1526x2048): 79.789
Elapsed time for attention_prob_times_values (80x2048x2048x1526): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1526): 75.069

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2382.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1527x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1527x2048): 84.806
Elapsed time for attention_prob_times_values (80x2048x2048x1527): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1527): 73.809

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2432.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1528x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1528x2048): 82.680
Elapsed time for attention_prob_times_values (80x2048x2048x1528): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1528): 80.391

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2514.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1529x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1529x2048): 79.297
Elapsed time for attention_prob_times_values (80x2048x2048x1529): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1529): 69.336

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2283.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1530x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1530x2048): 85.419
Elapsed time for attention_prob_times_values (80x2048x2048x1530): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1530): 75.433

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2474.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1531x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1531x2048): 79.515
Elapsed time for attention_prob_times_values (80x2048x2048x1531): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1531): 73.511

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2360.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1532x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1532x2048): 81.454
Elapsed time for attention_prob_times_values (80x2048x2048x1532): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1532): 74.572

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2407.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1533x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1533x2048): 84.385
Elapsed time for attention_prob_times_values (80x2048x2048x1533): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1533): 71.628

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2397.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1534x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1534x2048): 80.687
Elapsed time for attention_prob_times_values (80x2048x2048x1534): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1534): 77.604

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2449.501
MLP duration (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2630x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2630x2048): 92.382
Elapsed time for attention_prob_times_values (32x2048x2048x2630): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2630): 88.569

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1948.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2631x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2631x2048): 90.539
Elapsed time for attention_prob_times_values (32x2048x2048x2631): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2631): 85.161

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1891.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2632x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2632x2048): 93.938
Elapsed time for attention_prob_times_values (32x2048x2048x2632): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2632): 74.183

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1787.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2633x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2633x2048): 90.588
Elapsed time for attention_prob_times_values (32x2048x2048x2633): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2633): 86.262

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1906.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2634x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2634x2048): 91.317
Elapsed time for attention_prob_times_values (32x2048x2048x2634): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2634): 88.542

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1940.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2635x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2635x2048): 89.508
Elapsed time for attention_prob_times_values (32x2048x2048x2635): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2635): 86.200

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1895.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2636x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2636x2048): 92.656
Elapsed time for attention_prob_times_values (32x2048x2048x2636): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2636): 84.586

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1909.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2637x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2637x2048): 90.430
Elapsed time for attention_prob_times_values (32x2048x2048x2637): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2637): 83.359

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1873.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2638x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2638x2048): 91.983
Elapsed time for attention_prob_times_values (32x2048x2048x2638): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2638): 88.629

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1950.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2639x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2639x2048): 90.185
Elapsed time for attention_prob_times_values (32x2048x2048x2639): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2639): 86.411

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1907.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2640x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2640x2048): 94.427
Elapsed time for attention_prob_times_values (32x2048x2048x2640): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2640): 76.094

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1822.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2641x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2641x2048): 89.896
Elapsed time for attention_prob_times_values (32x2048x2048x2641): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2641): 86.562

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1907.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2642x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2642x2048): 91.504
Elapsed time for attention_prob_times_values (32x2048x2048x2642): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2642): 88.718

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1949.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2643x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2643x2048): 89.915
Elapsed time for attention_prob_times_values (32x2048x2048x2643): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2643): 86.484

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1908.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2644x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2644x2048): 92.047
Elapsed time for attention_prob_times_values (32x2048x2048x2644): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2644): 88.820

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1957.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2645x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2645x2048): 89.656
Elapsed time for attention_prob_times_values (32x2048x2048x2645): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2645): 86.114

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1903.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2646x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2646x2048): 91.626
Elapsed time for attention_prob_times_values (32x2048x2048x2646): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2646): 88.657

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1953.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2647x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2647x2048): 89.954
Elapsed time for attention_prob_times_values (32x2048x2048x2647): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2647): 86.510

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1912.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2648x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2648x2048): 92.791
Elapsed time for attention_prob_times_values (32x2048x2048x2648): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2648): 70.594

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1739.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1535x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1535x2048): 81.517
Elapsed time for attention_prob_times_values (80x2048x2048x1535): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1535): 73.706

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2398.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1536x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1536x2048): 93.389
Elapsed time for attention_prob_times_values (80x2048x2048x1536): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1536): 91.308

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2862.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1537x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1537x2048): 85.163
Elapsed time for attention_prob_times_values (80x2048x2048x1537): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1537): 69.572

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2375.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1538x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1538x2048): 85.011
Elapsed time for attention_prob_times_values (80x2048x2048x1538): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1538): 76.179

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2494.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1539x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1539x2048): 83.355
Elapsed time for attention_prob_times_values (80x2048x2048x1539): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1539): 69.883

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2361.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1540x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1540x2048): 87.083
Elapsed time for attention_prob_times_values (80x2048x2048x1540): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1540): 76.327

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2528.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1541x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1541x2048): 85.265
Elapsed time for attention_prob_times_values (80x2048x2048x1541): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1541): 70.707

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2404.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1542x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1542x2048): 86.487
Elapsed time for attention_prob_times_values (80x2048x2048x1542): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1542): 76.197

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2521.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1543x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1543x2048): 85.420
Elapsed time for attention_prob_times_values (80x2048x2048x1543): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1543): 70.799

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2410.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2649x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2649x2048): 89.476
Elapsed time for attention_prob_times_values (32x2048x2048x2649): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2649): 86.633

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1909.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2650x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2650x2048): 91.179
Elapsed time for attention_prob_times_values (32x2048x2048x2650): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2650): 88.950

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1954.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2651x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2651x2048): 89.364
Elapsed time for attention_prob_times_values (32x2048x2048x2651): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2651): 86.342

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1906.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2652x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2652x2048): 91.769
Elapsed time for attention_prob_times_values (32x2048x2048x2652): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2652): 88.831

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1960.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2653x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2653x2048): 89.458
Elapsed time for attention_prob_times_values (32x2048x2048x2653): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2653): 86.678

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1912.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2654x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2654x2048): 91.124
Elapsed time for attention_prob_times_values (32x2048x2048x2654): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2654): 88.735

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1954.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2655x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2655x2048): 89.638
Elapsed time for attention_prob_times_values (32x2048x2048x2655): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2655): 86.824

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1917.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2656x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2656x2048): 101.010
Elapsed time for attention_prob_times_values (32x2048x2048x2656): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2656): 89.151

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 2059.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2657x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2657x2048): 91.188
Elapsed time for attention_prob_times_values (32x2048x2048x2657): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2657): 86.862

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1935.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2658x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2658x2048): 92.792
Elapsed time for attention_prob_times_values (32x2048x2048x2658): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2658): 88.979

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1977.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2659x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2659x2048): 90.949
Elapsed time for attention_prob_times_values (32x2048x2048x2659): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2659): 84.218

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1904.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2660x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2660x2048): 93.279
Elapsed time for attention_prob_times_values (32x2048x2048x2660): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2660): 88.983

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1983.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2661x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2661x2048): 90.833
Elapsed time for attention_prob_times_values (32x2048x2048x2661): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2661): 86.628

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1932.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2662x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2662x2048): 92.319
Elapsed time for attention_prob_times_values (32x2048x2048x2662): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2662): 88.926

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1974.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2663x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2663x2048): 90.673
Elapsed time for attention_prob_times_values (32x2048x2048x2663): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2663): 86.663

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1932.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2664x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2664x2048): 93.819
Elapsed time for attention_prob_times_values (32x2048x2048x2664): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2664): 70.646

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1758.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2665x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2665x2048): 90.320
Elapsed time for attention_prob_times_values (32x2048x2048x2665): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2665): 86.480

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1928.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2666x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2666x2048): 92.024
Elapsed time for attention_prob_times_values (32x2048x2048x2666): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2666): 89.218

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1977.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2667x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2667x2048): 90.295
Elapsed time for attention_prob_times_values (32x2048x2048x2667): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2667): 86.908

Attention duration (in seconds): 0.0162
num_attention_heads: 20, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1544x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1544x2048): 87.130
Elapsed time for attention_prob_times_values (80x2048x2048x1544): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1544): 81.569

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2625.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1545x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1545x2048): 85.000
Elapsed time for attention_prob_times_values (80x2048x2048x1545): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1545): 71.075

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2413.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1546x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1546x2048): 84.630
Elapsed time for attention_prob_times_values (80x2048x2048x1546): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1546): 74.853

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2478.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1547x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1547x2048): 84.978
Elapsed time for attention_prob_times_values (80x2048x2048x1547): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1547): 70.029

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2396.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1548x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1548x2048): 85.960
Elapsed time for attention_prob_times_values (80x2048x2048x1548): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1548): 76.716

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2532.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1549x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1549x2048): 82.996
Elapsed time for attention_prob_times_values (80x2048x2048x1549): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1549): 69.895

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2371.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1550x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1550x2048): 86.349
Elapsed time for attention_prob_times_values (80x2048x2048x1550): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1550): 75.103

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2512.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1551x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1551x2048): 84.519
Elapsed time for attention_prob_times_values (80x2048x2048x1551): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1551): 71.776

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2429.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1552x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1552x2048): 85.182
Elapsed time for attention_prob_times_values (80x2048x2048x1552): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1552): 79.778

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2579.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1553x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1553x2048): 82.353
Attention throughput (in TFLOP/s): 1934.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2668x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2668x2048): 92.314
Elapsed time for attention_prob_times_values (32x2048x2048x2668): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2668): 89.391

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1984.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2669x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2669x2048): 90.352
Elapsed time for attention_prob_times_values (32x2048x2048x2669): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2669): 86.713

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1933.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2670x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2670x2048): 92.264
Elapsed time for attention_prob_times_values (32x2048x2048x2670): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2670): 89.293

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1983.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2671x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2671x2048): 90.379
Elapsed time for attention_prob_times_values (32x2048x2048x2671): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2671): 86.775

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1936.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2672x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2672x2048): 94.598
Elapsed time for attention_prob_times_values (32x2048x2048x2672): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2672): 75.668

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1839.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2673x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2673x2048): 89.890
Elapsed time for attention_prob_times_values (32x2048x2048x2673): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2673): 86.747

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1932.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2674x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2674x2048): 91.688
Elapsed time for attention_prob_times_values (32x2048x2048x2674): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2674): 89.117

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1978.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2675x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2675x2048): 90.168
Elapsed time for attention_prob_times_values (32x2048x2048x2675): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2675): 86.840

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1937.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2676x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2676x2048): 92.237
Elapsed time for attention_prob_times_values (32x2048x2048x2676): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2676): 89.521

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1990.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (80x2048x2048x1553): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1553): 70.230

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2375.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1554x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1554x2048): 85.294
Elapsed time for attention_prob_times_values (80x2048x2048x1554): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1554): 76.829

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2534.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1555x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1555x2048): 83.035
Elapsed time for attention_prob_times_values (80x2048x2048x1555): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1555): 69.469

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2373.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1556x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1556x2048): 86.541
Elapsed time for attention_prob_times_values (80x2048x2048x1556): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1556): 75.436

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2530.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1557x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1557x2048): 83.798
Elapsed time for attention_prob_times_values (80x2048x2048x1557): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1557): 71.935

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2431.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1558x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1558x2048): 83.428
Elapsed time for attention_prob_times_values (80x2048x2048x1558): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1558): 75.580

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2492.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1559x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1559x2048): 85.151
Elapsed time for attention_prob_times_values (80x2048x2048x1559): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1559): 70.913

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2433.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1560x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1560x2048): 86.049
Elapsed time for attention_prob_times_values (80x2048x2048x1560): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1560): 82.522

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2651.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1561x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1561x2048): 81.835
Elapsed time for attention_prob_times_values (80x2048x2048x1561): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1561): 72.236

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2416.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1562x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1562x2048): 85.835
Elapsed time for attention_prob_times_values (80x2048x2048x1562): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1562): 77.062

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2558.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
num_attention_heads: 8, hidden_size: 21416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2677x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2677x2048): 89.912
Elapsed time for attention_prob_times_values (32x2048x2048x2677): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2677): 86.802

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1935.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2678x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2678x2048): 91.710
Elapsed time for attention_prob_times_values (32x2048x2048x2678): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2678): 89.457

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1985.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2679x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2679x2048): 90.195
Elapsed time for attention_prob_times_values (32x2048x2048x2679): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2679): 86.407

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1935.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2680x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2680x2048): 93.018
Elapsed time for attention_prob_times_values (32x2048x2048x2680): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2680): 70.462

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1759.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2681x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2681x2048): 89.523
Elapsed time for attention_prob_times_values (32x2048x2048x2681): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2681): 87.072

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1937.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2682x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2682x2048): 91.206
Elapsed time for attention_prob_times_values (32x2048x2048x2682): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2682): 89.610

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1984.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2683x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2683x2048): 87.597
Elapsed time for attention_prob_times_values (32x2048x2048x2683): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2683): 86.293

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1909.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2684x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2684x2048): 91.217
Elapsed time for attention_prob_times_values (32x2048x2048x2684): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2684): 89.477

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1984.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2685x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2685x2048): 89.146
Elapsed time for attention_prob_times_values (32x2048x2048x2685): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2685): 85.307

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1916.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2686x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2686x2048): 88.745
Elapsed time for attention_prob_times_values (32x2048x2048x2686): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2686): 89.261

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1956.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2687x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2687x2048): 88.890
Elapsed time for attention_prob_times_values (32x2048x2048x2687): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2687): 85.997

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1922.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2688x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2688x2048): 99.116
Elapsed time for attention_prob_times_values (32x2048x2048x2688): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2688): 91.091

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 2088.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2689x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2689x2048): 90.737
Elapsed time for attention_prob_times_values (32x2048x2048x2689): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2689): 84.042

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1920.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2690x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2690x2048): 92.422
Elapsed time for attention_prob_times_values (32x2048x2048x2690): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2690): 86.771

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1970.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2691x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2691x2048): 89.214
Elapsed time for attention_prob_times_values (32x2048x2048x2691): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2691): 84.504

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1911.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2692x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2692x2048): 90.836
Elapsed time for attention_prob_times_values (32x2048x2048x2692): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2692): 86.709

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1954.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2693x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2693x2048): 91.299
Elapsed time for attention_prob_times_values (32x2048x2048x2693): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2693): 84.503

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1934.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2694x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2694x2048): 92.885
Elapsed time for attention_prob_times_values (32x2048x2048x2694): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2694): 85.386

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1961.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2695x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2695x2048): 88.889
Elapsed time for attention_prob_times_values (32x2048x2048x2695): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2695): 84.585

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1911.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1563x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1563x2048): 84.856
Elapsed time for attention_prob_times_values (80x2048x2048x1563): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1563): 72.575

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2466.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1564x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1564x2048): 86.438
Elapsed time for attention_prob_times_values (80x2048x2048x1564): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1564): 77.417

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2576.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1565x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1565x2048): 85.004
Elapsed time for attention_prob_times_values (80x2048x2048x1565): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1565): 72.531

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2470.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1566x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1566x2048): 86.132
Elapsed time for attention_prob_times_values (80x2048x2048x1566): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1566): 77.321

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2573.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1567x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1567x2048): 84.900
Elapsed time for attention_prob_times_values (80x2048x2048x1567): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1567): 72.849

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2478.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1568x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1568x2048): 96.479
Elapsed time for attention_prob_times_values (80x2048x2048x1568): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1568): 84.685

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2852.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1569x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1569x2048): 86.850
Elapsed time for attention_prob_times_values (80x2048x2048x1569): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1569): 72.948

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2509.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1570x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1570x2048): 87.758
Elapsed time for attention_prob_times_values (80x2048x2048x1570): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1570): 77.450

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2605.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1571x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1571x2048): 86.541
Elapsed time for attention_prob_times_values (80x2048x2048x1571): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1571): 72.821

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2505.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2696x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2696x2048): 92.072
Elapsed time for attention_prob_times_values (32x2048x2048x2696): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2696): 65.219

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1684.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2697x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2697x2048): 90.635
Elapsed time for attention_prob_times_values (32x2048x2048x2697): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2697): 82.923

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1911.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2698x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2698x2048): 92.249
Elapsed time for attention_prob_times_values (32x2048x2048x2698): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2698): 85.513

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1959.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2699x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2699x2048): 90.629
Elapsed time for attention_prob_times_values (32x2048x2048x2699): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2699): 84.734

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1934.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2700x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2700x2048): 92.788
Elapsed time for attention_prob_times_values (32x2048x2048x2700): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2700): 86.977

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1983.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2701x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2701x2048): 90.688
Elapsed time for attention_prob_times_values (32x2048x2048x2701): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2701): 81.214

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1893.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2702x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2702x2048): 92.198
Elapsed time for attention_prob_times_values (32x2048x2048x2702): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2702): 84.248

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1946.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2703x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2703x2048): 90.424
Elapsed time for attention_prob_times_values (32x2048x2048x2703): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2703): 84.899

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1936.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2704x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2704x2048): 93.128
Elapsed time for attention_prob_times_values (32x2048x2048x2704): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2704): 70.265

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1772.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
--------
Elapsed time for attention_key_query_prob (80x2048x1572x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1572x2048): 87.994
Elapsed time for attention_prob_times_values (80x2048x2048x1572): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1572): 77.786

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2617.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1573x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1573x2048): 86.372
Elapsed time for attention_prob_times_values (80x2048x2048x1573): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1573): 71.773

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2487.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1574x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1574x2048): 84.302
Elapsed time for attention_prob_times_values (80x2048x2048x1574): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1574): 76.921

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2553.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1575x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1575x2048): 86.062
Elapsed time for attention_prob_times_values (80x2048x2048x1575): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1575): 72.599

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2501.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1576x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1576x2048): 87.968
Elapsed time for attention_prob_times_values (80x2048x2048x1576): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1576): 80.334

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2668.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1577x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1577x2048): 85.613
Elapsed time for attention_prob_times_values (80x2048x2048x1577): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1577): 72.510

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2496.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1578x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1578x2048): 86.515
Elapsed time for attention_prob_times_values (80x2048x2048x1578): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1578): 77.722

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2605.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1579x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1579x2048): 85.441
Elapsed time for attention_prob_times_values (80x2048x2048x1579): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1579): 72.582

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2499.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1580x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1580x2048): 87.233
Elapsed time for attention_prob_times_values (80x2048x2048x1580): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1580): 78.093

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2625.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1581x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1581x2048): 85.652
Elapsed time for attention_prob_times_values (80x2048x2048x1581): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1581): 72.839

Attention duration (in seconds): 0.0270
Elapsed time for attention_key_query_prob (32x2048x2705x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2705x2048): 87.076
Elapsed time for attention_prob_times_values (32x2048x2048x2705): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2705): 85.036

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1904.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2706x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2706x2048): 92.087
Elapsed time for attention_prob_times_values (32x2048x2048x2706): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2706): 84.508

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1951.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2707x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2707x2048): 90.641
Elapsed time for attention_prob_times_values (32x2048x2048x2707): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2707): 84.992

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1942.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2708x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2708x2048): 88.629
Elapsed time for attention_prob_times_values (32x2048x2048x2708): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2708): 87.117

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1946.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2709x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2709x2048): 86.389
Elapsed time for attention_prob_times_values (32x2048x2048x2709): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2709): 84.908

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1898.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2710x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2710x2048): 92.122
Elapsed time for attention_prob_times_values (32x2048x2048x2710): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2710): 87.000

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1984.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2711x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2711x2048): 91.119
Elapsed time for attention_prob_times_values (32x2048x2048x2711): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2711): 82.056

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1915.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2712x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2712x2048): 90.814
Elapsed time for attention_prob_times_values (32x2048x2048x2712): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2712): 64.612

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1675.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2713x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2713x2048): 89.205
Elapsed time for attention_prob_times_values (32x2048x2048x2713): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2713): 84.584

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1927.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2714x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2714x2048): 92.552
Elapsed time for attention_prob_times_values (32x2048x2048x2714): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2714): 84.631

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1963.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2715x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2715x2048): 91.932
Elapsed time for attention_prob_times_values (32x2048x2048x2715): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2715): 81.683

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1921.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2716x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2716x2048): 91.208
Elapsed time for attention_prob_times_values (32x2048x2048x2716): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2716): 87.223

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1981.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2717x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2717x2048): 92.252
Elapsed time for attention_prob_times_values (32x2048x2048x2717): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2717): 84.668

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1962.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2718x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2718x2048): 93.624
Elapsed time for attention_prob_times_values (32x2048x2048x2718): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2718): 84.996

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1981.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2719x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2719x2048): 92.412
Elapsed time for attention_prob_times_values (32x2048x2048x2719): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2719): 82.703

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1941.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2720x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2720x2048): 101.261
Elapsed time for attention_prob_times_values (32x2048x2048x2720): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2720): 74.886

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1915.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2721x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2721x2048): 90.315
Elapsed time for attention_prob_times_values (32x2048x2048x2721): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2721): 84.578

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1944.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2722x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2722x2048): 91.249
Elapsed time for attention_prob_times_values (32x2048x2048x2722): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2722): 87.095

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1984.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2723x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2723x2048): 92.506
Elapsed time for attention_prob_times_values (32x2048x2048x2723): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2723): 84.724

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1969.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 2509.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1582x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1582x2048): 86.836
Elapsed time for attention_prob_times_values (80x2048x2048x1582): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1582): 77.800

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2617.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1583x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1583x2048): 85.858
Elapsed time for attention_prob_times_values (80x2048x2048x1583): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1583): 72.872

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2516.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1584x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1584x2048): 88.048
Elapsed time for attention_prob_times_values (80x2048x2048x1584): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1584): 84.355

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2751.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1585x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1585x2048): 85.377
Elapsed time for attention_prob_times_values (80x2048x2048x1585): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1585): 72.738

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2510.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1586x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1586x2048): 86.601
Elapsed time for attention_prob_times_values (80x2048x2048x1586): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1586): 78.123

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2626.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1587x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1587x2048): 85.322
Elapsed time for attention_prob_times_values (80x2048x2048x1587): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1587): 72.839

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2514.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1588x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1588x2048): 87.029
Elapsed time for attention_prob_times_values (80x2048x2048x1588): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1588): 78.403

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2641.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1589x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1589x2048): 85.675
Elapsed time for attention_prob_times_values (80x2048x2048x1589): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1589): 71.646

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2499.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1590x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1590x2048): 85.974
Elapsed time for attention_prob_times_values (80x2048x2048x1590): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1590): 76.184

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2589.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2724x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2724x2048): 95.144
Elapsed time for attention_prob_times_values (32x2048x2048x2724): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2724): 85.350

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2004.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2725x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2725x2048): 89.438
Elapsed time for attention_prob_times_values (32x2048x2048x2725): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2725): 84.814

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1940.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2726x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2726x2048): 91.253
Elapsed time for attention_prob_times_values (32x2048x2048x2726): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2726): 87.280

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1989.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2727x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2727x2048): 92.338
Elapsed time for attention_prob_times_values (32x2048x2048x2727): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2727): 82.968

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1949.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2728x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2728x2048): 94.991
Elapsed time for attention_prob_times_values (32x2048x2048x2728): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2728): 63.441

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1697.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2729x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2729x2048): 89.582
Elapsed time for attention_prob_times_values (32x2048x2048x2729): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2729): 84.759

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1944.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2730x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2730x2048): 92.626
Elapsed time for attention_prob_times_values (32x2048x2048x2730): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2730): 87.490

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2009.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2731x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2731x2048): 90.923
Elapsed time for attention_prob_times_values (32x2048x2048x2731): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2731): 82.325

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1930.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2732x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2732x2048): 92.898
Elapsed time for attention_prob_times_values (32x2048x2048x2732): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2732): 85.710

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1992.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2733x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2733x2048): 91.176
Elapsed time for attention_prob_times_values (32x2048x2048x2733): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2733): 85.141

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1968.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2734x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2734x2048): 92.054
Elapsed time for attention_prob_times_values (32x2048x2048x2734): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2734): 87.633

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2007.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2735x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2735x2048): 89.117
Elapsed time for attention_prob_times_values (32x2048x2048x2735): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2735): 84.805

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1943.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2736x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2736x2048): 95.373
Elapsed time for attention_prob_times_values (32x2048x2048x2736): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2736): 69.721

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1802.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2737x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2737x2048): 92.048
Elapsed time for attention_prob_times_values (32x2048x2048x2737): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2737): 82.992

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1953.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2738x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2738x2048): 91.657
Elapsed time for attention_prob_times_values (32x2048x2048x2738): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2738): 87.609

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 2005.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2739x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2739x2048): 89.675
Elapsed time for attention_prob_times_values (32x2048x2048x2739): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2739): 85.008

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1954.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2740x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2740x2048): 94.376
Elapsed time for attention_prob_times_values (32x2048x2048x2740): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2740): 85.291

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 2007.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2741x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2741x2048): 91.974
Elapsed time for attention_prob_times_values (32x2048x2048x2741): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2741): 82.525

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1949.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2742x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2742x2048): 93.329
Elapsed time for attention_prob_times_values (32x2048x2048x2742): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2742): 87.546

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2025.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
========================================================================================================================
num_attention_heads: 20, hidden_size: 31820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1591x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1591x2048): 85.525
Elapsed time for attention_prob_times_values (80x2048x2048x1591): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1591): 72.287

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2513.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1592x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1592x2048): 85.414
Elapsed time for attention_prob_times_values (80x2048x2048x1592): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1592): 84.044

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2719.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1593x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1593x2048): 83.731
Elapsed time for attention_prob_times_values (80x2048x2048x1593): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1593): 71.558

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2478.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1594x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1594x2048): 84.940
Elapsed time for attention_prob_times_values (80x2048x2048x1594): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1594): 76.185

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2581.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1595x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1595x2048): 85.375
Elapsed time for attention_prob_times_values (80x2048x2048x1595): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1595): 71.648

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2505.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1596x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1596x2048): 85.479
Elapsed time for attention_prob_times_values (80x2048x2048x1596): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1596): 78.690

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2636.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1597x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1597x2048): 84.968
Elapsed time for attention_prob_times_values (80x2048x2048x1597): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1597): 72.471

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2518.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1598x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1598x2048): 86.285
Elapsed time for attention_prob_times_values (80x2048x2048x1598): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1598): 76.569

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2613.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1599x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1599x2048): 85.418
Elapsed time for attention_prob_times_values (80x2048x2048x1599): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1599): 72.230

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2522.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1600x2048): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2743x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2743x2048): 90.039
Elapsed time for attention_prob_times_values (32x2048x2048x2743): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2743): 84.814

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1959.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2744x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2744x2048): 93.228
Elapsed time for attention_prob_times_values (32x2048x2048x2744): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2744): 64.804

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1715.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2745x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2745x2048): 92.295
Elapsed time for attention_prob_times_values (32x2048x2048x2745): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2745): 82.698

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1957.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2746x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2746x2048): 93.071
Elapsed time for attention_prob_times_values (32x2048x2048x2746): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2746): 87.776

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2028.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2747x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2747x2048): 89.650
Elapsed time for attention_prob_times_values (32x2048x2048x2747): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2747): 85.266

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1963.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2748x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2748x2048): 91.787
Elapsed time for attention_prob_times_values (32x2048x2048x2748): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2748): 87.909

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 2017.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 21992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2749x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2749x2048): 91.425
Elapsed time for attention_prob_times_values (32x2048x2048x2749): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2749): 85.173

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1982.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2750x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2750x2048): 93.058
Elapsed time for attention_prob_times_values (32x2048x2048x2750): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2750): 86.121

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2011.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2751x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2751x2048): 90.118
Elapsed time for attention_prob_times_values (32x2048x2048x2751): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2751): 85.581

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1974.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1600x2048): 94.518
Elapsed time for attention_prob_times_values (80x2048x2048x1600): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1600): 87.675

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2933.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1601x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1601x2048): 86.807
Elapsed time for attention_prob_times_values (80x2048x2048x1601): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1601): 71.151

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2523.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1602x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1602x2048): 86.793
Elapsed time for attention_prob_times_values (80x2048x2048x1602): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1602): 77.876

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2650.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1603x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1603x2048): 86.919
Elapsed time for attention_prob_times_values (80x2048x2048x1603): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1603): 72.693

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2557.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1604x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1604x2048): 86.975
Elapsed time for attention_prob_times_values (80x2048x2048x1604): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1604): 78.910

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2675.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1605x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1605x2048): 86.210
Elapsed time for attention_prob_times_values (80x2048x2048x1605): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1605): 72.752

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2552.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1606x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1606x2048): 87.700
Elapsed time for attention_prob_times_values (80x2048x2048x1606): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1606): 78.054

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2673.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1607x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1607x2048): 86.253
Elapsed time for attention_prob_times_values (80x2048x2048x1607): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1607): 72.136

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2544.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1608x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1608x2048): 87.568
Elapsed time for attention_prob_times_values (80x2048x2048x1608): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1608): 84.706

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2790.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1609x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1609x2048): 85.199
Elapsed time for attention_prob_times_values (80x2048x2048x1609): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1609): 73.299

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2555.230
MLP duration (in seconds): 0.0000
Elapsed time for attention_key_query_prob (32x2048x2752x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2752x2048): 100.872
Elapsed time for attention_prob_times_values (32x2048x2048x2752): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2752): 85.966

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 2088.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2753x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2753x2048): 91.807
Elapsed time for attention_prob_times_values (32x2048x2048x2753): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2753): 83.350

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1966.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2754x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2754x2048): 92.981
Elapsed time for attention_prob_times_values (32x2048x2048x2754): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2754): 86.055

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2012.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2755x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2755x2048): 91.589
Elapsed time for attention_prob_times_values (32x2048x2048x2755): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2755): 85.751

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1994.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2756x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2756x2048): 91.369
Elapsed time for attention_prob_times_values (32x2048x2048x2756): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2756): 88.288

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2023.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2757x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2757x2048): 90.084
Elapsed time for attention_prob_times_values (32x2048x2048x2757): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2757): 85.867

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1981.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2758x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2758x2048): 92.768
Elapsed time for attention_prob_times_values (32x2048x2048x2758): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2758): 88.285

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 2039.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2759x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2759x2048): 90.880
Elapsed time for attention_prob_times_values (32x2048x2048x2759): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2759): 85.678

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1989.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2760x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2760x2048): 91.113
Elapsed time for attention_prob_times_values (32x2048x2048x2760): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2760): 67.583

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1750.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2761x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2761x2048): 87.537
Elapsed time for attention_prob_times_values (32x2048x2048x2761): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2761): 85.852

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1956.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2762x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2762x2048): 92.214
Elapsed time for attention_prob_times_values (32x2048x2048x2762): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2762): 88.352

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 2037.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2763x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2763x2048): 90.582
Elapsed time for attention_prob_times_values (32x2048x2048x2763): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2763): 81.599

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1939.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2764x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2764x2048): 88.026
Elapsed time for attention_prob_times_values (32x2048x2048x2764): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2764): 88.514

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1994.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2765x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2765x2048): 86.668
Elapsed time for attention_prob_times_values (32x2048x2048x2765): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2765): 86.107

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1952.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2766x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2766x2048): 92.312
Elapsed time for attention_prob_times_values (32x2048x2048x2766): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2766): 84.523

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1995.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2767x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2767x2048): 90.474
Elapsed time for attention_prob_times_values (32x2048x2048x2767): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2767): 83.430

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1963.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2768x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2768x2048): 94.729
Elapsed time for attention_prob_times_values (32x2048x2048x2768): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2768): 70.894

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1834.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2769x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2769x2048): 90.269
Elapsed time for attention_prob_times_values (32x2048x2048x2769): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2769): 86.557

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2000.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2770x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2770x2048): 92.153
Elapsed time for attention_prob_times_values (32x2048x2048x2770): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2770): 84.912

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2001.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1610x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1610x2048): 86.870
Elapsed time for attention_prob_times_values (80x2048x2048x1610): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1610): 78.825

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2681.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1611x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1611x2048): 85.767
Elapsed time for attention_prob_times_values (80x2048x2048x1611): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1611): 73.464

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2569.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1612x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1612x2048): 85.295
Elapsed time for attention_prob_times_values (80x2048x2048x1612): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1612): 79.091

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2666.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1613x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1613x2048): 84.488
Elapsed time for attention_prob_times_values (80x2048x2048x1613): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1613): 70.959

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2507.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1614x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1614x2048): 83.006
Elapsed time for attention_prob_times_values (80x2048x2048x1614): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1614): 75.004

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2562.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1615x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1615x2048): 85.767
Elapsed time for attention_prob_times_values (80x2048x2048x1615): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1615): 72.561

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2558.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1616x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1616x2048): 86.783
Elapsed time for attention_prob_times_values (80x2048x2048x1616): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1616): 85.739

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2808.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1617x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1617x2048): 82.995
Elapsed time for attention_prob_times_values (80x2048x2048x1617): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1617): 71.300

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2499.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1618x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1618x2048): 84.105
Elapsed time for attention_prob_times_values (80x2048x2048x1618): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1618): 75.528

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2594.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2771x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2771x2048): 90.544
Elapsed time for attention_prob_times_values (32x2048x2048x2771): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2771): 83.006

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1961.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2772x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2772x2048): 92.405
Elapsed time for attention_prob_times_values (32x2048x2048x2772): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2772): 88.811

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 2052.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2773x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2773x2048): 87.101
Elapsed time for attention_prob_times_values (32x2048x2048x2773): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2773): 86.366

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1965.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2774x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2774x2048): 87.658
Elapsed time for attention_prob_times_values (32x2048x2048x2774): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2774): 88.535

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1997.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2775x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2775x2048): 90.198
Elapsed time for attention_prob_times_values (32x2048x2048x2775): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2775): 86.383

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2001.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2776x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2776x2048): 93.126
Elapsed time for attention_prob_times_values (32x2048x2048x2776): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2776): 65.901

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1751.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2777x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2777x2048): 84.997
Elapsed time for attention_prob_times_values (32x2048x2048x2777): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2777): 86.464

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1945.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2778x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2778x2048): 88.462
Elapsed time for attention_prob_times_values (32x2048x2048x2778): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2778): 88.791

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2012.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2779x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2779x2048): 89.763
Elapsed time for attention_prob_times_values (32x2048x2048x2779): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2779): 84.295

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1974.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2780x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2780x2048): 92.089
Elapsed time for attention_prob_times_values (32x2048x2048x2780): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2780): 86.081

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2021.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2781x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2781x2048): 89.967
Elapsed time for attention_prob_times_values (32x2048x2048x2781): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2781): 86.706

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2006.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2782x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2782x2048): 91.519
Elapsed time for attention_prob_times_values (32x2048x2048x2782): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2782): 88.931

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 2050.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2783x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2783x2048): 87.964
Elapsed time for attention_prob_times_values (32x2048x2048x2783): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2783): 84.547

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1960.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2784x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2784x2048): 101.142
Elapsed time for attention_prob_times_values (32x2048x2048x2784): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2784): 80.185

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 2035.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2785x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2785x2048): 91.717
Elapsed time for attention_prob_times_values (32x2048x2048x2785): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2785): 86.766

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2029.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2786x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2786x2048): 90.590
Elapsed time for attention_prob_times_values (32x2048x2048x2786): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2786): 89.158

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 2045.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2787x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2787x2048): 88.869
Elapsed time for attention_prob_times_values (32x2048x2048x2787): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2787): 86.723

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1999.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2788x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2788x2048): 93.803
Elapsed time for attention_prob_times_values (32x2048x2048x2788): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2788): 87.389

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2061.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2789x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2789x2048): 91.193
Elapsed time for attention_prob_times_values (32x2048x2048x2789): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2789): 84.592

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 2000.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
num_attention_heads: 20, hidden_size: 32380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1619x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1619x2048): 85.415
Elapsed time for attention_prob_times_values (80x2048x2048x1619): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1619): 72.848

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2565.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1620x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1620x2048): 84.335
Elapsed time for attention_prob_times_values (80x2048x2048x1620): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1620): 79.475

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2671.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1621x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1621x2048): 83.072
Elapsed time for attention_prob_times_values (80x2048x2048x1621): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1621): 71.754

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2514.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1622x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1622x2048): 83.954
Elapsed time for attention_prob_times_values (80x2048x2048x1622): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1622): 76.845

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2622.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1623x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1623x2048): 85.699
Elapsed time for attention_prob_times_values (80x2048x2048x1623): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1623): 71.518

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2549.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1624x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1624x2048): 86.514
Elapsed time for attention_prob_times_values (80x2048x2048x1624): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1624): 85.484

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2813.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1625x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1625x2048): 83.772
Elapsed time for attention_prob_times_values (80x2048x2048x1625): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1625): 73.050

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2555.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1626x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1626x2048): 86.299
Elapsed time for attention_prob_times_values (80x2048x2048x1626): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1626): 75.956

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2646.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1627x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1627x2048): 85.600
Elapsed time for attention_prob_times_values (80x2048x2048x1627): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1627): 72.023

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2564.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1628x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1628x2048): 82.872
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2790x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2790x2048): 90.194
Elapsed time for attention_prob_times_values (32x2048x2048x2790): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2790): 89.225

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 2045.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2791x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2791x2048): 91.142
Elapsed time for attention_prob_times_values (32x2048x2048x2791): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2791): 86.794

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2027.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2792x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2792x2048): 94.352
Elapsed time for attention_prob_times_values (32x2048x2048x2792): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2792): 66.582

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1780.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2793x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2793x2048): 90.904
Elapsed time for attention_prob_times_values (32x2048x2048x2793): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2793): 84.789

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 2002.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2794x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2794x2048): 92.563
Elapsed time for attention_prob_times_values (32x2048x2048x2794): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2794): 89.335

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2075.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2795x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2795x2048): 90.796
Elapsed time for attention_prob_times_values (32x2048x2048x2795): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2795): 86.627

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2024.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2796x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2796x2048): 92.972
Elapsed time for attention_prob_times_values (32x2048x2048x2796): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2796): 89.070

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2078.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2797x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2797x2048): 90.934
Elapsed time for attention_prob_times_values (32x2048x2048x2797): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2797): 86.663

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2028.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2798x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2798x2048): 92.340
Elapsed time for attention_prob_times_values (32x2048x2048x2798): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2798): 89.489

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2077.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_prob_times_values (80x2048x2048x1628): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1628): 79.903

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2668.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1629x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1629x2048): 84.086
Elapsed time for attention_prob_times_values (80x2048x2048x1629): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1629): 72.561

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2556.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1630x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1630x2048): 84.937
Elapsed time for attention_prob_times_values (80x2048x2048x1630): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1630): 77.313

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2657.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1631x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1631x2048): 85.566
Elapsed time for attention_prob_times_values (80x2048x2048x1631): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1631): 74.993

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2626.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1632x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1632x2048): 94.877
Elapsed time for attention_prob_times_values (80x2048x2048x1632): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1632): 87.694

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2996.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1633x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1633x2048): 85.446
Elapsed time for attention_prob_times_values (80x2048x2048x1633): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1633): 74.419

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2616.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1634x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1634x2048): 89.239
Elapsed time for attention_prob_times_values (80x2048x2048x1634): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1634): 79.784

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2772.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1635x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1635x2048): 89.494
Elapsed time for attention_prob_times_values (80x2048x2048x1635): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1635): 74.511

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2678.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1636x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1636x2048): 87.415
Elapsed time for attention_prob_times_values (80x2048x2048x1636): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1636): 80.231

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2757.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1637x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1637x2048): 86.221
Elapsed time for attention_prob_times_values (80x2048x2048x1637): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1637): 74.662

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2638.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1638x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1638x2048): 87.444
Elapsed time for attention_prob_times_values (80x2048x2048x1638): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1638): 79.828

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2753.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (32x2048x2799x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2799x2048): 87.700
Elapsed time for attention_prob_times_values (32x2048x2048x2799): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2799): 86.909

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1996.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2800x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2800x2048): 94.798
Elapsed time for attention_prob_times_values (32x2048x2048x2800): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2800): 70.542

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1850.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2801x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2801x2048): 90.386
Elapsed time for attention_prob_times_values (32x2048x2048x2801): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2801): 86.983

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2028.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2802x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2802x2048): 92.253
Elapsed time for attention_prob_times_values (32x2048x2048x2802): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2802): 89.629

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2081.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2803x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2803x2048): 90.143
Elapsed time for attention_prob_times_values (32x2048x2048x2803): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2803): 87.023

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2027.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2804x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2804x2048): 91.787
Elapsed time for attention_prob_times_values (32x2048x2048x2804): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2804): 89.703

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 2078.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2805x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2805x2048): 90.476
Elapsed time for attention_prob_times_values (32x2048x2048x2805): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2805): 84.517

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2002.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2806x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2806x2048): 92.035
Elapsed time for attention_prob_times_values (32x2048x2048x2806): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2806): 86.105

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2039.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2807x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2807x2048): 90.357
Elapsed time for attention_prob_times_values (32x2048x2048x2807): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2807): 87.092

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2033.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2808x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2808x2048): 93.186
Elapsed time for attention_prob_times_values (32x2048x2048x2808): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2808): 66.387

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1778.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2809x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2809x2048): 90.037
Elapsed time for attention_prob_times_values (32x2048x2048x2809): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2809): 87.291

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2033.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2810x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2810x2048): 91.765
Elapsed time for attention_prob_times_values (32x2048x2048x2810): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2810): 89.638

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 2081.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2811x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2811x2048): 89.904
Elapsed time for attention_prob_times_values (32x2048x2048x2811): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2811): 87.346

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2034.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2812x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2812x2048): 91.782
Elapsed time for attention_prob_times_values (32x2048x2048x2812): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2812): 89.547

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 2082.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2813x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2813x2048): 89.775
Elapsed time for attention_prob_times_values (32x2048x2048x2813): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2813): 87.449

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2035.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2814x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2814x2048): 91.421
Elapsed time for attention_prob_times_values (32x2048x2048x2814): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2814): 89.957

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 2084.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2815x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2815x2048): 89.693
Elapsed time for attention_prob_times_values (32x2048x2048x2815): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2815): 87.386

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 2035.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2816x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2816x2048): 99.433
Elapsed time for attention_prob_times_values (32x2048x2048x2816): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2816): 92.909

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 2209.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2817x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2817x2048): 91.352
Elapsed time for attention_prob_times_values (32x2048x2048x2817): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2817): 84.124

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2015.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2818x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2818x2048): 93.044
Elapsed time for attention_prob_times_values (32x2048x2048x2818): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2818): 86.873

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2068.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2819x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2819x2048): 91.454
Elapsed time for attention_prob_times_values (32x2048x2048x2819): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2819): 84.484

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2022.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2820x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2820x2048): 93.164
Elapsed time for attention_prob_times_values (32x2048x2048x2820): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2820): 86.997

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2072.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2821x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2821x2048): 91.314
Elapsed time for attention_prob_times_values (32x2048x2048x2821): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2821): 84.376

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2020.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2822x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2822x2048): 75.149
Elapsed time for attention_prob_times_values (32x2048x2048x2822): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2822): 86.954

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1858.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2823x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2823x2048): 71.618
Elapsed time for attention_prob_times_values (32x2048x2048x2823): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2823): 84.550

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1787.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2824x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2824x2048): 76.994
Elapsed time for attention_prob_times_values (32x2048x2048x2824): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2824): 66.122

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1640.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2825x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2825x2048): 71.094
Elapsed time for attention_prob_times_values (32x2048x2048x2825): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2825): 84.487

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1781.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2826x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2826x2048): 74.788
Elapsed time for attention_prob_times_values (32x2048x2048x2826): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2826): 87.237

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1858.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2827x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2827x2048): 71.723
Elapsed time for attention_prob_times_values (32x2048x2048x2827): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2827): 84.639

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1792.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2828x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2828x2048): 75.276
Elapsed time for attention_prob_times_values (32x2048x2048x2828): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2828): 87.177

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1865.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2829x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2829x2048): 71.593
Elapsed time for attention_prob_times_values (32x2048x2048x2829): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2829): 84.936

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1794.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2830x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2830x2048): 74.871
Elapsed time for attention_prob_times_values (32x2048x2048x2830): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2830): 87.222

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1862.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2831x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2831x2048): 71.069
Elapsed time for attention_prob_times_values (32x2048x2048x2831): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2831): 84.862

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1788.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2832x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2832x2048): 78.168
Elapsed time for attention_prob_times_values (32x2048x2048x2832): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2832): 70.842

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1718.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2833x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2833x2048): 70.921
Elapsed time for attention_prob_times_values (32x2048x2048x2833): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2833): 84.931

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1788.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2834x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2834x2048): 74.665
Elapsed time for attention_prob_times_values (32x2048x2048x2834): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2834): 87.355

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1863.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2835x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2835x2048): 71.371
Elapsed time for attention_prob_times_values (32x2048x2048x2835): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2835): 84.889

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1795.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2836x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2836x2048): 75.131
Elapsed time for attention_prob_times_values (32x2048x2048x2836): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2836): 87.294

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1870.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2837x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2837x2048): 71.235
Elapsed time for attention_prob_times_values (32x2048x2048x2837): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2837): 85.167

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1797.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2838x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2838x2048): 74.597
Elapsed time for attention_prob_times_values (32x2048x2048x2838): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2838): 87.480

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1865.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2839x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2839x2048): 70.707
Elapsed time for attention_prob_times_values (32x2048x2048x2839): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2839): 85.252

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1791.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2840x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2840x2048): 76.427
Elapsed time for attention_prob_times_values (32x2048x2048x2840): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2840): 67.173

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1657.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2841x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2841x2048): 70.882
Elapsed time for attention_prob_times_values (32x2048x2048x2841): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2841): 85.392

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1796.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2842x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2842x2048): 74.535
Elapsed time for attention_prob_times_values (32x2048x2048x2842): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2842): 87.572

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1868.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2843x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2843x2048): 70.614
Elapsed time for attention_prob_times_values (32x2048x2048x2843): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2843): 84.980

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1790.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2844x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2844x2048): 75.111
Elapsed time for attention_prob_times_values (32x2048x2048x2844): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2844): 87.699

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1878.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2845x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2845x2048): 70.553
Elapsed time for attention_prob_times_values (32x2048x2048x2845): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2845): 85.339

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1794.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2846x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2846x2048): 74.489
Elapsed time for attention_prob_times_values (32x2048x2048x2846): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2846): 87.624

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1870.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2847x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2847x2048): 70.250
Elapsed time for attention_prob_times_values (32x2048x2048x2847): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2847): 85.383

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1791.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2848x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2848x2048): 77.637
Elapsed time for attention_prob_times_values (32x2048x2048x2848): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2848): 79.186

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1822.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2849x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2849x2048): 69.933
Elapsed time for attention_prob_times_values (32x2048x2048x2849): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2849): 85.568

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1790.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2850x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2850x2048): 74.184
Elapsed time for attention_prob_times_values (32x2048x2048x2850): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2850): 87.565

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1868.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2851x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2851x2048): 70.380
Elapsed time for attention_prob_times_values (32x2048x2048x2851): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2851): 85.640

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1798.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2852x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2852x2048): 75.213
Elapsed time for attention_prob_times_values (32x2048x2048x2852): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2852): 87.844

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1886.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2853x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2853x2048): 70.565
Elapsed time for attention_prob_times_values (32x2048x2048x2853): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2853): 85.620

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1801.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2854x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2854x2048): 74.063
Elapsed time for attention_prob_times_values (32x2048x2048x2854): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2854): 87.916

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1873.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2855x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2855x2048): 70.756
Elapsed time for attention_prob_times_values (32x2048x2048x2855): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2855): 85.684

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1806.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2856x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2856x2048): 76.249
Elapsed time for attention_prob_times_values (32x2048x2048x2856): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2856): 73.671

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1746.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2857x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2857x2048): 70.546
Elapsed time for attention_prob_times_values (32x2048x2048x2857): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2857): 85.646

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1804.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2858x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2858x2048): 73.874
Elapsed time for attention_prob_times_values (32x2048x2048x2858): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2858): 88.198

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1875.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2859x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2859x2048): 70.810
Elapsed time for attention_prob_times_values (32x2048x2048x2859): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2859): 85.697

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1809.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2860x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2860x2048): 75.027
Elapsed time for attention_prob_times_values (32x2048x2048x2860): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2860): 88.250

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1893.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2861x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2861x2048): 70.760
Elapsed time for attention_prob_times_values (32x2048x2048x2861): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2861): 85.616

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1809.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2862x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2862x2048): 74.598
Elapsed time for attention_prob_times_values (32x2048x2048x2862): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2862): 88.181

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1887.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2863x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2863x2048): 70.533
Elapsed time for attention_prob_times_values (32x2048x2048x2863): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2863): 85.589

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1807.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2864x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2864x2048): 77.149
Elapsed time for attention_prob_times_values (32x2048x2048x2864): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2864): 77.791

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1810.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2865x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2865x2048): 70.002
Elapsed time for attention_prob_times_values (32x2048x2048x2865): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2865): 88.064

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1823.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2866x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2866x2048): 73.091
Elapsed time for attention_prob_times_values (32x2048x2048x2866): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2866): 93.624

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1920.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2867x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2867x2048): 70.268
Elapsed time for attention_prob_times_values (32x2048x2048x2867): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2867): 87.448

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1823.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2868x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2868x2048): 73.865
Elapsed time for attention_prob_times_values (32x2048x2048x2868): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2868): 94.269

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1938.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2869x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2869x2048): 70.111
Elapsed time for attention_prob_times_values (32x2048x2048x2869): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2869): 87.990

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1827.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2870x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2870x2048): 72.950
Elapsed time for attention_prob_times_values (32x2048x2048x2870): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2870): 93.874

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1922.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2871x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2871x2048): 70.103
Elapsed time for attention_prob_times_values (32x2048x2048x2871): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2871): 88.008

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1828.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2872x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2872x2048): 76.011
Elapsed time for attention_prob_times_values (32x2048x2048x2872): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2872): 74.703

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1766.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2873x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2873x2048): 69.670
Elapsed time for attention_prob_times_values (32x2048x2048x2873): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2873): 87.507

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1818.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 22992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2874x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2874x2048): 72.338
Elapsed time for attention_prob_times_values (32x2048x2048x2874): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2874): 93.994

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1917.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2875x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2875x2048): 69.644
Elapsed time for attention_prob_times_values (32x2048x2048x2875): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2875): 87.782

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1822.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2876x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2876x2048): 73.737
Elapsed time for attention_prob_times_values (32x2048x2048x2876): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2876): 94.419

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1943.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2877x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2877x2048): 69.742
Elapsed time for attention_prob_times_values (32x2048x2048x2877): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2877): 87.582

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1822.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2878x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2878x2048): 71.883
Elapsed time for attention_prob_times_values (32x2048x2048x2878): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2878): 94.010

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1913.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2879x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2879x2048): 69.218
Elapsed time for attention_prob_times_values (32x2048x2048x2879): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2879): 88.071

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1820.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2880x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2880x2048): 91.676
Elapsed time for attention_prob_times_values (32x2048x2048x2880): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2880): 90.341

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2138.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2881x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2881x2048): 71.295
Elapsed time for attention_prob_times_values (32x2048x2048x2881): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2881): 86.902

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1841.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2882x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2882x2048): 74.627
Elapsed time for attention_prob_times_values (32x2048x2048x2882): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2882): 94.018

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1956.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2883x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2883x2048): 71.491
Elapsed time for attention_prob_times_values (32x2048x2048x2883): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2883): 87.741

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1853.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
slurmstepd: error: *** JOB 1507175 ON frontier08424 CANCELLED AT 2023-11-22T16:43:13 DUE TO TIME LIMIT ***
