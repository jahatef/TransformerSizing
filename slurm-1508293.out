
Lmod is automatically replacing "PrgEnv-cray/8.3.3" with "PrgEnv-gnu/8.3.3".


Lmod is automatically replacing "cce/15.0.0" with "gcc/12.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.23     2) darshan-runtime/3.4.0

0
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
[2023-11-24 16:45:25,276] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 12, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1760x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1760x2048): 97.544
Elapsed time for attention_prob_times_values (48x2048x2048x1760): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1760): 88.564

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 2007.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1761x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1761x2048): 88.298
Elapsed time for attention_prob_times_values (48x2048x2048x1761): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1761): 84.951

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1873.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1762x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1762x2048): 90.202
Elapsed time for attention_prob_times_values (48x2048x2048x1762): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1762): 87.283

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1920.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1763x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1763x2048): 88.194
Elapsed time for attention_prob_times_values (48x2048x2048x1763): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1763): 84.852

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1873.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1764x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1764x2048): 90.797
Elapsed time for attention_prob_times_values (48x2048x2048x1764): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1764): 87.270

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1928.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1765x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1765x2048): 87.829
Elapsed time for attention_prob_times_values (48x2048x2048x1765): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1765): 84.718

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1870.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1766x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1766x2048): 88.847
Elapsed time for attention_prob_times_values (48x2048x2048x1766): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1766): 87.142

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1908.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1767x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1767x2048): 87.393
Elapsed time for attention_prob_times_values (48x2048x2048x1767): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1767): 84.817

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1868.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1768x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1768x2048): 89.264
Elapsed time for attention_prob_times_values (48x2048x2048x1768): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1768): 78.910

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1819.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1769x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1769x2048): 86.937
Elapsed time for attention_prob_times_values (48x2048x2048x1769): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1769): 84.954

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1867.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1770x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1770x2048): 87.842
Elapsed time for attention_prob_times_values (48x2048x2048x1770): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1770): 87.285

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1903.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1771x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1771x2048): 87.157
Elapsed time for attention_prob_times_values (48x2048x2048x1771): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1771): 84.891

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1871.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1772x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1772x2048): 88.500
Elapsed time for attention_prob_times_values (48x2048x2048x1772): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1772): 87.566

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1916.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1773x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1773x2048): 87.072
Elapsed time for attention_prob_times_values (48x2048x2048x1773): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1773): 84.912

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1872.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1774x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1774x2048): 88.025
Elapsed time for attention_prob_times_values (48x2048x2048x1774): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1774): 87.490

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1912.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1775x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1775x2048): 87.117
Elapsed time for attention_prob_times_values (48x2048x2048x1775): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1775): 85.080

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1876.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1776x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1776x2048): 89.441
Elapsed time for attention_prob_times_values (48x2048x2048x1776): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1776): 87.182

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1925.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1777x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1777x2048): 86.792
Elapsed time for attention_prob_times_values (48x2048x2048x1777): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1777): 85.135

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1875.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1778x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1778x2048): 87.968
Elapsed time for attention_prob_times_values (48x2048x2048x1778): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1778): 87.782

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1918.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1779x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1779x2048): 86.912
Elapsed time for attention_prob_times_values (48x2048x2048x1779): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1779): 85.243

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1880.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1780x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1780x2048): 88.414
Elapsed time for attention_prob_times_values (48x2048x2048x1780): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1780): 87.874

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1926.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1781x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1781x2048): 86.949
Elapsed time for attention_prob_times_values (48x2048x2048x1781): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1781): 85.210

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1882.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1782x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1782x2048): 87.675
Elapsed time for attention_prob_times_values (48x2048x2048x1782): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1782): 87.839

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1920.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1783x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1783x2048): 86.919
Elapsed time for attention_prob_times_values (48x2048x2048x1783): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1783): 85.280

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1884.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1784x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1784x2048): 88.779
Elapsed time for attention_prob_times_values (48x2048x2048x1784): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1784): 83.694

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1887.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1785x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1785x2048): 86.674
Elapsed time for attention_prob_times_values (48x2048x2048x1785): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1785): 85.455

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1886.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1786x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1786x2048): 87.454
Elapsed time for attention_prob_times_values (48x2048x2048x1786): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1786): 87.977

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1923.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1787x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1787x2048): 86.694
Elapsed time for attention_prob_times_values (48x2048x2048x1787): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1787): 85.587

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1889.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1788x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1788x2048): 88.091
Elapsed time for attention_prob_times_values (48x2048x2048x1788): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1788): 88.173

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1934.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1789x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1789x2048): 86.371
Elapsed time for attention_prob_times_values (48x2048x2048x1789): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1789): 85.690

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1889.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1790x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1790x2048): 87.472
Elapsed time for attention_prob_times_values (48x2048x2048x1790): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1790): 88.249

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1930.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1791x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1791x2048): 86.629
Elapsed time for attention_prob_times_values (48x2048x2048x1791): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1791): 85.728

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1894.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1792x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1792x2048): 95.155
Elapsed time for attention_prob_times_values (48x2048x2048x1792): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1792): 91.852

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 2056.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1793x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1793x2048): 87.637
Elapsed time for attention_prob_times_values (48x2048x2048x1793): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1793): 81.482

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1858.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1794x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1794x2048): 88.679
Elapsed time for attention_prob_times_values (48x2048x2048x1794): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1794): 84.070

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1900.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1795x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1795x2048): 87.149
Elapsed time for attention_prob_times_values (48x2048x2048x1795): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1795): 81.687

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1858.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1796x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1796x2048): 89.208
Elapsed time for attention_prob_times_values (48x2048x2048x1796): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1796): 84.180

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1909.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1797x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1797x2048): 87.652
Elapsed time for attention_prob_times_values (48x2048x2048x1797): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1797): 81.785

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1866.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1798x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1798x2048): 88.542
Elapsed time for attention_prob_times_values (48x2048x2048x1798): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1798): 84.201

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1905.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1799x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1799x2048): 87.628
Elapsed time for attention_prob_times_values (48x2048x2048x1799): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1799): 81.929

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1869.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1800x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1800x2048): 89.376
Elapsed time for attention_prob_times_values (48x2048x2048x1800): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1800): 82.879

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1900.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1801x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1801x2048): 87.212
Elapsed time for attention_prob_times_values (48x2048x2048x1801): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1801): 81.995

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1868.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1802x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1802x2048): 88.084
Elapsed time for attention_prob_times_values (48x2048x2048x1802): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1802): 84.335

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1905.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1803x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1803x2048): 87.357
Elapsed time for attention_prob_times_values (48x2048x2048x1803): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1803): 82.133

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1873.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1804x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1804x2048): 88.512
Elapsed time for attention_prob_times_values (48x2048x2048x1804): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1804): 84.529

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1914.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1805x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1805x2048): 87.186
Elapsed time for attention_prob_times_values (48x2048x2048x1805): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1805): 82.197

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1874.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1806x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1806x2048): 88.216
Elapsed time for attention_prob_times_values (48x2048x2048x1806): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1806): 84.473

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1912.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1807x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1807x2048): 87.348
Elapsed time for attention_prob_times_values (48x2048x2048x1807): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1807): 82.344

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1879.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1808x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1808x2048): 89.699
Elapsed time for attention_prob_times_values (48x2048x2048x1808): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1808): 84.138

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1926.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1809x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1809x2048): 87.011
Elapsed time for attention_prob_times_values (48x2048x2048x1809): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1809): 82.440

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1879.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1810x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1810x2048): 87.969
Elapsed time for attention_prob_times_values (48x2048x2048x1810): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1810): 84.641

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1916.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1811x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1811x2048): 87.072
Elapsed time for attention_prob_times_values (48x2048x2048x1811): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1811): 82.464

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1882.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1812x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1812x2048): 88.892
Elapsed time for attention_prob_times_values (48x2048x2048x1812): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1812): 84.770

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1929.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1813x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1813x2048): 87.608
Elapsed time for attention_prob_times_values (48x2048x2048x1813): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1813): 82.531

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1890.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1814x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1814x2048): 88.254
Elapsed time for attention_prob_times_values (48x2048x2048x1814): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1814): 84.732

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1924.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1815x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1815x2048): 87.606
Elapsed time for attention_prob_times_values (48x2048x2048x1815): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1815): 82.631

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1893.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1816x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1816x2048): 89.176
Elapsed time for attention_prob_times_values (48x2048x2048x1816): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1816): 84.047

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1928.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1817x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1817x2048): 86.768
Elapsed time for attention_prob_times_values (48x2048x2048x1817): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1817): 82.659

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1887.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1818x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1818x2048): 87.761
Elapsed time for attention_prob_times_values (48x2048x2048x1818): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1818): 84.938

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1925.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1819x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1819x2048): 86.814
Elapsed time for attention_prob_times_values (48x2048x2048x1819): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1819): 82.708

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1890.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1820x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1820x2048): 88.330
Elapsed time for attention_prob_times_values (48x2048x2048x1820): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1820): 85.045

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1934.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1821x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1821x2048): 86.802
Elapsed time for attention_prob_times_values (48x2048x2048x1821): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1821): 82.690

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1892.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1822x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1822x2048): 87.651
Elapsed time for attention_prob_times_values (48x2048x2048x1822): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1822): 85.054

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1929.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1823x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1823x2048): 87.237
Elapsed time for attention_prob_times_values (48x2048x2048x1823): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1823): 82.937

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1901.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1824x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1824x2048): 97.403
Elapsed time for attention_prob_times_values (48x2048x2048x1824): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1824): 85.895

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 2042.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1825x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1825x2048): 88.453
Elapsed time for attention_prob_times_values (48x2048x2048x1825): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1825): 82.987

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1917.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1826x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1826x2048): 89.493
Elapsed time for attention_prob_times_values (48x2048x2048x1826): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1826): 85.201

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1955.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1827x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1827x2048): 88.431
Elapsed time for attention_prob_times_values (48x2048x2048x1827): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1827): 82.947

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1918.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1828x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1828x2048): 89.810
Elapsed time for attention_prob_times_values (48x2048x2048x1828): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1828): 85.334

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1962.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1829x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1829x2048): 88.254
Elapsed time for attention_prob_times_values (48x2048x2048x1829): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1829): 82.973

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1918.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1830x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1830x2048): 88.655
Elapsed time for attention_prob_times_values (48x2048x2048x1830): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1830): 85.333

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1951.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1831x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1831x2048): 87.782
Elapsed time for attention_prob_times_values (48x2048x2048x1831): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1831): 82.690

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1912.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1832x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1832x2048): 89.613
Elapsed time for attention_prob_times_values (48x2048x2048x1832): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1832): 84.681

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1956.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1833x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1833x2048): 87.209
Elapsed time for attention_prob_times_values (48x2048x2048x1833): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1833): 83.080

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1912.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1834x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1834x2048): 88.116
Elapsed time for attention_prob_times_values (48x2048x2048x1834): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1834): 85.480

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1951.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1835x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1835x2048): 87.403
Elapsed time for attention_prob_times_values (48x2048x2048x1835): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1835): 83.158

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1917.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1836x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1836x2048): 89.020
Elapsed time for attention_prob_times_values (48x2048x2048x1836): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1836): 85.187

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1960.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1837x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1837x2048): 87.520
Elapsed time for attention_prob_times_values (48x2048x2048x1837): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1837): 83.155

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1921.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1838x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1838x2048): 88.406
Elapsed time for attention_prob_times_values (48x2048x2048x1838): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1838): 85.451

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1958.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1839x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1839x2048): 87.700
Elapsed time for attention_prob_times_values (48x2048x2048x1839): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1839): 83.263

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1926.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1840x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1840x2048): 89.887
Elapsed time for attention_prob_times_values (48x2048x2048x1840): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1840): 85.389

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1976.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1841x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1841x2048): 87.136
Elapsed time for attention_prob_times_values (48x2048x2048x1841): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1841): 83.465

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1924.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1842x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1842x2048): 88.049
Elapsed time for attention_prob_times_values (48x2048x2048x1842): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1842): 85.677

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1961.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1843x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1843x2048): 87.113
Elapsed time for attention_prob_times_values (48x2048x2048x1843): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1843): 83.435

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1926.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1844x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1844x2048): 88.831
Elapsed time for attention_prob_times_values (48x2048x2048x1844): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1844): 85.575

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1970.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1845x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1845x2048): 87.420
Elapsed time for attention_prob_times_values (48x2048x2048x1845): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1845): 83.532

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1932.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1846x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1846x2048): 88.078
Elapsed time for attention_prob_times_values (48x2048x2048x1846): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1846): 85.651

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1965.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1847x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1847x2048): 87.331
Elapsed time for attention_prob_times_values (48x2048x2048x1847): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1847): 83.574

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1934.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1848x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1848x2048): 89.463
Elapsed time for attention_prob_times_values (48x2048x2048x1848): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1848): 85.324

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1978.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1849x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1849x2048): 87.053
Elapsed time for attention_prob_times_values (48x2048x2048x1849): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1849): 83.657

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1934.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1850x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1850x2048): 87.855
Elapsed time for attention_prob_times_values (48x2048x2048x1850): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1850): 85.522

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1965.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1851x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1851x2048): 87.150
Elapsed time for attention_prob_times_values (48x2048x2048x1851): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1851): 83.804

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1938.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1852x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1852x2048): 88.887
Elapsed time for attention_prob_times_values (48x2048x2048x1852): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1852): 85.989

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1984.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1853x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1853x2048): 87.344
Elapsed time for attention_prob_times_values (48x2048x2048x1853): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1853): 83.679

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1941.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1854x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1854x2048): 88.144
Elapsed time for attention_prob_times_values (48x2048x2048x1854): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1854): 85.733

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1975.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1855x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1855x2048): 87.502
Elapsed time for attention_prob_times_values (48x2048x2048x1855): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1855): 83.893

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1947.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1856x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1856x2048): 97.372
Elapsed time for attention_prob_times_values (48x2048x2048x1856): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1856): 88.814

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 2113.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1857x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1857x2048): 88.481
Elapsed time for attention_prob_times_values (48x2048x2048x1857): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1857): 83.912

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1960.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1858x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1858x2048): 89.289
Elapsed time for attention_prob_times_values (48x2048x2048x1858): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1858): 86.180

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1997.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1859x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1859x2048): 88.309
Elapsed time for attention_prob_times_values (48x2048x2048x1859): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1859): 84.065

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1962.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1860x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1860x2048): 90.077
Elapsed time for attention_prob_times_values (48x2048x2048x1860): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1860): 86.114

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2007.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1861x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1861x2048): 88.151
Elapsed time for attention_prob_times_values (48x2048x2048x1861): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1861): 83.926

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1961.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1862x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1862x2048): 89.023
Elapsed time for attention_prob_times_values (48x2048x2048x1862): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1862): 86.150

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1998.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1863x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1863x2048): 87.921
Elapsed time for attention_prob_times_values (48x2048x2048x1863): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1863): 84.207

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1964.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1864x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1864x2048): 90.142
Elapsed time for attention_prob_times_values (48x2048x2048x1864): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1864): 82.258

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1965.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1865x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1865x2048): 87.809
Elapsed time for attention_prob_times_values (48x2048x2048x1865): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1865): 84.102

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1963.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1866x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1866x2048): 88.448
Elapsed time for attention_prob_times_values (48x2048x2048x1866): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1866): 85.906

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1993.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1867x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1867x2048): 87.830
Elapsed time for attention_prob_times_values (48x2048x2048x1867): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1867): 84.062

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1965.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1868x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1868x2048): 89.421
Elapsed time for attention_prob_times_values (48x2048x2048x1868): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1868): 86.434

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 2012.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1869x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1869x2048): 87.743
Elapsed time for attention_prob_times_values (48x2048x2048x1869): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1869): 84.350

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1969.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1870x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1870x2048): 88.737
Elapsed time for attention_prob_times_values (48x2048x2048x1870): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1870): 85.796

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1999.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1871x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1871x2048): 87.707
Elapsed time for attention_prob_times_values (48x2048x2048x1871): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1871): 84.574

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1974.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1872x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1872x2048): 89.972
Elapsed time for attention_prob_times_values (48x2048x2048x1872): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1872): 86.953

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2028.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1873x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1873x2048): 87.239
Elapsed time for attention_prob_times_values (48x2048x2048x1873): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1873): 84.386

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1968.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1874x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1874x2048): 88.242
Elapsed time for attention_prob_times_values (48x2048x2048x1874): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1874): 86.553

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2006.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1875x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1875x2048): 87.419
Elapsed time for attention_prob_times_values (48x2048x2048x1875): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1875): 84.243

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1971.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1876x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1876x2048): 89.117
Elapsed time for attention_prob_times_values (48x2048x2048x1876): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1876): 86.672

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2019.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1877x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1877x2048): 87.573
Elapsed time for attention_prob_times_values (48x2048x2048x1877): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1877): 84.639

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1979.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1878x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1878x2048): 88.340
Elapsed time for attention_prob_times_values (48x2048x2048x1878): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1878): 86.795

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2014.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1879x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1879x2048): 87.427
Elapsed time for attention_prob_times_values (48x2048x2048x1879): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1879): 84.562

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1979.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1880x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1880x2048): 89.759
Elapsed time for attention_prob_times_values (48x2048x2048x1880): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1880): 85.782

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2020.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1881x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1881x2048): 87.153
Elapsed time for attention_prob_times_values (48x2048x2048x1881): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1881): 84.456

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1976.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1882x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1882x2048): 88.028
Elapsed time for attention_prob_times_values (48x2048x2048x1882): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1882): 86.605

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2012.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1883x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1883x2048): 87.289
Elapsed time for attention_prob_times_values (48x2048x2048x1883): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1883): 83.773

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1972.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1884x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1884x2048): 89.021
Elapsed time for attention_prob_times_values (48x2048x2048x1884): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1884): 86.908

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2029.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1885x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1885x2048): 87.216
Elapsed time for attention_prob_times_values (48x2048x2048x1885): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1885): 84.701

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1984.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1886x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1886x2048): 88.061
Elapsed time for attention_prob_times_values (48x2048x2048x1886): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1886): 86.726

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2018.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1887x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1887x2048): 87.381
Elapsed time for attention_prob_times_values (48x2048x2048x1887): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1887): 84.458

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1985.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1888x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1888x2048): 97.479
Elapsed time for attention_prob_times_values (48x2048x2048x1888): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1888): 88.998

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2151.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1889x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1889x2048): 88.460
Elapsed time for attention_prob_times_values (48x2048x2048x1889): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1889): 85.012

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2005.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1890x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1890x2048): 89.367
Elapsed time for attention_prob_times_values (48x2048x2048x1890): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1890): 87.085

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2041.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1891x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1891x2048): 88.304
Elapsed time for attention_prob_times_values (48x2048x2048x1891): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1891): 85.059

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2006.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1892x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1892x2048): 90.047
Elapsed time for attention_prob_times_values (48x2048x2048x1892): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1892): 86.460

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2044.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1893x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1893x2048): 88.237
Elapsed time for attention_prob_times_values (48x2048x2048x1893): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1893): 84.637

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2003.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1894x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1894x2048): 89.006
Elapsed time for attention_prob_times_values (48x2048x2048x1894): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1894): 85.912

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2028.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1895x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1895x2048): 87.888
Elapsed time for attention_prob_times_values (48x2048x2048x1895): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1895): 84.121

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1994.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1896x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1896x2048): 89.948
Elapsed time for attention_prob_times_values (48x2048x2048x1896): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1896): 87.419

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2058.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1897x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1897x2048): 87.735
Elapsed time for attention_prob_times_values (48x2048x2048x1897): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1897): 84.271

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1997.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1898x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1898x2048): 88.582
Elapsed time for attention_prob_times_values (48x2048x2048x1898): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1898): 85.476

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2022.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1899x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1899x2048): 87.816
Elapsed time for attention_prob_times_values (48x2048x2048x1899): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1899): 84.062

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1997.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1900x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1900x2048): 89.255
Elapsed time for attention_prob_times_values (48x2048x2048x1900): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1900): 86.615

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2045.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1901x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1901x2048): 87.894
Elapsed time for attention_prob_times_values (48x2048x2048x1901): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1901): 84.307

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2003.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1902x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1902x2048): 88.722
Elapsed time for attention_prob_times_values (48x2048x2048x1902): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1902): 85.998

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2034.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1903x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1903x2048): 87.734
Elapsed time for attention_prob_times_values (48x2048x2048x1903): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1903): 84.365

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2004.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1904x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1904x2048): 90.307
Elapsed time for attention_prob_times_values (48x2048x2048x1904): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1904): 88.216

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2080.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1905x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1905x2048): 87.376
Elapsed time for attention_prob_times_values (48x2048x2048x1905): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1905): 85.081

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2010.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1906x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1906x2048): 88.489
Elapsed time for attention_prob_times_values (48x2048x2048x1906): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1906): 86.566

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2042.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1907x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1907x2048): 87.565
Elapsed time for attention_prob_times_values (48x2048x2048x1907): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1907): 84.591

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2009.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1908x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1908x2048): 88.975
Elapsed time for attention_prob_times_values (48x2048x2048x1908): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1908): 87.022

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2055.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1909x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1909x2048): 87.559
Elapsed time for attention_prob_times_values (48x2048x2048x1909): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1909): 84.820

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2013.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1910x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1910x2048): 88.424
Elapsed time for attention_prob_times_values (48x2048x2048x1910): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1910): 86.668

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2046.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1911x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1911x2048): 87.546
Elapsed time for attention_prob_times_values (48x2048x2048x1911): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1911): 84.546

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2012.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1912x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1912x2048): 89.525
Elapsed time for attention_prob_times_values (48x2048x2048x1912): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1912): 87.051

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2066.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1913x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1913x2048): 87.238
Elapsed time for attention_prob_times_values (48x2048x2048x1913): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1913): 84.839

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2014.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1914x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1914x2048): 87.996
Elapsed time for attention_prob_times_values (48x2048x2048x1914): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1914): 86.936

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2049.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1915x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1915x2048): 87.289
Elapsed time for attention_prob_times_values (48x2048x2048x1915): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1915): 85.029

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2019.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 22992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1916x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1916x2048): 88.840
Elapsed time for attention_prob_times_values (48x2048x2048x1916): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1916): 86.782

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2059.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1917x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1917x2048): 87.026
Elapsed time for attention_prob_times_values (48x2048x2048x1917): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1917): 83.856

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2004.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1918x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1918x2048): 88.236
Elapsed time for attention_prob_times_values (48x2048x2048x1918): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1918): 86.522

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2051.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1919x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1919x2048): 87.471
Elapsed time for attention_prob_times_values (48x2048x2048x1919): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1919): 84.522

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2019.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1920x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1920x2048): 95.736
Elapsed time for attention_prob_times_values (48x2048x2048x1920): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1920): 91.994

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 2204.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1921x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1921x2048): 88.451
Elapsed time for attention_prob_times_values (48x2048x2048x1921): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1921): 81.355

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1992.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1922x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1922x2048): 89.389
Elapsed time for attention_prob_times_values (48x2048x2048x1922): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1922): 84.036

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2037.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1923x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1923x2048): 87.836
Elapsed time for attention_prob_times_values (48x2048x2048x1923): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1923): 81.791

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1993.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1924x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1924x2048): 89.801
Elapsed time for attention_prob_times_values (48x2048x2048x1924): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1924): 84.176

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2046.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1925x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1925x2048): 88.188
Elapsed time for attention_prob_times_values (48x2048x2048x1925): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1925): 81.849

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2000.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1926x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1926x2048): 89.052
Elapsed time for attention_prob_times_values (48x2048x2048x1926): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1926): 84.145

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2039.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1927x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1927x2048): 88.081
Elapsed time for attention_prob_times_values (48x2048x2048x1927): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1927): 81.963

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2002.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1928x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1928x2048): 90.015
Elapsed time for attention_prob_times_values (48x2048x2048x1928): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1928): 82.739

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2034.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1929x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1929x2048): 87.733
Elapsed time for attention_prob_times_values (48x2048x2048x1929): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1929): 82.064

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2001.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1930x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1930x2048): 88.566
Elapsed time for attention_prob_times_values (48x2048x2048x1930): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1930): 84.399

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2041.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1931x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1931x2048): 87.940
Elapsed time for attention_prob_times_values (48x2048x2048x1931): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1931): 82.176

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2007.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1932x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1932x2048): 89.359
Elapsed time for attention_prob_times_values (48x2048x2048x1932): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1932): 84.471

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2053.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1933x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1933x2048): 87.896
Elapsed time for attention_prob_times_values (48x2048x2048x1933): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1933): 82.305

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2010.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1934x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1934x2048): 88.979
Elapsed time for attention_prob_times_values (48x2048x2048x1934): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1934): 84.410

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2050.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1935x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1935x2048): 88.051
Elapsed time for attention_prob_times_values (48x2048x2048x1935): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1935): 82.345

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2014.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1936x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1936x2048): 90.506
Elapsed time for attention_prob_times_values (48x2048x2048x1936): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1936): 83.821

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2061.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1937x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1937x2048): 87.670
Elapsed time for attention_prob_times_values (48x2048x2048x1937): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1937): 82.473

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2014.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1938x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1938x2048): 88.548
Elapsed time for attention_prob_times_values (48x2048x2048x1938): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1938): 84.534

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2050.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1939x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1939x2048): 87.680
Elapsed time for attention_prob_times_values (48x2048x2048x1939): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1939): 82.544

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2017.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1940x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1940x2048): 89.163
Elapsed time for attention_prob_times_values (48x2048x2048x1940): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1940): 84.685

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2061.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1941x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1941x2048): 87.855
Elapsed time for attention_prob_times_values (48x2048x2048x1941): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1941): 82.556

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2021.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1942x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1942x2048): 88.592
Elapsed time for attention_prob_times_values (48x2048x2048x1942): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1942): 84.482

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2054.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1943x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1943x2048): 87.882
Elapsed time for attention_prob_times_values (48x2048x2048x1943): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1943): 82.652

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2024.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1944x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1944x2048): 89.610
Elapsed time for attention_prob_times_values (48x2048x2048x1944): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1944): 83.216

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2052.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1945x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1945x2048): 87.600
Elapsed time for attention_prob_times_values (48x2048x2048x1945): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1945): 82.745

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2024.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1946x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1946x2048): 88.344
Elapsed time for attention_prob_times_values (48x2048x2048x1946): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1946): 84.876

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2060.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1947x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1947x2048): 87.629
Elapsed time for attention_prob_times_values (48x2048x2048x1947): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1947): 82.761

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2027.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1948x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1948x2048): 89.203
Elapsed time for attention_prob_times_values (48x2048x2048x1948): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1948): 85.093

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2075.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1949x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1949x2048): 87.775
Elapsed time for attention_prob_times_values (48x2048x2048x1949): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1949): 82.815

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2031.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1950x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1950x2048): 88.640
Elapsed time for attention_prob_times_values (48x2048x2048x1950): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1950): 84.801

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2067.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1951x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1951x2048): 87.776
Elapsed time for attention_prob_times_values (48x2048x2048x1951): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1951): 82.927

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2035.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1952x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1952x2048): 97.724
Elapsed time for attention_prob_times_values (48x2048x2048x1952): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1952): 85.551

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2178.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1953x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1953x2048): 88.791
Elapsed time for attention_prob_times_values (48x2048x2048x1953): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1953): 82.989

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2049.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1954x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1954x2048): 89.638
Elapsed time for attention_prob_times_values (48x2048x2048x1954): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1954): 85.269

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2088.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1955x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1955x2048): 88.670
Elapsed time for attention_prob_times_values (48x2048x2048x1955): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1955): 82.880

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2048.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1956x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1956x2048): 90.252
Elapsed time for attention_prob_times_values (48x2048x2048x1956): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1956): 85.192

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2096.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1957x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1957x2048): 88.625
Elapsed time for attention_prob_times_values (48x2048x2048x1957): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1957): 82.807

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2049.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1958x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1958x2048): 89.360
Elapsed time for attention_prob_times_values (48x2048x2048x1958): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1958): 85.103

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2087.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1959x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1959x2048): 88.511
Elapsed time for attention_prob_times_values (48x2048x2048x1959): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1959): 82.767

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2049.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1960x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1960x2048): 90.313
Elapsed time for attention_prob_times_values (48x2048x2048x1960): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1960): 73.284

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1939.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1961x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1961x2048): 88.123
Elapsed time for attention_prob_times_values (48x2048x2048x1961): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1961): 83.130

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2051.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1962x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1962x2048): 88.944
Elapsed time for attention_prob_times_values (48x2048x2048x1962): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1962): 85.295

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2089.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1963x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1963x2048): 88.387
Elapsed time for attention_prob_times_values (48x2048x2048x1963): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1963): 83.078

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2055.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1964x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1964x2048): 89.930
Elapsed time for attention_prob_times_values (48x2048x2048x1964): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1964): 85.390

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2103.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1965x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1965x2048): 88.303
Elapsed time for attention_prob_times_values (48x2048x2048x1965): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1965): 83.089

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2057.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1966x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1966x2048): 89.157
Elapsed time for attention_prob_times_values (48x2048x2048x1966): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1966): 85.429

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2097.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1967x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1967x2048): 88.104
Elapsed time for attention_prob_times_values (48x2048x2048x1967): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1967): 83.174

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2057.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1968x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1968x2048): 90.523
Elapsed time for attention_prob_times_values (48x2048x2048x1968): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1968): 85.056

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2110.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1969x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1969x2048): 87.710
Elapsed time for attention_prob_times_values (48x2048x2048x1969): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1969): 83.223

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2056.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1970x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1970x2048): 88.830
Elapsed time for attention_prob_times_values (48x2048x2048x1970): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1970): 85.504

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2098.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1971x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1971x2048): 87.697
Elapsed time for attention_prob_times_values (48x2048x2048x1971): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1971): 83.287

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2058.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1972x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1972x2048): 89.508
Elapsed time for attention_prob_times_values (48x2048x2048x1972): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1972): 85.621

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2110.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1973x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1973x2048): 87.879
Elapsed time for attention_prob_times_values (48x2048x2048x1973): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1973): 83.161

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2061.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1974x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1974x2048): 88.806
Elapsed time for attention_prob_times_values (48x2048x2048x1974): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1974): 85.541

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2103.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1975x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1975x2048): 87.704
Elapsed time for attention_prob_times_values (48x2048x2048x1975): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1975): 83.140

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2061.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1976x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1976x2048): 89.963
Elapsed time for attention_prob_times_values (48x2048x2048x1976): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1976): 83.952

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2098.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1977x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1977x2048): 87.554
Elapsed time for attention_prob_times_values (48x2048x2048x1977): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1977): 83.226

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2062.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1978x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1978x2048): 88.679
Elapsed time for attention_prob_times_values (48x2048x2048x1978): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1978): 85.611

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2106.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1979x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1979x2048): 87.647
Elapsed time for attention_prob_times_values (48x2048x2048x1979): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1979): 83.310

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2066.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1980x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1980x2048): 89.458
Elapsed time for attention_prob_times_values (48x2048x2048x1980): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1980): 85.947

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2121.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1981x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1981x2048): 87.531
Elapsed time for attention_prob_times_values (48x2048x2048x1981): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1981): 83.475

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2069.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1982x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1982x2048): 88.586
Elapsed time for attention_prob_times_values (48x2048x2048x1982): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1982): 85.927

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2113.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1983x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1983x2048): 87.753
Elapsed time for attention_prob_times_values (48x2048x2048x1983): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1983): 83.549

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2074.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1984x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1984x2048): 97.619
Elapsed time for attention_prob_times_values (48x2048x2048x1984): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1984): 93.689

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 2318.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1985x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1985x2048): 88.877
Elapsed time for attention_prob_times_values (48x2048x2048x1985): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1985): 83.490

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2088.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1986x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1986x2048): 89.940
Elapsed time for attention_prob_times_values (48x2048x2048x1986): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1986): 86.028

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2134.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1987x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1987x2048): 88.654
Elapsed time for attention_prob_times_values (48x2048x2048x1987): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1987): 83.474

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2088.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1988x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1988x2048): 90.587
Elapsed time for attention_prob_times_values (48x2048x2048x1988): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1988): 86.238

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2146.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1989x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1989x2048): 88.475
Elapsed time for attention_prob_times_values (48x2048x2048x1989): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1989): 83.656

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2090.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1990x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1990x2048): 89.524
Elapsed time for attention_prob_times_values (48x2048x2048x1990): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1990): 86.142

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2135.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1991x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1991x2048): 88.301
Elapsed time for attention_prob_times_values (48x2048x2048x1991): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1991): 83.943

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2094.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1992x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1992x2048): 90.730
Elapsed time for attention_prob_times_values (48x2048x2048x1992): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1992): 92.606

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2231.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1993x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1993x2048): 88.154
Elapsed time for attention_prob_times_values (48x2048x2048x1993): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1993): 83.925

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2094.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1994x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1994x2048): 89.221
Elapsed time for attention_prob_times_values (48x2048x2048x1994): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1994): 86.350

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2138.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1995x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1995x2048): 88.321
Elapsed time for attention_prob_times_values (48x2048x2048x1995): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1995): 84.181

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2101.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1996x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1996x2048): 90.060
Elapsed time for attention_prob_times_values (48x2048x2048x1996): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1996): 86.510

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2152.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1997x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1997x2048): 88.061
Elapsed time for attention_prob_times_values (48x2048x2048x1997): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1997): 84.101

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2099.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1998x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1998x2048): 89.037
Elapsed time for attention_prob_times_values (48x2048x2048x1998): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1998): 86.594

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2143.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 23988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1999x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1999x2048): 88.211
Elapsed time for attention_prob_times_values (48x2048x2048x1999): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1999): 84.263

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2105.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2000x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2000x2048): 90.784
Elapsed time for attention_prob_times_values (48x2048x2048x2000): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2000): 93.358

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2249.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2001x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2001x2048): 87.831
Elapsed time for attention_prob_times_values (48x2048x2048x2001): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2001): 84.489

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2105.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2002x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2002x2048): 88.763
Elapsed time for attention_prob_times_values (48x2048x2048x2002): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2002): 86.628

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2144.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2003x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2003x2048): 87.939
Elapsed time for attention_prob_times_values (48x2048x2048x2003): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2003): 84.459

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2108.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2004x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2004x2048): 89.567
Elapsed time for attention_prob_times_values (48x2048x2048x2004): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2004): 86.743

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2157.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2005x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2005x2048): 88.007
Elapsed time for attention_prob_times_values (48x2048x2048x2005): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2005): 84.483

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2111.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2006x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2006x2048): 88.852
Elapsed time for attention_prob_times_values (48x2048x2048x2006): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2006): 86.761

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2151.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2007x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2007x2048): 88.120
Elapsed time for attention_prob_times_values (48x2048x2048x2007): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2007): 84.436

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2114.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2008x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2008x2048): 90.684
Elapsed time for attention_prob_times_values (48x2048x2048x2008): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2008): 93.301

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2256.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2009x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2009x2048): 88.507
Elapsed time for attention_prob_times_values (48x2048x2048x2009): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2009): 84.520

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2122.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2010x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2010x2048): 88.901
Elapsed time for attention_prob_times_values (48x2048x2048x2010): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2010): 86.814

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2157.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2011x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2011x2048): 87.786
Elapsed time for attention_prob_times_values (48x2048x2048x2011): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2011): 84.562

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2116.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2012x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2012x2048): 89.366
Elapsed time for attention_prob_times_values (48x2048x2048x2012): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2012): 86.947

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2166.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2013x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2013x2048): 87.769
Elapsed time for attention_prob_times_values (48x2048x2048x2013): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2013): 84.514

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2117.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2014x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2014x2048): 88.734
Elapsed time for attention_prob_times_values (48x2048x2048x2014): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2014): 86.924

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2160.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2015x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2015x2048): 87.950
Elapsed time for attention_prob_times_values (48x2048x2048x2015): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2015): 84.695

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2123.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2016x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2016x2048): 97.736
Elapsed time for attention_prob_times_values (48x2048x2048x2016): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2016): 94.570

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2367.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2017x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2017x2048): 88.896
Elapsed time for attention_prob_times_values (48x2048x2048x2017): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2017): 84.813

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2138.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2018x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2018x2048): 89.900
Elapsed time for attention_prob_times_values (48x2048x2048x2018): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2018): 87.032

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2179.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2019x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2019x2048): 88.910
Elapsed time for attention_prob_times_values (48x2048x2048x2019): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2019): 84.709

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2139.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2020x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2020x2048): 90.646
Elapsed time for attention_prob_times_values (48x2048x2048x2020): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2020): 87.110

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2191.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2021x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2021x2048): 88.649
Elapsed time for attention_prob_times_values (48x2048x2048x2021): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2021): 84.679

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2138.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2022x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2022x2048): 89.626
Elapsed time for attention_prob_times_values (48x2048x2048x2022): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2022): 87.169

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2182.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2023x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2023x2048): 89.438
Elapsed time for attention_prob_times_values (48x2048x2048x2023): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2023): 84.675

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2149.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2024x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2024x2048): 92.240
Elapsed time for attention_prob_times_values (48x2048x2048x2024): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2024): 93.122

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2290.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2025x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2025x2048): 90.064
Elapsed time for attention_prob_times_values (48x2048x2048x2025): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2025): 84.562

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2157.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2026x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2026x2048): 89.242
Elapsed time for attention_prob_times_values (48x2048x2048x2026): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2026): 87.166

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2182.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2027x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2027x2048): 88.439
Elapsed time for attention_prob_times_values (48x2048x2048x2027): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2027): 84.536

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2139.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2028x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2028x2048): 89.749
Elapsed time for attention_prob_times_values (48x2048x2048x2028): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2028): 87.336

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2192.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2029x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2029x2048): 88.396
Elapsed time for attention_prob_times_values (48x2048x2048x2029): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2029): 84.645

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2142.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2030x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2030x2048): 89.399
Elapsed time for attention_prob_times_values (48x2048x2048x2030): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2030): 87.198

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2188.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2031x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2031x2048): 88.381
Elapsed time for attention_prob_times_values (48x2048x2048x2031): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2031): 84.626

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2144.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2032x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2032x2048): 91.127
Elapsed time for attention_prob_times_values (48x2048x2048x2032): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2032): 94.661

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2304.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2033x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2033x2048): 88.134
Elapsed time for attention_prob_times_values (48x2048x2048x2033): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2033): 84.843

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2146.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2034x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2034x2048): 89.354
Elapsed time for attention_prob_times_values (48x2048x2048x2034): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2034): 87.388

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2194.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2035x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2035x2048): 89.245
Elapsed time for attention_prob_times_values (48x2048x2048x2035): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2035): 84.688

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2159.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2036x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2036x2048): 89.947
Elapsed time for attention_prob_times_values (48x2048x2048x2036): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2036): 87.543

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2205.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2037x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2037x2048): 89.133
Elapsed time for attention_prob_times_values (48x2048x2048x2037): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2037): 84.730

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2160.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2038x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2038x2048): 90.222
Elapsed time for attention_prob_times_values (48x2048x2048x2038): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2038): 87.251

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2207.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2039x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2039x2048): 91.317
Elapsed time for attention_prob_times_values (48x2048x2048x2039): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2039): 78.919

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2107.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2040x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2040x2048): 90.230
Elapsed time for attention_prob_times_values (48x2048x2048x2040): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2040): 90.405

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2249.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2041x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2041x2048): 91.020
Elapsed time for attention_prob_times_values (48x2048x2048x2041): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2041): 79.551

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2115.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2042x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2042x2048): 89.993
Elapsed time for attention_prob_times_values (48x2048x2048x2042): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2042): 86.723

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2201.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2043x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2043x2048): 88.937
Elapsed time for attention_prob_times_values (48x2048x2048x2043): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2043): 84.205

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2157.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2044x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2044x2048): 89.870
Elapsed time for attention_prob_times_values (48x2048x2048x2044): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2044): 87.335

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2210.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2045x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2045x2048): 87.912
Elapsed time for attention_prob_times_values (48x2048x2048x2045): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2045): 84.426

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2150.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2046x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2046x2048): 89.056
Elapsed time for attention_prob_times_values (48x2048x2048x2046): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2046): 87.257

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2201.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2047x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2047x2048): 88.056
Elapsed time for attention_prob_times_values (48x2048x2048x2047): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2047): 84.832

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2159.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2048x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2048x2048): 96.957
Elapsed time for attention_prob_times_values (48x2048x2048x2048): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2048): 97.034

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2424.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2049x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2049x2048): 88.855
Elapsed time for attention_prob_times_values (48x2048x2048x2049): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2049): 80.453

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2112.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2050x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2050x2048): 89.816
Elapsed time for attention_prob_times_values (48x2048x2048x2050): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2050): 83.720

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2168.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2051x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2051x2048): 88.537
Elapsed time for attention_prob_times_values (48x2048x2048x2051): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2051): 80.991

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2117.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2052x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2052x2048): 90.766
Elapsed time for attention_prob_times_values (48x2048x2048x2052): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2052): 83.799

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2182.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2053x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2053x2048): 89.641
Elapsed time for attention_prob_times_values (48x2048x2048x2053): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2053): 80.887

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2130.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2054x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2054x2048): 90.352
Elapsed time for attention_prob_times_values (48x2048x2048x2054): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2054): 83.600

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2177.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2055x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2055x2048): 91.322
Elapsed time for attention_prob_times_values (48x2048x2048x2055): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2055): 80.621

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2147.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2056x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2056x2048): 89.229
Elapsed time for attention_prob_times_values (48x2048x2048x2056): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2056): 85.937

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2197.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2057x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2057x2048): 91.567
Elapsed time for attention_prob_times_values (48x2048x2048x2057): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2057): 77.276

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2104.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2058x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2058x2048): 90.540
Elapsed time for attention_prob_times_values (48x2048x2048x2058): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2058): 83.140

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2177.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2059x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2059x2048): 89.344
Elapsed time for attention_prob_times_values (48x2048x2048x2059): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2059): 80.778

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2132.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2060x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2060x2048): 90.191
Elapsed time for attention_prob_times_values (48x2048x2048x2060): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2060): 83.687

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2182.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2061x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2061x2048): 89.502
Elapsed time for attention_prob_times_values (48x2048x2048x2061): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2061): 80.960

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2138.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2062x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2062x2048): 89.771
Elapsed time for attention_prob_times_values (48x2048x2048x2062): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2062): 83.877

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2182.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2063x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2063x2048): 88.451
Elapsed time for attention_prob_times_values (48x2048x2048x2063): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2063): 81.652

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2137.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2064x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2064x2048): 90.987
Elapsed time for attention_prob_times_values (48x2048x2048x2064): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2064): 90.077

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2280.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2065x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2065x2048): 87.950
Elapsed time for attention_prob_times_values (48x2048x2048x2065): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2065): 81.689

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2134.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2066x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2066x2048): 89.130
Elapsed time for attention_prob_times_values (48x2048x2048x2066): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2066): 84.269

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2184.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2067x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2067x2048): 88.088
Elapsed time for attention_prob_times_values (48x2048x2048x2067): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2067): 81.770

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2139.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2068x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2068x2048): 89.640
Elapsed time for attention_prob_times_values (48x2048x2048x2068): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2068): 84.371

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2193.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2069x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2069x2048): 88.294
Elapsed time for attention_prob_times_values (48x2048x2048x2069): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2069): 81.897

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2145.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2070x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2070x2048): 89.088
Elapsed time for attention_prob_times_values (48x2048x2048x2070): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2070): 84.324

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2188.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2071x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2071x2048): 88.932
Elapsed time for attention_prob_times_values (48x2048x2048x2071): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2071): 81.959

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2155.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2072x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2072x2048): 91.804
Elapsed time for attention_prob_times_values (48x2048x2048x2072): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2072): 89.873

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2296.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2073x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2073x2048): 89.670
Elapsed time for attention_prob_times_values (48x2048x2048x2073): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2073): 82.117

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2168.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2074x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2074x2048): 88.943
Elapsed time for attention_prob_times_values (48x2048x2048x2074): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2074): 84.508

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2193.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2075x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2075x2048): 88.098
Elapsed time for attention_prob_times_values (48x2048x2048x2075): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2075): 82.156

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2152.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2076x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2076x2048): 89.456
Elapsed time for attention_prob_times_values (48x2048x2048x2076): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2076): 84.620

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2202.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2077x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2077x2048): 87.867
Elapsed time for attention_prob_times_values (48x2048x2048x2077): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2077): 82.145

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2151.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2078x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2078x2048): 88.842
Elapsed time for attention_prob_times_values (48x2048x2048x2078): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2078): 84.671

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2198.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2079x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2079x2048): 88.038
Elapsed time for attention_prob_times_values (48x2048x2048x2079): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2079): 82.380

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2158.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2080x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2080x2048): 97.869
Elapsed time for attention_prob_times_values (48x2048x2048x2080): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2080): 91.370

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2398.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2081x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2081x2048): 89.015
Elapsed time for attention_prob_times_values (48x2048x2048x2081): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2081): 82.459

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2173.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2082x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2082x2048): 89.921
Elapsed time for attention_prob_times_values (48x2048x2048x2082): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2082): 84.903

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2218.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2083x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2083x2048): 88.995
Elapsed time for attention_prob_times_values (48x2048x2048x2083): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2083): 82.496

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2175.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2084x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2084x2048): 90.575
Elapsed time for attention_prob_times_values (48x2048x2048x2084): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2084): 85.073

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2230.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2085x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2085x2048): 88.755
Elapsed time for attention_prob_times_values (48x2048x2048x2085): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2085): 82.596

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2176.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2086x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2086x2048): 89.759
Elapsed time for attention_prob_times_values (48x2048x2048x2086): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2086): 85.093

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2222.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2087x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2087x2048): 88.732
Elapsed time for attention_prob_times_values (48x2048x2048x2087): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2087): 82.415

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2175.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2088x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2088x2048): 91.313
Elapsed time for attention_prob_times_values (48x2048x2048x2088): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2088): 90.868

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2319.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2089x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2089x2048): 89.087
Elapsed time for attention_prob_times_values (48x2048x2048x2089): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2089): 82.887

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2188.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2090x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2090x2048): 89.616
Elapsed time for attention_prob_times_values (48x2048x2048x2090): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2090): 85.248

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2227.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2091x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2091x2048): 88.599
Elapsed time for attention_prob_times_values (48x2048x2048x2091): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2091): 82.809

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2183.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2092x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2092x2048): 90.141
Elapsed time for attention_prob_times_values (48x2048x2048x2092): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2092): 85.491

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2239.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2093x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2093x2048): 88.484
Elapsed time for attention_prob_times_values (48x2048x2048x2093): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2093): 82.886

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2184.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2094x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2094x2048): 89.517
Elapsed time for attention_prob_times_values (48x2048x2048x2094): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2094): 85.455

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2233.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2095x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2095x2048): 88.540
Elapsed time for attention_prob_times_values (48x2048x2048x2095): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2095): 83.049

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2189.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2096x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2096x2048): 90.923
Elapsed time for attention_prob_times_values (48x2048x2048x2096): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2096): 91.767

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2334.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2097x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2097x2048): 88.157
Elapsed time for attention_prob_times_values (48x2048x2048x2097): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2097): 83.128

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2188.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2098x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2098x2048): 89.108
Elapsed time for attention_prob_times_values (48x2048x2048x2098): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2098): 85.809

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2236.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2099x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2099x2048): 88.203
Elapsed time for attention_prob_times_values (48x2048x2048x2099): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2099): 83.331

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2193.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2100x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2100x2048): 90.038
Elapsed time for attention_prob_times_values (48x2048x2048x2100): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2100): 85.903

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2251.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2101x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2101x2048): 88.519
Elapsed time for attention_prob_times_values (48x2048x2048x2101): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2101): 83.393

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2200.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2102x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2102x2048): 89.332
Elapsed time for attention_prob_times_values (48x2048x2048x2102): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2102): 85.933

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2245.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2103x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2103x2048): 88.554
Elapsed time for attention_prob_times_values (48x2048x2048x2103): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2103): 83.541

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2204.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2104x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2104x2048): 90.542
Elapsed time for attention_prob_times_values (48x2048x2048x2104): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2104): 91.801

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2339.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2105x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2105x2048): 88.396
Elapsed time for attention_prob_times_values (48x2048x2048x2105): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2105): 83.402

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2202.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2106x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2106x2048): 89.297
Elapsed time for attention_prob_times_values (48x2048x2048x2106): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2106): 86.015

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2250.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2107x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2107x2048): 88.364
Elapsed time for attention_prob_times_values (48x2048x2048x2107): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2107): 83.709

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2208.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2108x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2108x2048): 89.947
Elapsed time for attention_prob_times_values (48x2048x2048x2108): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2108): 86.299

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2264.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2109x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2109x2048): 88.344
Elapsed time for attention_prob_times_values (48x2048x2048x2109): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2109): 83.789

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2211.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2110x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2110x2048): 89.245
Elapsed time for attention_prob_times_values (48x2048x2048x2110): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2110): 86.283

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2257.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2111x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2111x2048): 88.527
Elapsed time for attention_prob_times_values (48x2048x2048x2111): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2111): 83.906

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2217.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2112x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2112x2048): 98.258
Elapsed time for attention_prob_times_values (48x2048x2048x2112): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2112): 93.855

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2472.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2113x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2113x2048): 89.529
Elapsed time for attention_prob_times_values (48x2048x2048x2113): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2113): 83.814

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2230.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2114x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2114x2048): 90.399
Elapsed time for attention_prob_times_values (48x2048x2048x2114): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2114): 86.409

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2277.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2115x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2115x2048): 89.419
Elapsed time for attention_prob_times_values (48x2048x2048x2115): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2115): 84.071

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2234.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2116x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2116x2048): 91.013
Elapsed time for attention_prob_times_values (48x2048x2048x2116): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2116): 86.660

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2290.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2117x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2117x2048): 89.186
Elapsed time for attention_prob_times_values (48x2048x2048x2117): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2117): 84.115

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2234.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2118x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2118x2048): 90.155
Elapsed time for attention_prob_times_values (48x2048x2048x2118): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2118): 86.576

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2280.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2119x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2119x2048): 89.064
Elapsed time for attention_prob_times_values (48x2048x2048x2119): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2119): 84.268

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2237.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2120x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2120x2048): 91.034
Elapsed time for attention_prob_times_values (48x2048x2048x2120): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2120): 92.487

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2371.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2121x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2121x2048): 88.704
Elapsed time for attention_prob_times_values (48x2048x2048x2121): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2121): 84.365

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2235.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2122x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2122x2048): 89.744
Elapsed time for attention_prob_times_values (48x2048x2048x2122): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2122): 86.786

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2282.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2123x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2123x2048): 89.216
Elapsed time for attention_prob_times_values (48x2048x2048x2123): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2123): 84.236

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2242.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2124x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2124x2048): 90.419
Elapsed time for attention_prob_times_values (48x2048x2048x2124): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2124): 86.876

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2294.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2125x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2125x2048): 88.819
Elapsed time for attention_prob_times_values (48x2048x2048x2125): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2125): 84.486

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2243.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2126x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2126x2048): 89.726
Elapsed time for attention_prob_times_values (48x2048x2048x2126): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2126): 86.788

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2286.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2127x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2127x2048): 88.808
Elapsed time for attention_prob_times_values (48x2048x2048x2127): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2127): 84.628

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2246.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2128x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2128x2048): 91.097
Elapsed time for attention_prob_times_values (48x2048x2048x2128): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2128): 93.263

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2390.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2129x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2129x2048): 88.463
Elapsed time for attention_prob_times_values (48x2048x2048x2129): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2129): 84.769

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2246.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2130x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2130x2048): 89.406
Elapsed time for attention_prob_times_values (48x2048x2048x2130): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2130): 87.001

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2289.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2131x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2131x2048): 88.402
Elapsed time for attention_prob_times_values (48x2048x2048x2131): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2131): 84.711

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2247.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2132x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2132x2048): 90.020
Elapsed time for attention_prob_times_values (48x2048x2048x2132): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2132): 87.035

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2299.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2133x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2133x2048): 88.603
Elapsed time for attention_prob_times_values (48x2048x2048x2133): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2133): 84.828

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2253.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2134x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2134x2048): 89.354
Elapsed time for attention_prob_times_values (48x2048x2048x2134): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2134): 87.119

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2294.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2135x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2135x2048): 88.525
Elapsed time for attention_prob_times_values (48x2048x2048x2135): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2135): 84.818

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2254.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2136x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2136x2048): 90.436
Elapsed time for attention_prob_times_values (48x2048x2048x2136): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2136): 93.103

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2388.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2137x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2137x2048): 88.209
Elapsed time for attention_prob_times_values (48x2048x2048x2137): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2137): 84.989

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2254.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2138x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2138x2048): 89.135
Elapsed time for attention_prob_times_values (48x2048x2048x2138): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2138): 87.094

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2295.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2139x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2139x2048): 88.410
Elapsed time for attention_prob_times_values (48x2048x2048x2139): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2139): 84.758

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2255.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2140x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2140x2048): 90.014
Elapsed time for attention_prob_times_values (48x2048x2048x2140): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2140): 87.220

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2310.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2141x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2141x2048): 88.426
Elapsed time for attention_prob_times_values (48x2048x2048x2141): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2141): 84.948

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2260.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2142x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2142x2048): 89.207
Elapsed time for attention_prob_times_values (48x2048x2048x2142): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2142): 87.268

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2302.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2143x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2143x2048): 88.496
Elapsed time for attention_prob_times_values (48x2048x2048x2143): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2143): 85.076

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2265.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2144x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2144x2048): 98.082
Elapsed time for attention_prob_times_values (48x2048x2048x2144): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2144): 94.383

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2513.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2145x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2145x2048): 89.477
Elapsed time for attention_prob_times_values (48x2048x2048x2145): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2145): 85.147

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2280.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2146x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2146x2048): 90.325
Elapsed time for attention_prob_times_values (48x2048x2048x2146): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2146): 87.363

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2322.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2147x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2147x2048): 89.138
Elapsed time for attention_prob_times_values (48x2048x2048x2147): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2147): 85.124

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2278.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2148x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2148x2048): 90.793
Elapsed time for attention_prob_times_values (48x2048x2048x2148): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2148): 87.512

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2332.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2149x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2149x2048): 89.011
Elapsed time for attention_prob_times_values (48x2048x2048x2149): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2149): 85.202

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2279.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2150x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2150x2048): 90.040
Elapsed time for attention_prob_times_values (48x2048x2048x2150): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2150): 87.507

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2324.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2151x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2151x2048): 88.974
Elapsed time for attention_prob_times_values (48x2048x2048x2151): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2151): 85.197

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2281.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2152x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2152x2048): 90.856
Elapsed time for attention_prob_times_values (48x2048x2048x2152): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2152): 93.728

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2419.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2153x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2153x2048): 88.625
Elapsed time for attention_prob_times_values (48x2048x2048x2153): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2153): 85.248

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2279.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2154x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2154x2048): 89.532
Elapsed time for attention_prob_times_values (48x2048x2048x2154): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2154): 87.510

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2322.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2155x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2155x2048): 88.726
Elapsed time for attention_prob_times_values (48x2048x2048x2155): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2155): 85.283

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2283.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2156x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2156x2048): 90.213
Elapsed time for attention_prob_times_values (48x2048x2048x2156): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2156): 87.751

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2336.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2157x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2157x2048): 88.830
Elapsed time for attention_prob_times_values (48x2048x2048x2157): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2157): 85.293

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2286.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2158x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2158x2048): 89.822
Elapsed time for attention_prob_times_values (48x2048x2048x2158): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2158): 87.645

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2332.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2159x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2159x2048): 88.848
Elapsed time for attention_prob_times_values (48x2048x2048x2159): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2159): 85.280

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2288.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2160x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2160x2048): 91.301
Elapsed time for attention_prob_times_values (48x2048x2048x2160): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2160): 94.464

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2443.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2161x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2161x2048): 88.503
Elapsed time for attention_prob_times_values (48x2048x2048x2161): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2161): 85.515

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2289.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2162x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2162x2048): 89.462
Elapsed time for attention_prob_times_values (48x2048x2048x2162): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2162): 87.717

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2332.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2163x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2163x2048): 88.456
Elapsed time for attention_prob_times_values (48x2048x2048x2163): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2163): 85.486

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2290.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2164x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2164x2048): 89.983
Elapsed time for attention_prob_times_values (48x2048x2048x2164): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2164): 87.973

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2345.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2165x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2165x2048): 88.624
Elapsed time for attention_prob_times_values (48x2048x2048x2165): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2165): 85.439

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2294.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 25992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2166x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2166x2048): 89.400
Elapsed time for attention_prob_times_values (48x2048x2048x2166): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2166): 88.042

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2340.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2167x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2167x2048): 88.538
Elapsed time for attention_prob_times_values (48x2048x2048x2167): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2167): 85.398

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2294.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2168x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2168x2048): 90.461
Elapsed time for attention_prob_times_values (48x2048x2048x2168): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2168): 94.274

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2438.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2169x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2169x2048): 88.330
Elapsed time for attention_prob_times_values (48x2048x2048x2169): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2169): 85.681

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2297.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2170x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2170x2048): 89.222
Elapsed time for attention_prob_times_values (48x2048x2048x2170): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2170): 88.160

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2343.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2171x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2171x2048): 88.324
Elapsed time for attention_prob_times_values (48x2048x2048x2171): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2171): 85.903

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2302.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2172x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2172x2048): 89.922
Elapsed time for attention_prob_times_values (48x2048x2048x2172): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2172): 88.422

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2358.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2173x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2173x2048): 88.222
Elapsed time for attention_prob_times_values (48x2048x2048x2173): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2173): 85.994

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2304.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2174x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2174x2048): 89.424
Elapsed time for attention_prob_times_values (48x2048x2048x2174): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2174): 88.399

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2353.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2175x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2175x2048): 88.548
Elapsed time for attention_prob_times_values (48x2048x2048x2175): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2175): 85.717

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2307.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2176x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2176x2048): 96.483
Elapsed time for attention_prob_times_values (48x2048x2048x2176): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2176): 96.727

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2560.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2177x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2177x2048): 89.508
Elapsed time for attention_prob_times_values (48x2048x2048x2177): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2177): 82.097

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2270.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2178x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2178x2048): 90.552
Elapsed time for attention_prob_times_values (48x2048x2048x2178): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2178): 84.666

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2321.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2179x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2179x2048): 88.921
Elapsed time for attention_prob_times_values (48x2048x2048x2179): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2179): 82.467

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2270.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2180x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2180x2048): 90.848
Elapsed time for attention_prob_times_values (48x2048x2048x2180): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2180): 84.853

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2329.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2181x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2181x2048): 89.259
Elapsed time for attention_prob_times_values (48x2048x2048x2181): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2181): 82.237

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2273.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2182x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2182x2048): 90.071
Elapsed time for attention_prob_times_values (48x2048x2048x2182): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2182): 84.687

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2319.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2183x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2183x2048): 89.149
Elapsed time for attention_prob_times_values (48x2048x2048x2183): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2183): 82.353

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2275.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2184x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2184x2048): 91.001
Elapsed time for attention_prob_times_values (48x2048x2048x2184): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2184): 89.929

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2405.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2185x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2185x2048): 88.825
Elapsed time for attention_prob_times_values (48x2048x2048x2185): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2185): 82.800

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2280.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2186x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2186x2048): 89.701
Elapsed time for attention_prob_times_values (48x2048x2048x2186): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2186): 84.881

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2321.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2187x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2187x2048): 88.932
Elapsed time for attention_prob_times_values (48x2048x2048x2187): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2187): 82.835

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2284.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2188x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2188x2048): 90.427
Elapsed time for attention_prob_times_values (48x2048x2048x2188): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2188): 85.139

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2336.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2189x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2189x2048): 88.893
Elapsed time for attention_prob_times_values (48x2048x2048x2189): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2189): 82.922

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2286.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2190x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2190x2048): 89.874
Elapsed time for attention_prob_times_values (48x2048x2048x2190): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2190): 85.013

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2329.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2191x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2191x2048): 89.079
Elapsed time for attention_prob_times_values (48x2048x2048x2191): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2191): 82.953

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2291.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2192x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2192x2048): 91.594
Elapsed time for attention_prob_times_values (48x2048x2048x2192): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2192): 90.637

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2431.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2193x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2193x2048): 88.915
Elapsed time for attention_prob_times_values (48x2048x2048x2193): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2193): 83.099

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2293.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2194x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2194x2048): 89.737
Elapsed time for attention_prob_times_values (48x2048x2048x2194): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2194): 84.913

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2330.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2195x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2195x2048): 88.783
Elapsed time for attention_prob_times_values (48x2048x2048x2195): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2195): 83.093

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2293.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2196x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2196x2048): 90.401
Elapsed time for attention_prob_times_values (48x2048x2048x2196): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2196): 85.291

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2346.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2197x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2197x2048): 88.902
Elapsed time for attention_prob_times_values (48x2048x2048x2197): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2197): 83.154

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2298.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2198x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2198x2048): 89.859
Elapsed time for attention_prob_times_values (48x2048x2048x2198): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2198): 85.169

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2340.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2199x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2199x2048): 88.817
Elapsed time for attention_prob_times_values (48x2048x2048x2199): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2199): 83.205

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2300.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2200x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2200x2048): 90.617
Elapsed time for attention_prob_times_values (48x2048x2048x2200): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2200): 90.465

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2424.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2201x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2201x2048): 88.525
Elapsed time for attention_prob_times_values (48x2048x2048x2201): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2201): 83.251

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2299.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2202x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2202x2048): 89.398
Elapsed time for attention_prob_times_values (48x2048x2048x2202): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2202): 85.213

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2338.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2203x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2203x2048): 88.527
Elapsed time for attention_prob_times_values (48x2048x2048x2203): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2203): 83.299

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2301.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2204x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2204x2048): 89.963
Elapsed time for attention_prob_times_values (48x2048x2048x2204): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2204): 85.427

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2351.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2205x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2205x2048): 88.555
Elapsed time for attention_prob_times_values (48x2048x2048x2205): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2205): 83.309

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2304.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2206x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2206x2048): 89.416
Elapsed time for attention_prob_times_values (48x2048x2048x2206): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2206): 85.407

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2345.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2207x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2207x2048): 88.680
Elapsed time for attention_prob_times_values (48x2048x2048x2207): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2207): 83.444

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2309.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2208x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2208x2048): 98.242
Elapsed time for attention_prob_times_values (48x2048x2048x2208): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2208): 91.673

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2548.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2209x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2209x2048): 89.822
Elapsed time for attention_prob_times_values (48x2048x2048x2209): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2209): 83.501

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2326.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2210x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2210x2048): 90.618
Elapsed time for attention_prob_times_values (48x2048x2048x2210): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2210): 85.557

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2367.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2211x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2211x2048): 89.533
Elapsed time for attention_prob_times_values (48x2048x2048x2211): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2211): 83.414

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2324.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2212x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2212x2048): 91.167
Elapsed time for attention_prob_times_values (48x2048x2048x2212): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2212): 85.594

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2376.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2213x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2213x2048): 89.599
Elapsed time for attention_prob_times_values (48x2048x2048x2213): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2213): 83.440

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2327.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2214x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2214x2048): 90.428
Elapsed time for attention_prob_times_values (48x2048x2048x2214): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2214): 85.612

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2369.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2215x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2215x2048): 89.279
Elapsed time for attention_prob_times_values (48x2048x2048x2215): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2215): 83.489

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2326.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2216x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2216x2048): 91.125
Elapsed time for attention_prob_times_values (48x2048x2048x2216): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2216): 91.058

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2456.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2217x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2217x2048): 88.976
Elapsed time for attention_prob_times_values (48x2048x2048x2217): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2217): 83.536

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2324.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2218x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2218x2048): 89.939
Elapsed time for attention_prob_times_values (48x2048x2048x2218): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2218): 85.826

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2370.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2219x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2219x2048): 89.145
Elapsed time for attention_prob_times_values (48x2048x2048x2219): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2219): 83.539

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2329.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2220x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2220x2048): 90.533
Elapsed time for attention_prob_times_values (48x2048x2048x2220): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2220): 85.907

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2381.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2221x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2221x2048): 89.051
Elapsed time for attention_prob_times_values (48x2048x2048x2221): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2221): 83.579

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2330.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2222x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2222x2048): 90.047
Elapsed time for attention_prob_times_values (48x2048x2048x2222): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2222): 85.815

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2376.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2223x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2223x2048): 89.127
Elapsed time for attention_prob_times_values (48x2048x2048x2223): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2223): 83.611

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2333.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2224x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2224x2048): 91.503
Elapsed time for attention_prob_times_values (48x2048x2048x2224): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2224): 91.688

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2478.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2225x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2225x2048): 89.095
Elapsed time for attention_prob_times_values (48x2048x2048x2225): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2225): 83.812

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2338.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2226x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2226x2048): 89.862
Elapsed time for attention_prob_times_values (48x2048x2048x2226): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2226): 86.068

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2381.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2227x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2227x2048): 88.876
Elapsed time for attention_prob_times_values (48x2048x2048x2227): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2227): 83.838

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2338.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2228x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2228x2048): 90.442
Elapsed time for attention_prob_times_values (48x2048x2048x2228): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2228): 86.108

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2391.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2229x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2229x2048): 89.106
Elapsed time for attention_prob_times_values (48x2048x2048x2229): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2229): 83.813

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2342.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2230x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2230x2048): 90.017
Elapsed time for attention_prob_times_values (48x2048x2048x2230): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2230): 86.176

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2389.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2231x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2231x2048): 89.074
Elapsed time for attention_prob_times_values (48x2048x2048x2231): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2231): 83.832

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2344.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2232x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2232x2048): 91.000
Elapsed time for attention_prob_times_values (48x2048x2048x2232): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2232): 91.649

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2480.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2233x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2233x2048): 88.680
Elapsed time for attention_prob_times_values (48x2048x2048x2233): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2233): 83.986

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2343.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2234x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2234x2048): 89.511
Elapsed time for attention_prob_times_values (48x2048x2048x2234): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2234): 86.336

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2388.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2235x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2235x2048): 88.843
Elapsed time for attention_prob_times_values (48x2048x2048x2235): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2235): 84.083

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2349.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2236x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2236x2048): 90.346
Elapsed time for attention_prob_times_values (48x2048x2048x2236): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2236): 86.562

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2405.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2237x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2237x2048): 88.810
Elapsed time for attention_prob_times_values (48x2048x2048x2237): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2237): 83.938

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2348.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2238x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2238x2048): 89.959
Elapsed time for attention_prob_times_values (48x2048x2048x2238): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2238): 86.394

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2399.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2239x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2239x2048): 88.989
Elapsed time for attention_prob_times_values (48x2048x2048x2239): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2239): 84.222

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2357.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2240x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2240x2048): 98.265
Elapsed time for attention_prob_times_values (48x2048x2048x2240): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2240): 93.690

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2613.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2241x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2241x2048): 89.982
Elapsed time for attention_prob_times_values (48x2048x2048x2241): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2241): 84.200

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2371.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2242x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2242x2048): 90.808
Elapsed time for attention_prob_times_values (48x2048x2048x2242): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2242): 86.616

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2418.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2243x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2243x2048): 89.902
Elapsed time for attention_prob_times_values (48x2048x2048x2243): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2243): 84.275

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2373.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2244x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2244x2048): 91.454
Elapsed time for attention_prob_times_values (48x2048x2048x2244): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2244): 86.756

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2430.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2245x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2245x2048): 89.771
Elapsed time for attention_prob_times_values (48x2048x2048x2245): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2245): 84.422

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2376.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2246x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2246x2048): 90.503
Elapsed time for attention_prob_times_values (48x2048x2048x2246): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2246): 86.641

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2418.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2247x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2247x2048): 89.415
Elapsed time for attention_prob_times_values (48x2048x2048x2247): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2247): 84.354

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2372.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2248x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2248x2048): 91.337
Elapsed time for attention_prob_times_values (48x2048x2048x2248): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2248): 92.454

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2512.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 26988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2249x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2249x2048): 89.271
Elapsed time for attention_prob_times_values (48x2048x2048x2249): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2249): 84.501

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2375.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2250x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2250x2048): 90.101
Elapsed time for attention_prob_times_values (48x2048x2048x2250): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2250): 86.796

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2419.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2251x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2251x2048): 89.489
Elapsed time for attention_prob_times_values (48x2048x2048x2251): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2251): 84.533

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2380.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2252x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2252x2048): 90.890
Elapsed time for attention_prob_times_values (48x2048x2048x2252): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2252): 87.028

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2435.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2253x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2253x2048): 89.260
Elapsed time for attention_prob_times_values (48x2048x2048x2253): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2253): 84.844

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2383.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2254x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2254x2048): 90.158
Elapsed time for attention_prob_times_values (48x2048x2048x2254): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2254): 87.053

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2428.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2255x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2255x2048): 89.283
Elapsed time for attention_prob_times_values (48x2048x2048x2255): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2255): 85.057

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2389.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2256x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2256x2048): 91.583
Elapsed time for attention_prob_times_values (48x2048x2048x2256): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2256): 93.103

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2533.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2257x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2257x2048): 88.997
Elapsed time for attention_prob_times_values (48x2048x2048x2257): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2257): 85.138

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2388.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2258x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2258x2048): 89.975
Elapsed time for attention_prob_times_values (48x2048x2048x2258): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2258): 87.242

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2432.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2259x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2259x2048): 88.995
Elapsed time for attention_prob_times_values (48x2048x2048x2259): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2259): 85.003

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2388.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2260x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2260x2048): 90.555
Elapsed time for attention_prob_times_values (48x2048x2048x2260): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2260): 87.291

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2443.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2261x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2261x2048): 89.125
Elapsed time for attention_prob_times_values (48x2048x2048x2261): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2261): 85.111

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2394.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2262x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2262x2048): 89.984
Elapsed time for attention_prob_times_values (48x2048x2048x2262): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2262): 87.322

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2438.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2263x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2263x2048): 89.236
Elapsed time for attention_prob_times_values (48x2048x2048x2263): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2263): 85.066

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2396.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2264x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2264x2048): 91.080
Elapsed time for attention_prob_times_values (48x2048x2048x2264): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2264): 93.164

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2535.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2265x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2265x2048): 89.337
Elapsed time for attention_prob_times_values (48x2048x2048x2265): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2265): 85.387

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2404.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2266x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2266x2048): 90.043
Elapsed time for attention_prob_times_values (48x2048x2048x2266): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2266): 87.371

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2443.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2267x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2267x2048): 90.136
Elapsed time for attention_prob_times_values (48x2048x2048x2267): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2267): 85.240

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2415.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2268x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2268x2048): 90.899
Elapsed time for attention_prob_times_values (48x2048x2048x2268): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2268): 87.698

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2461.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2269x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2269x2048): 89.981
Elapsed time for attention_prob_times_values (48x2048x2048x2269): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2269): 85.260

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2415.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2270x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2270x2048): 90.005
Elapsed time for attention_prob_times_values (48x2048x2048x2270): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2270): 87.499

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2449.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2271x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2271x2048): 89.290
Elapsed time for attention_prob_times_values (48x2048x2048x2271): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2271): 85.363

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2410.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2272x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2272x2048): 98.422
Elapsed time for attention_prob_times_values (48x2048x2048x2272): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2272): 94.509

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2663.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2273x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2273x2048): 90.103
Elapsed time for attention_prob_times_values (48x2048x2048x2273): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2273): 85.460

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2424.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2274x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2274x2048): 90.905
Elapsed time for attention_prob_times_values (48x2048x2048x2274): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2274): 87.595

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2466.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2275x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2275x2048): 89.849
Elapsed time for attention_prob_times_values (48x2048x2048x2275): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2275): 85.377

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2421.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2276x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2276x2048): 91.430
Elapsed time for attention_prob_times_values (48x2048x2048x2276): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2276): 87.700

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2477.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2277x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2277x2048): 89.654
Elapsed time for attention_prob_times_values (48x2048x2048x2277): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2277): 85.297

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2420.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2278x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2278x2048): 90.527
Elapsed time for attention_prob_times_values (48x2048x2048x2278): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2278): 87.663

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2466.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2279x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2279x2048): 89.470
Elapsed time for attention_prob_times_values (48x2048x2048x2279): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2279): 85.307

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2419.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2280x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2280x2048): 91.452
Elapsed time for attention_prob_times_values (48x2048x2048x2280): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2280): 93.622

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2564.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2281x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2281x2048): 89.214
Elapsed time for attention_prob_times_values (48x2048x2048x2281): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2281): 85.305

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2418.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2282x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2282x2048): 90.284
Elapsed time for attention_prob_times_values (48x2048x2048x2282): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2282): 87.664

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2467.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2283x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2283x2048): 89.785
Elapsed time for attention_prob_times_values (48x2048x2048x2283): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2283): 85.281

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2427.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2284x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2284x2048): 90.943
Elapsed time for attention_prob_times_values (48x2048x2048x2284): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2284): 87.892

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2482.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2285x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2285x2048): 89.557
Elapsed time for attention_prob_times_values (48x2048x2048x2285): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2285): 85.331

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2427.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2286x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2286x2048): 90.402
Elapsed time for attention_prob_times_values (48x2048x2048x2286): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2286): 87.861

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2476.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2287x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2287x2048): 89.426
Elapsed time for attention_prob_times_values (48x2048x2048x2287): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2287): 85.353

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2428.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2288x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2288x2048): 91.797
Elapsed time for attention_prob_times_values (48x2048x2048x2288): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2288): 94.276

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2587.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2289x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2289x2048): 89.040
Elapsed time for attention_prob_times_values (48x2048x2048x2289): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2289): 85.445

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2426.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2290x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2290x2048): 90.050
Elapsed time for attention_prob_times_values (48x2048x2048x2290): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2290): 87.957

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2477.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2291x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2291x2048): 89.233
Elapsed time for attention_prob_times_values (48x2048x2048x2291): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2291): 85.508

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2431.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2292x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2292x2048): 90.495
Elapsed time for attention_prob_times_values (48x2048x2048x2292): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2292): 88.032

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2486.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2293x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2293x2048): 89.188
Elapsed time for attention_prob_times_values (48x2048x2048x2293): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2293): 85.549

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2433.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2294x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2294x2048): 89.968
Elapsed time for attention_prob_times_values (48x2048x2048x2294): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2294): 88.161

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2483.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2295x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2295x2048): 89.142
Elapsed time for attention_prob_times_values (48x2048x2048x2295): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2295): 85.612

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2436.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2296x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2296x2048): 90.953
Elapsed time for attention_prob_times_values (48x2048x2048x2296): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2296): 94.069

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2580.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2297x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2297x2048): 88.869
Elapsed time for attention_prob_times_values (48x2048x2048x2297): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2297): 85.751

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2436.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2298x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2298x2048): 89.702
Elapsed time for attention_prob_times_values (48x2048x2048x2298): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2298): 88.188

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2484.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2299x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2299x2048): 88.928
Elapsed time for attention_prob_times_values (48x2048x2048x2299): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2299): 85.612

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2437.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2300x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2300x2048): 90.321
Elapsed time for attention_prob_times_values (48x2048x2048x2300): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2300): 88.407

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2497.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2301x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2301x2048): 88.720
Elapsed time for attention_prob_times_values (48x2048x2048x2301): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2301): 85.787

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2439.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2302x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2302x2048): 89.724
Elapsed time for attention_prob_times_values (48x2048x2048x2302): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2302): 88.247

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2489.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2303x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2303x2048): 88.895
Elapsed time for attention_prob_times_values (48x2048x2048x2303): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2303): 85.651

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2441.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2304x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2304x2048): 96.753
Elapsed time for attention_prob_times_values (48x2048x2048x2304): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2304): 96.458

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2704.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2305x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2305x2048): 89.785
Elapsed time for attention_prob_times_values (48x2048x2048x2305): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2305): 82.050

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2401.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2306x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2306x2048): 90.719
Elapsed time for attention_prob_times_values (48x2048x2048x2306): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2306): 84.827

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2456.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2307x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2307x2048): 89.294
Elapsed time for attention_prob_times_values (48x2048x2048x2307): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2307): 82.476

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2404.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2308x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2308x2048): 91.381
Elapsed time for attention_prob_times_values (48x2048x2048x2308): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2308): 84.923

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2469.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2309x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2309x2048): 89.703
Elapsed time for attention_prob_times_values (48x2048x2048x2309): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2309): 82.467

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2411.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2310x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2310x2048): 90.617
Elapsed time for attention_prob_times_values (48x2048x2048x2310): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2310): 84.943

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2461.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2311x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2311x2048): 89.694
Elapsed time for attention_prob_times_values (48x2048x2048x2311): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2311): 82.602

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2415.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2312x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2312x2048): 91.415
Elapsed time for attention_prob_times_values (48x2048x2048x2312): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2312): 90.523

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2555.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2313x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2313x2048): 89.371
Elapsed time for attention_prob_times_values (48x2048x2048x2313): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2313): 82.691

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2414.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2314x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2314x2048): 90.137
Elapsed time for attention_prob_times_values (48x2048x2048x2314): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2314): 85.054

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2460.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2315x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2315x2048): 74.798
Elapsed time for attention_prob_times_values (48x2048x2048x2315): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2315): 82.764

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2210.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2316x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2316x2048): 90.667
Elapsed time for attention_prob_times_values (48x2048x2048x2316): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2316): 85.143

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2471.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2317x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2317x2048): 89.337
Elapsed time for attention_prob_times_values (48x2048x2048x2317): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2317): 82.837

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2420.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2318x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2318x2048): 90.258
Elapsed time for attention_prob_times_values (48x2048x2048x2318): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2318): 85.028

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2466.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2319x2048): 0.5045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2319x2048): 1.851
Elapsed time for attention_prob_times_values (48x2048x2048x2319): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2319): 82.953

Attention duration (in seconds): 0.5157
Attention throughput (in TFLOP/s): 102.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2320x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2320x2048): 91.871
Elapsed time for attention_prob_times_values (48x2048x2048x2320): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2320): 69.064

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2222.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2321x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2321x2048): 89.075
Elapsed time for attention_prob_times_values (48x2048x2048x2321): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2321): 83.051

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2423.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2322x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2322x2048): 90.030
Elapsed time for attention_prob_times_values (48x2048x2048x2322): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2322): 85.134

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2468.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2323x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2323x2048): 89.105
Elapsed time for attention_prob_times_values (48x2048x2048x2323): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2323): 83.045

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2426.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2324x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2324x2048): 90.556
Elapsed time for attention_prob_times_values (48x2048x2048x2324): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2324): 85.264

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2479.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2325x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2325x2048): 89.169
Elapsed time for attention_prob_times_values (48x2048x2048x2325): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2325): 83.072

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2429.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2326x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2326x2048): 90.247
Elapsed time for attention_prob_times_values (48x2048x2048x2326): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2326): 85.167

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2476.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2327x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2327x2048): 89.339
Elapsed time for attention_prob_times_values (48x2048x2048x2327): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2327): 83.123

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2434.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2328x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2328x2048): 91.300
Elapsed time for attention_prob_times_values (48x2048x2048x2328): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2328): 69.630

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2234.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2329x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2329x2048): 89.696
Elapsed time for attention_prob_times_values (48x2048x2048x2329): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2329): 83.161

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2441.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2330x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2330x2048): 90.013
Elapsed time for attention_prob_times_values (48x2048x2048x2330): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2330): 85.401

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2480.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2331x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2331x2048): 90.015
Elapsed time for attention_prob_times_values (48x2048x2048x2331): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2331): 83.193

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2448.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2332x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2332x2048): 90.876
Elapsed time for attention_prob_times_values (48x2048x2048x2332): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2332): 85.549

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2496.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 27996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2333x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2333x2048): 89.235
Elapsed time for attention_prob_times_values (48x2048x2048x2333): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2333): 83.326

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2442.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2334x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2334x2048): 90.228
Elapsed time for attention_prob_times_values (48x2048x2048x2334): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2334): 85.538

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2489.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2335x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2335x2048): 89.265
Elapsed time for attention_prob_times_values (48x2048x2048x2335): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2335): 83.392

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2445.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2336x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2336x2048): 98.447
Elapsed time for attention_prob_times_values (48x2048x2048x2336): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2336): 84.499

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2580.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2337x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2337x2048): 90.553
Elapsed time for attention_prob_times_values (48x2048x2048x2337): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2337): 83.540

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2466.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2338x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2338x2048): 90.944
Elapsed time for attention_prob_times_values (48x2048x2048x2338): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2338): 85.832

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2507.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2339x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2339x2048): 90.192
Elapsed time for attention_prob_times_values (48x2048x2048x2339): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2339): 83.504

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2463.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2340x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2340x2048): 91.486
Elapsed time for attention_prob_times_values (48x2048x2048x2340): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2340): 85.877

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2517.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2341x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2341x2048): 89.793
Elapsed time for attention_prob_times_values (48x2048x2048x2341): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2341): 83.453

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2459.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2342x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2342x2048): 90.960
Elapsed time for attention_prob_times_values (48x2048x2048x2342): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2342): 85.798

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2511.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2343x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2343x2048): 89.735
Elapsed time for attention_prob_times_values (48x2048x2048x2343): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2343): 83.519

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2461.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2344x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2344x2048): 91.665
Elapsed time for attention_prob_times_values (48x2048x2048x2344): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2344): 81.070

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2449.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2345x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2345x2048): 89.518
Elapsed time for attention_prob_times_values (48x2048x2048x2345): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2345): 83.578

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2462.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2346x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2346x2048): 90.330
Elapsed time for attention_prob_times_values (48x2048x2048x2346): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2346): 85.917

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2509.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2347x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2347x2048): 90.019
Elapsed time for attention_prob_times_values (48x2048x2048x2347): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2347): 83.621

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2471.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2348x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2348x2048): 91.295
Elapsed time for attention_prob_times_values (48x2048x2048x2348): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2348): 85.986

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2525.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2349x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2349x2048): 89.908
Elapsed time for attention_prob_times_values (48x2048x2048x2349): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2349): 83.604

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2471.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2350x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2350x2048): 91.930
Elapsed time for attention_prob_times_values (48x2048x2048x2350): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2350): 85.968

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2535.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2351x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2351x2048): 89.761
Elapsed time for attention_prob_times_values (48x2048x2048x2351): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2351): 83.568

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2471.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2352x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2352x2048): 92.849
Elapsed time for attention_prob_times_values (48x2048x2048x2352): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2352): 67.752

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2237.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2353x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2353x2048): 89.542
Elapsed time for attention_prob_times_values (48x2048x2048x2353): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2353): 83.800

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2473.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2354x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2354x2048): 90.487
Elapsed time for attention_prob_times_values (48x2048x2048x2354): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2354): 86.165

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2523.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2355x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2355x2048): 89.557
Elapsed time for attention_prob_times_values (48x2048x2048x2355): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2355): 83.896

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2477.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2356x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2356x2048): 91.126
Elapsed time for attention_prob_times_values (48x2048x2048x2356): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2356): 86.361

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2537.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2357x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2357x2048): 89.557
Elapsed time for attention_prob_times_values (48x2048x2048x2357): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2357): 83.956

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2480.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2358x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2358x2048): 90.456
Elapsed time for attention_prob_times_values (48x2048x2048x2358): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2358): 86.300

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2529.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2359x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2359x2048): 89.448
Elapsed time for attention_prob_times_values (48x2048x2048x2359): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2359): 84.031

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2482.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2360x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2360x2048): 91.474
Elapsed time for attention_prob_times_values (48x2048x2048x2360): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2360): 67.586

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2227.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2361x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2361x2048): 89.261
Elapsed time for attention_prob_times_values (48x2048x2048x2361): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2361): 84.040

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2481.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2362x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2362x2048): 90.126
Elapsed time for attention_prob_times_values (48x2048x2048x2362): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2362): 86.563

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2532.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2363x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2363x2048): 89.307
Elapsed time for attention_prob_times_values (48x2048x2048x2363): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2363): 84.245

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2487.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2364x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2364x2048): 90.942
Elapsed time for attention_prob_times_values (48x2048x2048x2364): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2364): 86.675

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2547.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2365x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2365x2048): 89.464
Elapsed time for attention_prob_times_values (48x2048x2048x2365): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2365): 84.264

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2492.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2366x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2366x2048): 90.487
Elapsed time for attention_prob_times_values (48x2048x2048x2366): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2366): 86.822

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2545.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2367x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2367x2048): 89.606
Elapsed time for attention_prob_times_values (48x2048x2048x2367): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2367): 84.368

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2497.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2368x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2368x2048): 98.699
Elapsed time for attention_prob_times_values (48x2048x2048x2368): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2368): 88.684

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2685.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2369x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2369x2048): 90.528
Elapsed time for attention_prob_times_values (48x2048x2048x2369): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2369): 84.281

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2510.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2370x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2370x2048): 91.527
Elapsed time for attention_prob_times_values (48x2048x2048x2370): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2370): 86.877

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2564.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2371x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2371x2048): 90.412
Elapsed time for attention_prob_times_values (48x2048x2048x2371): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2371): 84.578

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2515.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2372x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2372x2048): 92.002
Elapsed time for attention_prob_times_values (48x2048x2048x2372): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2372): 86.922

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2574.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2373x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2373x2048): 90.276
Elapsed time for attention_prob_times_values (48x2048x2048x2373): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2373): 84.371

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2512.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2374x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2374x2048): 91.429
Elapsed time for attention_prob_times_values (48x2048x2048x2374): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2374): 86.950

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2568.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2375x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2375x2048): 90.192
Elapsed time for attention_prob_times_values (48x2048x2048x2375): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2375): 84.658

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2518.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2376x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2376x2048): 91.990
Elapsed time for attention_prob_times_values (48x2048x2048x2376): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2376): 65.668

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2210.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2377x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2377x2048): 89.755
Elapsed time for attention_prob_times_values (48x2048x2048x2377): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2377): 84.742

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2515.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2378x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2378x2048): 90.700
Elapsed time for attention_prob_times_values (48x2048x2048x2378): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2378): 86.998

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2563.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2379x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2379x2048): 90.143
Elapsed time for attention_prob_times_values (48x2048x2048x2379): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2379): 84.769

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2523.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2380x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2380x2048): 91.699
Elapsed time for attention_prob_times_values (48x2048x2048x2380): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2380): 87.222

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2582.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2381x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2381x2048): 89.838
Elapsed time for attention_prob_times_values (48x2048x2048x2381): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2381): 84.677

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2519.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2382x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2382x2048): 90.752
Elapsed time for attention_prob_times_values (48x2048x2048x2382): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2382): 87.137

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2570.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2383x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2383x2048): 89.859
Elapsed time for attention_prob_times_values (48x2048x2048x2383): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2383): 84.759

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2523.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2384x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2384x2048): 92.116
Elapsed time for attention_prob_times_values (48x2048x2048x2384): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2384): 69.785

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2297.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2385x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2385x2048): 89.483
Elapsed time for attention_prob_times_values (48x2048x2048x2385): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2385): 84.909

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2522.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2386x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2386x2048): 90.642
Elapsed time for attention_prob_times_values (48x2048x2048x2386): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2386): 87.277

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2575.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2387x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2387x2048): 89.536
Elapsed time for attention_prob_times_values (48x2048x2048x2387): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2387): 84.946

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2525.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2388x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2388x2048): 91.067
Elapsed time for attention_prob_times_values (48x2048x2048x2388): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2388): 87.390

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2585.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2389x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2389x2048): 89.599
Elapsed time for attention_prob_times_values (48x2048x2048x2389): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2389): 85.238

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2533.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2390x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2390x2048): 90.392
Elapsed time for attention_prob_times_values (48x2048x2048x2390): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2390): 87.271

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2576.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2391x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2391x2048): 89.555
Elapsed time for attention_prob_times_values (48x2048x2048x2391): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2391): 84.963

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2530.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2392x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2392x2048): 91.720
Elapsed time for attention_prob_times_values (48x2048x2048x2392): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2392): 68.234

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2271.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2393x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2393x2048): 89.436
Elapsed time for attention_prob_times_values (48x2048x2048x2393): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2393): 85.314

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2536.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2394x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2394x2048): 90.095
Elapsed time for attention_prob_times_values (48x2048x2048x2394): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2394): 87.422

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2578.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2395x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2395x2048): 89.338
Elapsed time for attention_prob_times_values (48x2048x2048x2395): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2395): 85.075

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2533.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2396x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2396x2048): 90.879
Elapsed time for attention_prob_times_values (48x2048x2048x2396): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2396): 87.463

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2591.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2397x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2397x2048): 89.267
Elapsed time for attention_prob_times_values (48x2048x2048x2397): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2397): 85.226

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2536.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2398x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2398x2048): 90.278
Elapsed time for attention_prob_times_values (48x2048x2048x2398): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2398): 87.493

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2586.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2399x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2399x2048): 90.231
Elapsed time for attention_prob_times_values (48x2048x2048x2399): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2399): 85.059

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2549.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2400x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2400x2048): 98.511
Elapsed time for attention_prob_times_values (48x2048x2048x2400): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2400): 86.429

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2681.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2401x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2401x2048): 90.415
Elapsed time for attention_prob_times_values (48x2048x2048x2401): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2401): 85.191

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2556.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2402x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2402x2048): 91.244
Elapsed time for attention_prob_times_values (48x2048x2048x2402): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2402): 87.641

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2606.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2403x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2403x2048): 90.201
Elapsed time for attention_prob_times_values (48x2048x2048x2403): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2403): 85.458

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2559.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2404x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2404x2048): 91.797
Elapsed time for attention_prob_times_values (48x2048x2048x2404): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2404): 87.657

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2616.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2405x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2405x2048): 90.069
Elapsed time for attention_prob_times_values (48x2048x2048x2405): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2405): 85.226

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2555.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2406x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2406x2048): 90.887
Elapsed time for attention_prob_times_values (48x2048x2048x2406): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2406): 87.592

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2604.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2407x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2407x2048): 89.814
Elapsed time for attention_prob_times_values (48x2048x2048x2407): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2407): 85.291

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2555.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2408x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2408x2048): 91.649
Elapsed time for attention_prob_times_values (48x2048x2048x2408): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2408): 67.467

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2270.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2409x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2409x2048): 89.440
Elapsed time for attention_prob_times_values (48x2048x2048x2409): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2409): 85.228

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2551.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2410x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2410x2048): 90.473
Elapsed time for attention_prob_times_values (48x2048x2048x2410): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2410): 87.709

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2604.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2411x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2411x2048): 89.707
Elapsed time for attention_prob_times_values (48x2048x2048x2411): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2411): 85.267

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2557.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2412x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2412x2048): 91.028
Elapsed time for attention_prob_times_values (48x2048x2048x2412): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2412): 87.984

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2618.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2413x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2413x2048): 89.563
Elapsed time for attention_prob_times_values (48x2048x2048x2413): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2413): 85.250

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2557.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2414x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2414x2048): 90.559
Elapsed time for attention_prob_times_values (48x2048x2048x2414): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2414): 87.877

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2612.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2415x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2415x2048): 89.665
Elapsed time for attention_prob_times_values (48x2048x2048x2415): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2415): 85.388

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2563.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2416x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2416x2048): 92.180
Elapsed time for attention_prob_times_values (48x2048x2048x2416): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2416): 77.492

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2468.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2417x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2417x2048): 89.454
Elapsed time for attention_prob_times_values (48x2048x2048x2417): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2417): 85.346

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2561.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2418x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2418x2048): 90.665
Elapsed time for attention_prob_times_values (48x2048x2048x2418): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2418): 87.979

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2619.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2419x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2419x2048): 89.504
Elapsed time for attention_prob_times_values (48x2048x2048x2419): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2419): 85.503

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2566.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2420x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2420x2048): 90.855
Elapsed time for attention_prob_times_values (48x2048x2048x2420): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2420): 88.134

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2626.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2421x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2421x2048): 89.621
Elapsed time for attention_prob_times_values (48x2048x2048x2421): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2421): 85.556

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2571.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2422x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2422x2048): 90.358
Elapsed time for attention_prob_times_values (48x2048x2048x2422): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2422): 88.087

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2621.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2423x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2423x2048): 89.523
Elapsed time for attention_prob_times_values (48x2048x2048x2423): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2423): 85.560

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2571.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2424x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2424x2048): 91.391
Elapsed time for attention_prob_times_values (48x2048x2048x2424): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2424): 68.022

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2293.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2425x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2425x2048): 89.232
Elapsed time for attention_prob_times_values (48x2048x2048x2425): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2425): 85.736

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2572.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2426x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2426x2048): 90.123
Elapsed time for attention_prob_times_values (48x2048x2048x2426): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2426): 88.354

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2625.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2427x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2427x2048): 89.362
Elapsed time for attention_prob_times_values (48x2048x2048x2427): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2427): 85.762

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2576.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2428x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2428x2048): 90.790
Elapsed time for attention_prob_times_values (48x2048x2048x2428): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2428): 88.417

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2638.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2429x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2429x2048): 89.061
Elapsed time for attention_prob_times_values (48x2048x2048x2429): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2429): 85.964

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2577.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2430x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2430x2048): 90.390
Elapsed time for attention_prob_times_values (48x2048x2048x2430): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2430): 88.469

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2635.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2431x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2431x2048): 89.482
Elapsed time for attention_prob_times_values (48x2048x2048x2431): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2431): 85.963

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2585.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2432x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2432x2048): 97.172
Elapsed time for attention_prob_times_values (48x2048x2048x2432): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2432): 91.340

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2777.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2433x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2433x2048): 90.375
Elapsed time for attention_prob_times_values (48x2048x2048x2433): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2433): 82.536

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2546.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2434x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2434x2048): 91.336
Elapsed time for attention_prob_times_values (48x2048x2048x2434): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2434): 85.352

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2605.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2435x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2435x2048): 89.860
Elapsed time for attention_prob_times_values (48x2048x2048x2435): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2435): 82.969

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2548.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2436x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2436x2048): 92.167
Elapsed time for attention_prob_times_values (48x2048x2048x2436): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2436): 85.502

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2621.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2437x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2437x2048): 91.165
Elapsed time for attention_prob_times_values (48x2048x2048x2437): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2437): 82.968

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2567.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2438x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2438x2048): 91.247
Elapsed time for attention_prob_times_values (48x2048x2048x2438): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2438): 85.411

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2609.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2439x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2439x2048): 90.118
Elapsed time for attention_prob_times_values (48x2048x2048x2439): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2439): 83.006

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2556.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2440x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2440x2048): 92.388
Elapsed time for attention_prob_times_values (48x2048x2048x2440): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2440): 84.399

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2610.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2441x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2441x2048): 90.237
Elapsed time for attention_prob_times_values (48x2048x2048x2441): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2441): 83.175

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2562.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2442x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2442x2048): 90.703
Elapsed time for attention_prob_times_values (48x2048x2048x2442): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2442): 85.541

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2607.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2443x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2443x2048): 89.890
Elapsed time for attention_prob_times_values (48x2048x2048x2443): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2443): 83.245

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2561.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2444x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2444x2048): 91.725
Elapsed time for attention_prob_times_values (48x2048x2048x2444): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2444): 85.707

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2626.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2445x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2445x2048): 90.109
Elapsed time for attention_prob_times_values (48x2048x2048x2445): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2445): 83.189

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2565.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2446x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2446x2048): 90.909
Elapsed time for attention_prob_times_values (48x2048x2048x2446): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2446): 85.679

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2616.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2447x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2447x2048): 90.188
Elapsed time for attention_prob_times_values (48x2048x2048x2447): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2447): 83.421

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2572.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2448x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2448x2048): 93.405
Elapsed time for attention_prob_times_values (48x2048x2048x2448): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2448): 84.777

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2638.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2449x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2449x2048): 90.070
Elapsed time for attention_prob_times_values (48x2048x2048x2449): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2449): 83.523

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2574.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2450x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2450x2048): 90.687
Elapsed time for attention_prob_times_values (48x2048x2048x2450): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2450): 85.787

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2619.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2451x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2451x2048): 90.002
Elapsed time for attention_prob_times_values (48x2048x2048x2451): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2451): 83.492

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2574.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2452x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2452x2048): 92.685
Elapsed time for attention_prob_times_values (48x2048x2048x2452): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2452): 85.864

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2650.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2453x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2453x2048): 89.876
Elapsed time for attention_prob_times_values (48x2048x2048x2453): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2453): 83.537

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2575.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2454x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2454x2048): 90.656
Elapsed time for attention_prob_times_values (48x2048x2048x2454): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2454): 85.843

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2624.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2455x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2455x2048): 89.856
Elapsed time for attention_prob_times_values (48x2048x2048x2455): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2455): 83.642

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2579.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2456x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2456x2048): 91.949
Elapsed time for attention_prob_times_values (48x2048x2048x2456): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2456): 84.510

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2622.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2457x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2457x2048): 89.485
Elapsed time for attention_prob_times_values (48x2048x2048x2457): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2457): 83.683

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2576.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2458x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2458x2048): 90.306
Elapsed time for attention_prob_times_values (48x2048x2048x2458): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2458): 85.992

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2625.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2459x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2459x2048): 89.608
Elapsed time for attention_prob_times_values (48x2048x2048x2459): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2459): 83.658

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2580.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2460x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2460x2048): 91.145
Elapsed time for attention_prob_times_values (48x2048x2048x2460): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2460): 86.118

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2641.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2461x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2461x2048): 89.588
Elapsed time for attention_prob_times_values (48x2048x2048x2461): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2461): 83.794

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2583.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2462x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2462x2048): 90.621
Elapsed time for attention_prob_times_values (48x2048x2048x2462): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2462): 86.100

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2635.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2463x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2463x2048): 91.038
Elapsed time for attention_prob_times_values (48x2048x2048x2463): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2463): 83.860

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2607.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2464x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2464x2048): 98.825
Elapsed time for attention_prob_times_values (48x2048x2048x2464): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2464): 86.739

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2760.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2465x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2465x2048): 91.096
Elapsed time for attention_prob_times_values (48x2048x2048x2465): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2465): 83.881

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2610.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2466x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2466x2048): 91.887
Elapsed time for attention_prob_times_values (48x2048x2048x2466): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2466): 86.135

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2658.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2467x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2467x2048): 92.036
Elapsed time for attention_prob_times_values (48x2048x2048x2467): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2467): 83.794

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2623.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2468x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2468x2048): 92.973
Elapsed time for attention_prob_times_values (48x2048x2048x2468): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2468): 86.272

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2677.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2469x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2469x2048): 90.192
Elapsed time for attention_prob_times_values (48x2048x2048x2469): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2469): 83.816

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2600.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2470x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2470x2048): 91.257
Elapsed time for attention_prob_times_values (48x2048x2048x2470): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2470): 86.178

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2654.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2471x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2471x2048): 90.685
Elapsed time for attention_prob_times_values (48x2048x2048x2471): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2471): 83.717

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2608.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2472x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2472x2048): 92.009
Elapsed time for attention_prob_times_values (48x2048x2048x2472): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2472): 84.856

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2645.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2473x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2473x2048): 89.735
Elapsed time for attention_prob_times_values (48x2048x2048x2473): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2473): 83.836

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2598.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2474x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2474x2048): 90.644
Elapsed time for attention_prob_times_values (48x2048x2048x2474): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2474): 86.321

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2652.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2475x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2475x2048): 89.946
Elapsed time for attention_prob_times_values (48x2048x2048x2475): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2475): 83.821

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2603.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2476x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2476x2048): 91.481
Elapsed time for attention_prob_times_values (48x2048x2048x2476): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2476): 86.459

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2668.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2477x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2477x2048): 89.841
Elapsed time for attention_prob_times_values (48x2048x2048x2477): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2477): 83.858

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2604.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2478x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2478x2048): 90.916
Elapsed time for attention_prob_times_values (48x2048x2048x2478): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2478): 86.356

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2660.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2479x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2479x2048): 90.366
Elapsed time for attention_prob_times_values (48x2048x2048x2479): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2479): 83.882

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2614.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2480x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2480x2048): 92.214
Elapsed time for attention_prob_times_values (48x2048x2048x2480): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2480): 85.915

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2674.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2481x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2481x2048): 89.546
Elapsed time for attention_prob_times_values (48x2048x2048x2481): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2481): 84.104

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2608.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2482x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2482x2048): 90.763
Elapsed time for attention_prob_times_values (48x2048x2048x2482): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2482): 86.499

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2665.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2483x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2483x2048): 89.739
Elapsed time for attention_prob_times_values (48x2048x2048x2483): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2483): 84.055

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2612.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2484x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2484x2048): 91.257
Elapsed time for attention_prob_times_values (48x2048x2048x2484): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2484): 86.642

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2676.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2485x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2485x2048): 89.761
Elapsed time for attention_prob_times_values (48x2048x2048x2485): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2485): 84.160

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2616.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2486x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2486x2048): 90.719
Elapsed time for attention_prob_times_values (48x2048x2048x2486): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2486): 86.601

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2670.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2487x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2487x2048): 89.751
Elapsed time for attention_prob_times_values (48x2048x2048x2487): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2487): 84.200

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2619.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2488x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2488x2048): 91.632
Elapsed time for attention_prob_times_values (48x2048x2048x2488): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2488): 85.688

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2670.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2489x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2489x2048): 89.475
Elapsed time for attention_prob_times_values (48x2048x2048x2489): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2489): 84.293

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2618.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2490x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2490x2048): 90.398
Elapsed time for attention_prob_times_values (48x2048x2048x2490): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2490): 86.799

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2672.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2491x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2491x2048): 89.555
Elapsed time for attention_prob_times_values (48x2048x2048x2491): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2491): 84.412

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2623.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2492x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2492x2048): 91.204
Elapsed time for attention_prob_times_values (48x2048x2048x2492): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2492): 86.996

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2689.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2493x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2493x2048): 89.615
Elapsed time for attention_prob_times_values (48x2048x2048x2493): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2493): 84.475

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2627.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2494x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2494x2048): 90.694
Elapsed time for attention_prob_times_values (48x2048x2048x2494): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2494): 86.969

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2683.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2495x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2495x2048): 89.905
Elapsed time for attention_prob_times_values (48x2048x2048x2495): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2495): 84.588

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2635.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2496x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2496x2048): 98.908
Elapsed time for attention_prob_times_values (48x2048x2048x2496): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2496): 89.496

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2842.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2497x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2497x2048): 90.695
Elapsed time for attention_prob_times_values (48x2048x2048x2497): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2497): 84.536

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2648.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2498x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2498x2048): 91.652
Elapsed time for attention_prob_times_values (48x2048x2048x2498): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2498): 87.193

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2705.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 29988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2499x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2499x2048): 90.645
Elapsed time for attention_prob_times_values (48x2048x2048x2499): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2499): 84.693

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2652.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2500x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2500x2048): 92.174
Elapsed time for attention_prob_times_values (48x2048x2048x2500): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2500): 87.310

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2716.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2501x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2501x2048): 90.424
Elapsed time for attention_prob_times_values (48x2048x2048x2501): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2501): 84.737

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2651.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2502x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2502x2048): 91.356
Elapsed time for attention_prob_times_values (48x2048x2048x2502): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2502): 87.237

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2706.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2503x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2503x2048): 90.344
Elapsed time for attention_prob_times_values (48x2048x2048x2503): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2503): 84.900

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2655.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2504x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2504x2048): 92.175
Elapsed time for attention_prob_times_values (48x2048x2048x2504): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2504): 86.489

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2707.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2505x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2505x2048): 89.999
Elapsed time for attention_prob_times_values (48x2048x2048x2505): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2505): 85.031

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2654.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2506x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2506x2048): 91.036
Elapsed time for attention_prob_times_values (48x2048x2048x2506): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2506): 87.371

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2707.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2507x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2507x2048): 90.238
Elapsed time for attention_prob_times_values (48x2048x2048x2507): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2507): 84.990

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2659.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2508x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2508x2048): 91.651
Elapsed time for attention_prob_times_values (48x2048x2048x2508): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2508): 87.502

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2720.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2509x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2509x2048): 90.165
Elapsed time for attention_prob_times_values (48x2048x2048x2509): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2509): 85.005

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2660.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2510x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2510x2048): 91.177
Elapsed time for attention_prob_times_values (48x2048x2048x2510): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2510): 87.498

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2715.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2511x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2511x2048): 90.497
Elapsed time for attention_prob_times_values (48x2048x2048x2511): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2511): 85.228

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2670.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2512x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2512x2048): 92.610
Elapsed time for attention_prob_times_values (48x2048x2048x2512): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2512): 87.288

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2735.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2513x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2513x2048): 89.911
Elapsed time for attention_prob_times_values (48x2048x2048x2513): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2513): 85.404

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2667.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2514x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2514x2048): 90.861
Elapsed time for attention_prob_times_values (48x2048x2048x2514): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2514): 87.617

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2717.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2515x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2515x2048): 89.888
Elapsed time for attention_prob_times_values (48x2048x2048x2515): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2515): 85.361

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2668.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2516x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2516x2048): 91.421
Elapsed time for attention_prob_times_values (48x2048x2048x2516): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2516): 87.772

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2730.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2517x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2517x2048): 89.936
Elapsed time for attention_prob_times_values (48x2048x2048x2517): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2517): 85.580

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2674.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2518x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2518x2048): 90.835
Elapsed time for attention_prob_times_values (48x2048x2048x2518): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2518): 87.706

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2722.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2519x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2519x2048): 89.911
Elapsed time for attention_prob_times_values (48x2048x2048x2519): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2519): 85.558

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2675.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2520x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2520x2048): 91.932
Elapsed time for attention_prob_times_values (48x2048x2048x2520): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2520): 87.234

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2733.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2521x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2521x2048): 89.720
Elapsed time for attention_prob_times_values (48x2048x2048x2521): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2521): 85.532

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2674.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2522x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2522x2048): 90.633
Elapsed time for attention_prob_times_values (48x2048x2048x2522): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2522): 87.909

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2727.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2523x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2523x2048): 89.822
Elapsed time for attention_prob_times_values (48x2048x2048x2523): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2523): 85.723

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2681.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2524x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2524x2048): 91.418
Elapsed time for attention_prob_times_values (48x2048x2048x2524): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2524): 88.101

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2743.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2525x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2525x2048): 89.791
Elapsed time for attention_prob_times_values (48x2048x2048x2525): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2525): 85.800

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2684.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2526x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2526x2048): 90.749
Elapsed time for attention_prob_times_values (48x2048x2048x2526): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2526): 88.090

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2735.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2527x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2527x2048): 89.985
Elapsed time for attention_prob_times_values (48x2048x2048x2527): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2527): 85.818

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2689.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2528x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2528x2048): 99.127
Elapsed time for attention_prob_times_values (48x2048x2048x2528): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2528): 89.331

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2877.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2529x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2529x2048): 91.041
Elapsed time for attention_prob_times_values (48x2048x2048x2529): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2529): 85.874

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2707.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2530x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2530x2048): 92.072
Elapsed time for attention_prob_times_values (48x2048x2048x2530): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2530): 88.171

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2760.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2531x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2531x2048): 91.070
Elapsed time for attention_prob_times_values (48x2048x2048x2531): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2531): 85.842

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2709.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2532x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2532x2048): 92.276
Elapsed time for attention_prob_times_values (48x2048x2048x2532): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2532): 88.309

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2768.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2533x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2533x2048): 90.525
Elapsed time for attention_prob_times_values (48x2048x2048x2533): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2533): 85.809

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2703.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2534x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2534x2048): 91.567
Elapsed time for attention_prob_times_values (48x2048x2048x2534): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2534): 88.271

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2759.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2535x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2535x2048): 90.539
Elapsed time for attention_prob_times_values (48x2048x2048x2535): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2535): 85.881

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2706.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2536x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2536x2048): 92.354
Elapsed time for attention_prob_times_values (48x2048x2048x2536): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2536): 87.840

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2765.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2537x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2537x2048): 90.189
Elapsed time for attention_prob_times_values (48x2048x2048x2537): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2537): 85.931

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2704.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2538x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2538x2048): 90.985
Elapsed time for attention_prob_times_values (48x2048x2048x2538): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2538): 88.365

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2756.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2539x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2539x2048): 90.253
Elapsed time for attention_prob_times_values (48x2048x2048x2539): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2539): 85.896

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2706.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2540x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2540x2048): 91.711
Elapsed time for attention_prob_times_values (48x2048x2048x2540): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2540): 88.474

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2770.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2541x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2541x2048): 90.267
Elapsed time for attention_prob_times_values (48x2048x2048x2541): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2541): 85.979

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2710.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2542x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2542x2048): 91.200
Elapsed time for attention_prob_times_values (48x2048x2048x2542): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2542): 88.302

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2762.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2543x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2543x2048): 90.271
Elapsed time for attention_prob_times_values (48x2048x2048x2543): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2543): 85.912

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2711.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2544x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2544x2048): 92.711
Elapsed time for attention_prob_times_values (48x2048x2048x2544): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2544): 88.606

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2791.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2545x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2545x2048): 89.985
Elapsed time for attention_prob_times_values (48x2048x2048x2545): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2545): 86.064

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2711.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2546x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2546x2048): 91.024
Elapsed time for attention_prob_times_values (48x2048x2048x2546): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2546): 88.543

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2768.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2547x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2547x2048): 90.057
Elapsed time for attention_prob_times_values (48x2048x2048x2547): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2547): 86.082

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2715.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2548x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2548x2048): 91.546
Elapsed time for attention_prob_times_values (48x2048x2048x2548): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2548): 88.476

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2776.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2549x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2549x2048): 90.363
Elapsed time for attention_prob_times_values (48x2048x2048x2549): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2549): 86.051

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2721.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2550x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2550x2048): 91.450
Elapsed time for attention_prob_times_values (48x2048x2048x2550): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2550): 88.314

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2774.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2551x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2551x2048): 90.612
Elapsed time for attention_prob_times_values (48x2048x2048x2551): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2551): 86.105

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2728.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2552x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2552x2048): 91.949
Elapsed time for attention_prob_times_values (48x2048x2048x2552): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2552): 87.904

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2777.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2553x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2553x2048): 89.813
Elapsed time for attention_prob_times_values (48x2048x2048x2553): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2553): 86.159

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2719.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2554x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2554x2048): 90.719
Elapsed time for attention_prob_times_values (48x2048x2048x2554): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2554): 88.753

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2775.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2555x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2555x2048): 89.793
Elapsed time for attention_prob_times_values (48x2048x2048x2555): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2555): 86.178

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2721.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2556x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2556x2048): 91.365
Elapsed time for attention_prob_times_values (48x2048x2048x2556): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2556): 88.741

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2786.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2557x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2557x2048): 89.554
Elapsed time for attention_prob_times_values (48x2048x2048x2557): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2557): 86.326

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2722.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2558x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2558x2048): 90.809
Elapsed time for attention_prob_times_values (48x2048x2048x2558): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2558): 88.854

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2782.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2559x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2559x2048): 89.982
Elapsed time for attention_prob_times_values (48x2048x2048x2559): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2559): 86.390

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2731.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2560x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2560x2048): 98.033
Elapsed time for attention_prob_times_values (48x2048x2048x2560): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2560): 91.945

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2941.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2561x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2561x2048): 89.006
Elapsed time for attention_prob_times_values (48x2048x2048x2561): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2561): 82.602

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2657.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2562x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2562x2048): 90.913
Elapsed time for attention_prob_times_values (48x2048x2048x2562): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2562): 85.214

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2729.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2563x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2563x2048): 89.086
Elapsed time for attention_prob_times_values (48x2048x2048x2563): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2563): 82.935

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2665.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2564x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2564x2048): 90.769
Elapsed time for attention_prob_times_values (48x2048x2048x2564): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2564): 85.530

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2734.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2565x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2565x2048): 89.068
Elapsed time for attention_prob_times_values (48x2048x2048x2565): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2565): 82.917

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2667.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2566x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2566x2048): 90.906
Elapsed time for attention_prob_times_values (48x2048x2048x2566): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2566): 85.262

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2733.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2567x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2567x2048): 89.013
Elapsed time for attention_prob_times_values (48x2048x2048x2567): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2567): 83.037

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2670.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2568x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2568x2048): 92.178
Elapsed time for attention_prob_times_values (48x2048x2048x2568): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2568): 83.442

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2723.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2569x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2569x2048): 89.216
Elapsed time for attention_prob_times_values (48x2048x2048x2569): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2569): 83.091

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2676.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2570x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2570x2048): 92.115
Elapsed time for attention_prob_times_values (48x2048x2048x2570): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2570): 85.351

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2757.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2571x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2571x2048): 90.129
Elapsed time for attention_prob_times_values (48x2048x2048x2571): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2571): 83.211

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2693.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2572x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2572x2048): 91.266
Elapsed time for attention_prob_times_values (48x2048x2048x2572): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2572): 85.673

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2752.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2573x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2573x2048): 89.033
Elapsed time for attention_prob_times_values (48x2048x2048x2573): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2573): 83.250

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2680.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2574x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2574x2048): 90.714
Elapsed time for attention_prob_times_values (48x2048x2048x2574): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2574): 85.628

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2745.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2575x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2575x2048): 88.905
Elapsed time for attention_prob_times_values (48x2048x2048x2575): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2575): 83.355

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2682.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2576x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2576x2048): 93.040
Elapsed time for attention_prob_times_values (48x2048x2048x2576): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2576): 84.594

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2763.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2577x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2577x2048): 88.442
Elapsed time for attention_prob_times_values (48x2048x2048x2577): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2577): 83.471

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2679.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2578x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2578x2048): 90.142
Elapsed time for attention_prob_times_values (48x2048x2048x2578): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2578): 85.811

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2744.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2579x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2579x2048): 88.531
Elapsed time for attention_prob_times_values (48x2048x2048x2579): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2579): 83.500

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2683.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2580x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2580x2048): 90.615
Elapsed time for attention_prob_times_values (48x2048x2048x2580): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2580): 85.631

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2750.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2581x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2581x2048): 88.379
Elapsed time for attention_prob_times_values (48x2048x2048x2581): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2581): 83.499

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2683.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2582x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2582x2048): 90.144
Elapsed time for attention_prob_times_values (48x2048x2048x2582): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2582): 85.819

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2748.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 30996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2583x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2583x2048): 88.659
Elapsed time for attention_prob_times_values (48x2048x2048x2583): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2583): 83.590

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2690.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2584x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2584x2048): 91.479
Elapsed time for attention_prob_times_values (48x2048x2048x2584): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2584): 83.813

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2736.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2585x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2585x2048): 88.016
Elapsed time for attention_prob_times_values (48x2048x2048x2585): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2585): 83.693

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2684.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2586x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2586x2048): 89.642
Elapsed time for attention_prob_times_values (48x2048x2048x2586): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2586): 85.945

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2747.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2587x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2587x2048): 88.068
Elapsed time for attention_prob_times_values (48x2048x2048x2587): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2587): 83.702

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2687.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2588x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2588x2048): 90.305
Elapsed time for attention_prob_times_values (48x2048x2048x2588): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2588): 86.128

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2762.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2589x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2589x2048): 88.012
Elapsed time for attention_prob_times_values (48x2048x2048x2589): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2589): 83.795

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2690.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2590x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2590x2048): 89.615
Elapsed time for attention_prob_times_values (48x2048x2048x2590): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2590): 85.833

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2749.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2591x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2591x2048): 88.281
Elapsed time for attention_prob_times_values (48x2048x2048x2591): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2591): 83.886

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2698.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2592x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2592x2048): 99.658
Elapsed time for attention_prob_times_values (48x2048x2048x2592): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2592): 86.294

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2902.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2593x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2593x2048): 89.900
Elapsed time for attention_prob_times_values (48x2048x2048x2593): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2593): 83.970

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2725.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2594x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2594x2048): 91.461
Elapsed time for attention_prob_times_values (48x2048x2048x2594): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2594): 86.165

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2786.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2595x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2595x2048): 89.814
Elapsed time for attention_prob_times_values (48x2048x2048x2595): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2595): 83.915

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2725.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2596x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2596x2048): 91.915
Elapsed time for attention_prob_times_values (48x2048x2048x2596): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2596): 86.406

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2798.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2597x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2597x2048): 89.374
Elapsed time for attention_prob_times_values (48x2048x2048x2597): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2597): 83.967

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2721.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2598x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2598x2048): 91.109
Elapsed time for attention_prob_times_values (48x2048x2048x2598): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2598): 86.240

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2786.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2599x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2599x2048): 89.357
Elapsed time for attention_prob_times_values (48x2048x2048x2599): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2599): 83.998

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2724.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2600x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2600x2048): 92.427
Elapsed time for attention_prob_times_values (48x2048x2048x2600): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2600): 84.675

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2781.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2601x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2601x2048): 89.217
Elapsed time for attention_prob_times_values (48x2048x2048x2601): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2601): 84.010

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2724.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2602x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2602x2048): 90.520
Elapsed time for attention_prob_times_values (48x2048x2048x2602): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2602): 86.517

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2786.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2603x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2603x2048): 89.045
Elapsed time for attention_prob_times_values (48x2048x2048x2603): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2603): 84.063

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2724.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2604x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2604x2048): 91.123
Elapsed time for attention_prob_times_values (48x2048x2048x2604): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2604): 86.492

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2796.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2605x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2605x2048): 89.070
Elapsed time for attention_prob_times_values (48x2048x2048x2605): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2605): 84.176

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2728.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2606x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2606x2048): 90.766
Elapsed time for attention_prob_times_values (48x2048x2048x2606): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2606): 86.545

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2794.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2607x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2607x2048): 89.022
Elapsed time for attention_prob_times_values (48x2048x2048x2607): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2607): 84.179

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2730.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2608x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2608x2048): 93.336
Elapsed time for attention_prob_times_values (48x2048x2048x2608): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2608): 85.684

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2820.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2609x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2609x2048): 88.694
Elapsed time for attention_prob_times_values (48x2048x2048x2609): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2609): 84.270

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2728.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2610x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2610x2048): 90.420
Elapsed time for attention_prob_times_values (48x2048x2048x2610): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2610): 86.643

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2795.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2611x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2611x2048): 88.765
Elapsed time for attention_prob_times_values (48x2048x2048x2611): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2611): 84.394

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2733.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2612x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2612x2048): 91.234
Elapsed time for attention_prob_times_values (48x2048x2048x2612): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2612): 86.773

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2811.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2613x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2613x2048): 88.619
Elapsed time for attention_prob_times_values (48x2048x2048x2613): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2613): 84.350

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2733.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2614x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2614x2048): 90.253
Elapsed time for attention_prob_times_values (48x2048x2048x2614): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2614): 86.601

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2796.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2615x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2615x2048): 88.805
Elapsed time for attention_prob_times_values (48x2048x2048x2615): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2615): 84.442

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2739.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2616x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2616x2048): 91.735
Elapsed time for attention_prob_times_values (48x2048x2048x2616): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2616): 85.282

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2798.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2617x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2617x2048): 88.417
Elapsed time for attention_prob_times_values (48x2048x2048x2617): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2617): 84.502

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2736.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2618x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2618x2048): 90.078
Elapsed time for attention_prob_times_values (48x2048x2048x2618): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2618): 86.825

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2801.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2619x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2619x2048): 88.457
Elapsed time for attention_prob_times_values (48x2048x2048x2619): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2619): 84.585

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2740.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2620x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2620x2048): 90.729
Elapsed time for attention_prob_times_values (48x2048x2048x2620): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2620): 87.032

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2816.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2621x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2621x2048): 88.423
Elapsed time for attention_prob_times_values (48x2048x2048x2621): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2621): 84.704

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2744.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2622x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2622x2048): 89.848
Elapsed time for attention_prob_times_values (48x2048x2048x2622): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2622): 87.162

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2807.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2623x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2623x2048): 88.604
Elapsed time for attention_prob_times_values (48x2048x2048x2623): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2623): 84.689

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2748.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2624x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2624x2048): 99.574
Elapsed time for attention_prob_times_values (48x2048x2048x2624): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2624): 88.939

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2983.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2625x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2625x2048): 90.235
Elapsed time for attention_prob_times_values (48x2048x2048x2625): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2625): 84.636

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2774.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2626x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2626x2048): 91.657
Elapsed time for attention_prob_times_values (48x2048x2048x2626): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2626): 87.185

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2839.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2627x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2627x2048): 90.023
Elapsed time for attention_prob_times_values (48x2048x2048x2627): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2627): 84.851

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2776.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2628x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2628x2048): 92.215
Elapsed time for attention_prob_times_values (48x2048x2048x2628): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2628): 87.373

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2853.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2629x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2629x2048): 89.740
Elapsed time for attention_prob_times_values (48x2048x2048x2629): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2629): 84.852

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2774.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2630x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2630x2048): 91.456
Elapsed time for attention_prob_times_values (48x2048x2048x2630): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2630): 87.151

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2840.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2631x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2631x2048): 89.570
Elapsed time for attention_prob_times_values (48x2048x2048x2631): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2631): 84.928

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2775.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2632x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2632x2048): 92.859
Elapsed time for attention_prob_times_values (48x2048x2048x2632): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2632): 85.807

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2840.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2633x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2633x2048): 89.479
Elapsed time for attention_prob_times_values (48x2048x2048x2633): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2633): 84.951

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2776.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2634x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2634x2048): 90.781
Elapsed time for attention_prob_times_values (48x2048x2048x2634): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2634): 87.272

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2835.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2635x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2635x2048): 89.378
Elapsed time for attention_prob_times_values (48x2048x2048x2635): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2635): 85.124

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2779.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2636x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2636x2048): 91.496
Elapsed time for attention_prob_times_values (48x2048x2048x2636): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2636): 87.422

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2851.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2637x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2637x2048): 89.256
Elapsed time for attention_prob_times_values (48x2048x2048x2637): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2637): 85.076

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2779.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2638x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2638x2048): 90.852
Elapsed time for attention_prob_times_values (48x2048x2048x2638): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2638): 87.386

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2843.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2639x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2639x2048): 89.109
Elapsed time for attention_prob_times_values (48x2048x2048x2639): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2639): 85.288

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2782.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2640x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2640x2048): 93.248
Elapsed time for attention_prob_times_values (48x2048x2048x2640): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2640): 86.618

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2868.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2641x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2641x2048): 88.731
Elapsed time for attention_prob_times_values (48x2048x2048x2641): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2641): 85.336

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2779.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2642x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2642x2048): 90.372
Elapsed time for attention_prob_times_values (48x2048x2048x2642): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2642): 87.462

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2841.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2643x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2643x2048): 88.832
Elapsed time for attention_prob_times_values (48x2048x2048x2643): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2643): 85.376

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2783.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2644x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2644x2048): 90.895
Elapsed time for attention_prob_times_values (48x2048x2048x2644): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2644): 87.540

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2852.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2645x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2645x2048): 88.672
Elapsed time for attention_prob_times_values (48x2048x2048x2645): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2645): 85.436

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2784.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2646x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2646x2048): 90.377
Elapsed time for attention_prob_times_values (48x2048x2048x2646): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2646): 87.402

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2844.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2647x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2647x2048): 88.790
Elapsed time for attention_prob_times_values (48x2048x2048x2647): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2647): 85.350

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2786.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2648x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2648x2048): 91.735
Elapsed time for attention_prob_times_values (48x2048x2048x2648): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2648): 85.955

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2842.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2649x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2649x2048): 88.283
Elapsed time for attention_prob_times_values (48x2048x2048x2649): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2649): 85.483

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2783.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2650x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2650x2048): 90.100
Elapsed time for attention_prob_times_values (48x2048x2048x2650): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2650): 87.639

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2848.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2651x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2651x2048): 88.312
Elapsed time for attention_prob_times_values (48x2048x2048x2651): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2651): 85.494

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2785.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2652x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2652x2048): 90.676
Elapsed time for attention_prob_times_values (48x2048x2048x2652): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2652): 87.638

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2859.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2653x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2653x2048): 88.251
Elapsed time for attention_prob_times_values (48x2048x2048x2653): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2653): 85.477

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2786.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2654x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2654x2048): 90.001
Elapsed time for attention_prob_times_values (48x2048x2048x2654): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2654): 87.615

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2850.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2655x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2655x2048): 88.515
Elapsed time for attention_prob_times_values (48x2048x2048x2655): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2655): 85.592

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2794.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2656x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2656x2048): 99.857
Elapsed time for attention_prob_times_values (48x2048x2048x2656): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2656): 88.405

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 3012.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2657x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2657x2048): 90.066
Elapsed time for attention_prob_times_values (48x2048x2048x2657): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2657): 85.694

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2822.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2658x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2658x2048): 91.731
Elapsed time for attention_prob_times_values (48x2048x2048x2658): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2658): 87.802

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2884.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2659x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2659x2048): 90.235
Elapsed time for attention_prob_times_values (48x2048x2048x2659): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2659): 85.629

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2825.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2660x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2660x2048): 92.064
Elapsed time for attention_prob_times_values (48x2048x2048x2660): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2660): 87.923

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2893.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2661x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2661x2048): 89.708
Elapsed time for attention_prob_times_values (48x2048x2048x2661): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2661): 85.689

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2820.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2662x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2662x2048): 91.323
Elapsed time for attention_prob_times_values (48x2048x2048x2662): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2662): 87.898

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2883.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2663x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2663x2048): 89.460
Elapsed time for attention_prob_times_values (48x2048x2048x2663): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2663): 85.509

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2816.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2664x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2664x2048): 92.564
Elapsed time for attention_prob_times_values (48x2048x2048x2664): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2664): 86.603

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2883.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2665x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2665x2048): 89.344
Elapsed time for attention_prob_times_values (48x2048x2048x2665): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2665): 85.441

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2815.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 31992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2666x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2666x2048): 91.106
Elapsed time for attention_prob_times_values (48x2048x2048x2666): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2666): 87.905

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2884.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2667x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2667x2048): 89.280
Elapsed time for attention_prob_times_values (48x2048x2048x2667): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2667): 85.483

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2817.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2668x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2668x2048): 91.372
Elapsed time for attention_prob_times_values (48x2048x2048x2668): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2668): 88.044

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2893.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2669x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2669x2048): 89.147
Elapsed time for attention_prob_times_values (48x2048x2048x2669): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2669): 85.639

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2819.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2670x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2670x2048): 90.921
Elapsed time for attention_prob_times_values (48x2048x2048x2670): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2670): 87.995

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2887.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2671x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2671x2048): 89.159
Elapsed time for attention_prob_times_values (48x2048x2048x2671): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2671): 85.580

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2820.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2672x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2672x2048): 93.417
Elapsed time for attention_prob_times_values (48x2048x2048x2672): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2672): 87.554

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2920.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2673x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2673x2048): 88.820
Elapsed time for attention_prob_times_values (48x2048x2048x2673): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2673): 85.751

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2820.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2674x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2674x2048): 90.452
Elapsed time for attention_prob_times_values (48x2048x2048x2674): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2674): 88.160

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2887.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2675x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2675x2048): 88.906
Elapsed time for attention_prob_times_values (48x2048x2048x2675): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2675): 85.871

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2825.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2676x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2676x2048): 91.024
Elapsed time for attention_prob_times_values (48x2048x2048x2676): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2676): 88.448

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2903.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2677x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2677x2048): 88.939
Elapsed time for attention_prob_times_values (48x2048x2048x2677): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2677): 85.780

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2827.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2678x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2678x2048): 90.533
Elapsed time for attention_prob_times_values (48x2048x2048x2678): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2678): 88.292

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2894.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2679x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2679x2048): 88.913
Elapsed time for attention_prob_times_values (48x2048x2048x2679): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2679): 85.872

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2830.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2680x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2680x2048): 91.806
Elapsed time for attention_prob_times_values (48x2048x2048x2680): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2680): 86.207

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2881.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2681x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2681x2048): 88.427
Elapsed time for attention_prob_times_values (48x2048x2048x2681): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2681): 86.008

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2826.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2682x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2682x2048): 90.059
Elapsed time for attention_prob_times_values (48x2048x2048x2682): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2682): 88.365

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2892.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2683x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2683x2048): 88.155
Elapsed time for attention_prob_times_values (48x2048x2048x2683): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2683): 85.742

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2820.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2684x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2684x2048): 89.870
Elapsed time for attention_prob_times_values (48x2048x2048x2684): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2684): 88.439

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2893.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2685x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2685x2048): 88.039
Elapsed time for attention_prob_times_values (48x2048x2048x2685): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2685): 85.874

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2822.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2686x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2686x2048): 89.632
Elapsed time for attention_prob_times_values (48x2048x2048x2686): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2686): 88.416

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2891.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2687x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2687x2048): 87.885
Elapsed time for attention_prob_times_values (48x2048x2048x2687): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2687): 85.934

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2823.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2688x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2688x2048): 97.146
Elapsed time for attention_prob_times_values (48x2048x2048x2688): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2688): 91.495

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 3062.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2689x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2689x2048): 89.477
Elapsed time for attention_prob_times_values (48x2048x2048x2689): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2689): 82.449

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2790.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2690x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2690x2048): 91.139
Elapsed time for attention_prob_times_values (48x2048x2048x2690): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2690): 85.369

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2867.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2691x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2691x2048): 89.628
Elapsed time for attention_prob_times_values (48x2048x2048x2691): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2691): 82.889

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2802.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2692x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2692x2048): 90.986
Elapsed time for attention_prob_times_values (48x2048x2048x2692): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2692): 85.521

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2869.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2693x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2693x2048): 90.851
Elapsed time for attention_prob_times_values (48x2048x2048x2693): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2693): 82.878

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2822.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2694x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2694x2048): 92.171
Elapsed time for attention_prob_times_values (48x2048x2048x2694): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2694): 85.417

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2887.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2695x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2695x2048): 90.183
Elapsed time for attention_prob_times_values (48x2048x2048x2695): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2695): 83.015

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2816.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2696x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2696x2048): 92.609
Elapsed time for attention_prob_times_values (48x2048x2048x2696): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2696): 69.680

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2592.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2697x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2697x2048): 89.298
Elapsed time for attention_prob_times_values (48x2048x2048x2697): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2697): 83.037

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2805.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2698x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2698x2048): 90.847
Elapsed time for attention_prob_times_values (48x2048x2048x2698): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2698): 85.459

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2872.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2699x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2699x2048): 89.399
Elapsed time for attention_prob_times_values (48x2048x2048x2699): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2699): 83.105

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2810.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2700x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2700x2048): 91.893
Elapsed time for attention_prob_times_values (48x2048x2048x2700): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2700): 85.622

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2893.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2701x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2701x2048): 89.406
Elapsed time for attention_prob_times_values (48x2048x2048x2701): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2701): 83.192

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2814.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2702x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2702x2048): 90.831
Elapsed time for attention_prob_times_values (48x2048x2048x2702): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2702): 85.547

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2878.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2703x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2703x2048): 89.057
Elapsed time for attention_prob_times_values (48x2048x2048x2703): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2703): 83.287

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2812.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2704x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2704x2048): 93.420
Elapsed time for attention_prob_times_values (48x2048x2048x2704): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2704): 84.142

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2894.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2705x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2705x2048): 89.015
Elapsed time for attention_prob_times_values (48x2048x2048x2705): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2705): 83.416

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2816.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 32472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2706x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2706x2048): 90.718
Elapsed time for attention_prob_times_values (48x2048x2048x2706): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2706): 85.640

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2882.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
slurmstepd: error: *** JOB 1508293 ON frontier03311 CANCELLED AT 2023-11-24T18:45:23 DUE TO TIME LIMIT ***
